<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>MySQL内部的两阶段提交</title>
    <link href="/2023/10/15/MySQL%E5%86%85%E9%83%A8%E7%9A%84%E4%B8%A4%E9%98%B6%E6%AE%B5%E6%8F%90%E4%BA%A4/"/>
    <url>/2023/10/15/MySQL%E5%86%85%E9%83%A8%E7%9A%84%E4%B8%A4%E9%98%B6%E6%AE%B5%E6%8F%90%E4%BA%A4/</url>
    
    <content type="html"><![CDATA[<p>MySQL的存储引擎都是插件式的，在创建表的时候可以指定不同的存储引擎，这些存储引擎之间是独立的，都拥有自己的数据文件和日志，因此MySQL内部的事务都可以看成是一个微缩版的分布式事务，需要在多个存储引擎之间协调。虽然现在大多数Mysql使用者都只使用了一个存储引擎InnoDB，但是开了Binlog后，也可以把Binlog看作一个日志形式的存储引擎，所以Binlog本身也是参与方之一，因此需要在InnoDB和Binlog之间进行2PC(两阶段提交)来提交一个事务。</p><h1 id="2PC协调者初始化"><a href="#2PC协调者初始化" class="headerlink" title="2PC协调者初始化"></a><strong>2PC协调者初始化</strong></h1><p>在2PC协议中有一个角色——协调者(Coordinator)，同样MySQL内部也需要一个这样的角色来协调参与方完成事务的提交，这个协调者就是代码中的全局变量tc_log，它是TC_LOG*类型的指针。TC_LOG类(Transaction Coordinator Log缩写)是一个接口类，主要是对外提供open、close、prepare、commit、rollback等方法，MYSQL_BIN_LOG类就继承了该类并且实现了这几个函数，在binlog开启的情况下，Mysql就会让mysql_bin_log来充当协调者的身份，这部分初始化代码如下：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-comment">/* total_ha_2pc对存储引擎数目进行了计数，每个存储引擎初始化的时候会定义一个</span><br><span class="hljs-comment">   st_mysql_plugin结构体，其中type设置为MYSQL_STORAGE_ENGINE_PLUGIN，</span><br><span class="hljs-comment">   之后会调用plugin_type_initialize[type]函数也即ha_initialize_handlerton</span><br><span class="hljs-comment">   函数，该函数对存储引擎plugin进行初始化并更新total_ha_2pc。*/</span><br><span class="hljs-keyword">if</span> (total_ha_2pc &gt; <span class="hljs-number">1</span> || (<span class="hljs-number">1</span> == total_ha_2pc &amp;&amp; opt_bin_log)) &#123; <span class="hljs-comment">//参与方多于1就需要2PC</span><br>  <span class="hljs-keyword">if</span> (dg::<span class="hljs-built_in">server_is_standby_mode</span>()) &#123; <span class="hljs-comment">/* 在standby模式是没有开启binlog的，使用tc_log_dummy的实现方法基本为空，相当于没有2PC流程 */</span><br>    tc_log = &amp;tc_log_dummy;<br>  &#125; <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span> (opt_bin_log) <span class="hljs-comment">//开启了binlog就让binlog做协调者</span><br>    tc_log = &amp;mysql_bin_log;<br>  <span class="hljs-keyword">else</span><br>    tc_log = &amp;tc_log_mmap;<br>&#125;<br></code></pre></td></tr></table></figure><h1 id="2PC流程"><a href="#2PC流程" class="headerlink" title="2PC流程"></a><strong>2PC流程</strong></h1><p>事务提交从ha_commit_trans函数开始，该函数里完成了2PC提交的全流程，下面是整个流程的调用栈，本小节会重点介绍入口函数和准备部分的内容，组提交放到下一小节展开介绍：</p><figure class="highlight arduino"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs arduino">ha_commit_trans<br>├── MYSQL_BIN_LOG::prepare <span class="hljs-comment">//两阶段提交的准备阶段</span><br>│   └── ha_prepare_low<br>│       ├── binlog_prepare <span class="hljs-comment">//Binlog进行准备，没做什么事情，仅设置单语句事务的commit_parent</span><br>│       └── innobase_xa_prepare <span class="hljs-comment">//InnoDB进行准备，主要是获取了事务XID并且写入到事务对应Undo Log Header中</span><br>│           └── trx_prepare_for_mysql<br>│               └── trx_prepare<br>│                   └── trx_prepare<br>└── MYSQL_BIN_LOG::commit <span class="hljs-comment">//两阶段提交的提交阶段</span><br>    └── MYSQL_BIN_LOG::ordered_commit <span class="hljs-comment">//组提交，在本小节之后展开介绍</span><br></code></pre></td></tr></table></figure><h2 id="autocommit参数介绍"><a href="#autocommit参数介绍" class="headerlink" title="autocommit参数介绍"></a>autocommit参数介绍</h2><p>为了更好理解ha_commit_trans函数的入参，先了解一下相关背景知识。在Innodb中，所有的活动都是在事务中发生，哪怕只是一条单独的语句。Mysql有一个控制自动提交策略的变量autocommit：</p><ul><li>当autocommit开启(&#x3D;1)的时候：客户端发来的每一条语句都默认形成一个单独的事务，只不过这个事务只有一条语句，在语句执行成功后便被当作事务自动提交，无需客户端明确指认；一个包含多语句的事务则是由BEGIN和COMMIT显式包裹，这些语句作为一个整体原子地提交或者回滚。</li><li>当autocommit关闭(&#x3D;0)的时候：客户端发来的每一条语句在没有COMMIT或ROLLBACK显式指认的情况下，都不会被当作事务自动提交，当COMMIT被客户端发出后，前面的所有语句被当作一个事务进行提交，并且同时开启下一个事务。</li></ul><p>关于autocommit更为详细的介绍可见官方文档<a href="https://dev.mysql.com/doc/refman/8.0/en/innodb-autocommit-commit-rollback.html">15.7.2.2 autocommit, Commit, and Rollback</a>。除了autocommit开启时会自动提交单语句事务外，还有一种情况会在客户端不显式指认COMMIT时提交事务，那就是隐式提交，很多DDL语句的执行都会导致当前事务被截断然后提交，就像是在DDL语句前加了一个COMMIT一样，这些DDL语句详见官方文档<a href="https://dev.mysql.com/doc/refman/8.0/en/implicit-commit.html">13.3.3 Statements That Cause an Implicit Commit</a>。</p><p>由上面的介绍可知，事务的提交可以简单分为三种情况：</p><ol><li>正常事务提交(Normal transaction，从trans_commit函数发起调用)，这是由客户端指定的事务，在autocommit开启的情况下是由BEGIN和COMMIT包裹，autocommit关闭时则是由前后两个COMMIT&#x2F;ROLLBACK包裹。</li><li>单语句事务提交(Statement transaction，从trans_commit_stmt函数发起调用)，这是在autocommit开启的情况下，每一个不在正常事务中的语句都会被单独组织成一个单语句事务进行提交，这种事务用户是无法通过ROLLBACK回滚的。</li><li>隐式事务提交(Implicit commit，从trans_commit_implicit函数发起调用)，这是一些DDL语句导致的进行中的事务被隐式提交。</li></ol><h2 id="入口函数-ha-commit-trans"><a href="#入口函数-ha-commit-trans" class="headerlink" title="入口函数 ha_commit_trans"></a>入口函数 ha_commit_trans</h2><p>有了这些背景知识，再来看ha_commit_trans函数的三个入参，其中all就是用来区分是否单语句事务的：</p><table><thead><tr><th>THD *thd</th><th>当前线程</th></tr></thead><tbody><tr><td>bool all</td><td>True：正常事务或者隐式提交</td></tr><tr><td>False：单语句事务</td><td></td></tr><tr><td>bool ignore_global_read_lock</td><td>是否可以无视全局读锁，该参数可以被用于修改内部表</td></tr></tbody></table><p>ha_commit_trans函数主要干了以下两件事情：</p><ol><li>根据参数all确定事务是单语句事务还是另外两种事务，据此可以确定参与的引擎(执行过程中发生写操作时会把自己登记进去)有哪些</li><li>参与方大于1那么就需要2PC来完成提交，接下来就分别调用tc_log-&gt;prepare和tc_log-&gt;commit</li></ol><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">ha_commit_trans</span><span class="hljs-params">(THD *thd, <span class="hljs-type">bool</span> all, <span class="hljs-type">bool</span> ignore_global_read_lock)</span> </span>&#123;<br>  ...<br>  <span class="hljs-comment">/* 事务上下文trn_ctx中的m_scope_info数组维护了两个THD_TRANS，两个THD_TRANS</span><br><span class="hljs-comment">     在不同的情况下被起用，正常事务和隐式提交用的是m_scope_info[1]，单语句事务用</span><br><span class="hljs-comment">     的是m_scope_info[0]，而Transaction_ctx::SESSION与Transaction_ctx::STMT</span><br><span class="hljs-comment">     就分别对应的1和0。于是通过ha_trx_info函数就可以获得相应THD_TRANS的成员变量</span><br><span class="hljs-comment">     m_ha_list，这个list里面存的是本事务影响(修改)到的所有存储引擎，每个存储引擎</span><br><span class="hljs-comment">     都在自己被修改时使用trans_register_ha函数将自己登记进去，这个函数是幂等的。*/</span><br>  Transaction_ctx *trn_ctx = thd-&gt;<span class="hljs-built_in">get_transaction</span>();<br>  Transaction_ctx::enum_trx_scope trx_scope = all ? Transaction_ctx::SESSION : Transaction_ctx::STMT;<br>  <span class="hljs-keyword">auto</span> ha_info = trn_ctx-&gt;<span class="hljs-built_in">ha_trx_info</span>(trx_scope);<br>  ...<br>  <span class="hljs-keyword">if</span> (ha_info &amp;&amp; !error) &#123;<br>    ...<br>    <span class="hljs-comment">/* 判断本事务是不是读写事务，is_real_trans在事务是正常事务、隐式提交或不处于</span><br><span class="hljs-comment">       正常事务内的单语句事务时为True，rw_ha_count则是对本事务修改的引擎数量进行</span><br><span class="hljs-comment">       计数，如果只是读了一个引擎那么将被跳过计数，大于0意味着至少对一个引擎进行了</span><br><span class="hljs-comment">       修改 */</span><br>    rw_trans = is_real_trans &amp;&amp; (rw_ha_count &gt; <span class="hljs-number">0</span>);<br>    <span class="hljs-keyword">if</span> (rw_trans &amp;&amp; !ignore_global_read_lock) &#123;<br>      ...<br>      <span class="hljs-comment">/* 获取MDL锁，这保证事务提交会被FLUSH TABLES WITH READ LOCK语句阻塞 */</span><br>      <span class="hljs-built_in">MDL_REQUEST_INIT</span>(&amp;mdl_request, MDL_key::COMMIT, <span class="hljs-string">&quot;&quot;</span>, <span class="hljs-string">&quot;&quot;</span>,<br>                       MDL_INTENTION_EXCLUSIVE, MDL_EXPLICIT);<br>      <span class="hljs-keyword">if</span> (thd-&gt;mdl_context.<span class="hljs-built_in">acquire_lock</span>(&amp;mdl_request, thd-&gt;variables.lock_wait_timeout))<br>      ...<br>    &#125;<br>    ...<br>    <span class="hljs-comment">/* no_2pc函数检查所有参与的存储引擎是否支持2PC协议，rw_ha_count则是前面计算得到的 */</span><br>    <span class="hljs-keyword">if</span> (!trn_ctx-&gt;<span class="hljs-built_in">no_2pc</span>(trx_scope) &amp;&amp; (trn_ctx-&gt;<span class="hljs-built_in">rw_ha_count</span>(trx_scope) &gt; <span class="hljs-number">1</span>))<br>      <span class="hljs-comment">/* 通过tc_log(开启binlog的话就是binlog)的prepare函数让所有参与的引擎prepare */</span><br>      error = tc_log-&gt;<span class="hljs-built_in">prepare</span>(thd, all);<br>  &#125;<br>  ... <span class="hljs-comment">/* 外部XA事务相关的先不关注 */</span><br>  <span class="hljs-comment">/* 通过tc_log让所有参与的存储引擎提交 */</span><br>  <span class="hljs-keyword">if</span> (error || (error = tc_log-&gt;<span class="hljs-built_in">commit</span>(thd, all)))<br>  ... <span class="hljs-comment">/* 清理工作，释放MDL锁等等 */</span><br>&#125;<br></code></pre></td></tr></table></figure><p>在InnoDB搭配Binlog的情况下，这个tc_log指针指向的就是mysql_bin_log，走的是MYSQL_BIN_LOG实现的两个方法，所以接下来看看这两个函数干了什么。</p><h3 id="准备阶段"><a href="#准备阶段" class="headerlink" title="准备阶段"></a>准备阶段</h3><p>其实MYSQL_BIN_LOG::prepare函数本身实现很简单，就干了两件事：</p><ol><li>设置durability_property延缓引擎层Redo Log刷盘</li><li>透传调用ha_prepare_low函数</li></ol><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">MYSQL_BIN_LOG::prepare</span><span class="hljs-params">(THD *thd, <span class="hljs-type">bool</span> all)</span> </span>&#123;<br>  ...<br>  <span class="hljs-comment">/* 为了减少IO次数，设置durability_property使得存储引擎InnoDB暂缓将Redo Log刷盘，</span><br><span class="hljs-comment">     延迟到最后组提交阶段成组刷盘 */</span><br>  thd-&gt;durability_property = HA_IGNORE_DURABILITY;<br>  <span class="hljs-comment">/* 透传参数到ha_prepare_low */</span><br>  <span class="hljs-type">int</span> error = <span class="hljs-built_in">ha_prepare_low</span>(thd, all);<br>  ... <span class="hljs-comment">/* 外部XA事务相关的先不关注 */</span><br>&#125;<br></code></pre></td></tr></table></figure><p>以Innodb为例，durability_property设置为HA_IGNORE_DURABILITY会在准备阶段跳过对Redo Log的刷盘：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-function"><span class="hljs-type">static</span> <span class="hljs-type">void</span> <span class="hljs-title">trx_flush_logs</span><span class="hljs-params">(<span class="hljs-type">trx_t</span> *trx, <span class="hljs-type">lsn_t</span> lsn)</span> </span>&#123;<br>  ...<br>  <span class="hljs-keyword">switch</span> (<span class="hljs-built_in">thd_requested_durability</span>(trx-&gt;mysql_thd)) &#123;<br>    <span class="hljs-keyword">case</span> HA_IGNORE_DURABILITY:<br>      <span class="hljs-keyword">break</span>; <span class="hljs-comment">//直接跳过刷盘</span><br>    <span class="hljs-keyword">case</span> HA_REGULAR_DURABILITY:<br>      trx-&gt;ddl_must_flush = <span class="hljs-literal">false</span>;<br>      <span class="hljs-built_in">trx_flush_log_if_needed</span>(lsn, trx);<br>  &#125;<br>&#125;<br></code></pre></td></tr></table></figure><p>透传调用的ha_prepare_low函数也很简单，将所有事务参与方的prepare函数都掉用了一遍，并做了一些计数，如下：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">ha_prepare_low</span><span class="hljs-params">(THD *thd, <span class="hljs-type">bool</span> all)</span> </span>&#123;<br>  ...<br>  <span class="hljs-comment">/* 获得本事务影响(修改)到的存储引擎的链表 */</span><br>  <span class="hljs-keyword">auto</span> ha_list = thd-&gt;<span class="hljs-built_in">get_transaction</span>()-&gt;<span class="hljs-built_in">ha_trx_info</span>(trx_scope);<br>  <span class="hljs-keyword">if</span> (ha_list) &#123;<br>    <span class="hljs-keyword">for</span> (<span class="hljs-keyword">auto</span> <span class="hljs-type">const</span> &amp;ha_info : ha_list) &#123; <span class="hljs-comment">//遍历所有引擎信息</span><br>      ... <span class="hljs-comment">/* 一些读写以及外部XA事务检查 */</span><br>      <span class="hljs-comment">/* 获得引擎对应的handlerton，binlog对应的是binlog_hton，</span><br><span class="hljs-comment">         Innodb对应的是innobase_hton，分别在各自初始化时准备好 */</span><br>      <span class="hljs-keyword">auto</span> ht = ha_info.<span class="hljs-built_in">ht</span>();<br>      <span class="hljs-comment">/* 调用引擎handlerton的prepare函数 */</span><br>      <span class="hljs-type">int</span> err = ht-&gt;<span class="hljs-built_in">prepare</span>(ht, thd, all);<br>      ...<br>      <span class="hljs-comment">/* 事务参与方准备完成计数+1 */</span><br>      thd-&gt;status_var.ha_prepare_count++;<br>      ...<br>    &#125;<br>  &#125;<br>  <span class="hljs-keyword">return</span> error;<br>&#125;<br></code></pre></td></tr></table></figure><p>到这里，先确定一下Binlog和InnoDB各自的prepare函数是什么，看一下binlog_hton和innobase_hton各自的初始化可以发现，两者的prepare函数分别是binlog_prepare函数和innobase_xa_prepare函数：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-function"><span class="hljs-type">static</span> <span class="hljs-type">int</span> <span class="hljs-title">binlog_init</span><span class="hljs-params">(<span class="hljs-type">void</span> *p)</span> </span>&#123;<br>  binlog_hton = (handlerton *)p;<br>  ...<br>  binlog_hton-&gt;commit = binlog_commit;<br>  binlog_hton-&gt;rollback = binlog_rollback;<br>  binlog_hton-&gt;prepare = binlog_prepare;<br>  ...<br>&#125;<br><br><span class="hljs-function"><span class="hljs-type">static</span> <span class="hljs-type">int</span> <span class="hljs-title">innodb_init</span><span class="hljs-params">(<span class="hljs-type">void</span> *p)</span> </span>&#123;<br>  handlerton *innobase_hton = (handlerton *)p;<br>  ...<br>  innobase_hton-&gt;commit = innobase_commit;<br>  innobase_hton-&gt;rollback = innobase_rollback;<br>  innobase_hton-&gt;prepare = innobase_xa_prepare;<br>  ...<br>&#125;<br></code></pre></td></tr></table></figure><h4 id="Binlog准备"><a href="#Binlog准备" class="headerlink" title="Binlog准备"></a>Binlog准备</h4><p>对于binlog而言，事务过程中已经把操作写入binlog cache，只需要最终挪到binlog文件中，所以可以认为本身就处于prepare状态，于是binlog_prepare这个函数实现就非常简单，对于单语句事务设置一下commit_parent(用于确定备库回放顺序)即可，详情如下：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-function"><span class="hljs-type">static</span> <span class="hljs-type">int</span> <span class="hljs-title">binlog_prepare</span><span class="hljs-params">(handlerton *, THD *thd, <span class="hljs-type">bool</span> all)</span> </span>&#123;<br>  <span class="hljs-comment">//只有单语句事务需要设置一下commit_parent</span><br>  <span class="hljs-keyword">if</span> (!all) &#123;<br>    thd-&gt;<span class="hljs-built_in">get_transaction</span>()-&gt;<span class="hljs-built_in">store_commit_parent</span>(mysql_bin_log.m_dependency_tracker.<span class="hljs-built_in">get_max_committed_timestamp</span>());<br>  &#125;<br>  <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><h4 id="InnoDB准备"><a href="#InnoDB准备" class="headerlink" title="InnoDB准备"></a>InnoDB准备</h4><p>Innodb的prepare阶段就要复杂一些，下面是innobase_xa_prepare函数的主干：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-function"><span class="hljs-type">static</span> <span class="hljs-type">int</span> <span class="hljs-title">innobase_xa_prepare</span><span class="hljs-params">(handlerton *hton, THD *thd, <span class="hljs-type">bool</span> prepare_trx)</span></span><br><span class="hljs-function"></span>&#123;<br>  <span class="hljs-comment">/* 根据线程thd获得Innodb对应的事务对象trx */</span><br>  <span class="hljs-type">trx_t</span> *trx = <span class="hljs-built_in">check_trx_exists</span>(thd);<br>  <span class="hljs-comment">/* 从线程获取事务的xid写到trx-&gt;xid中 */</span><br>  <span class="hljs-built_in">thd_get_xid</span>(thd, (MYSQL_XID *)trx-&gt;xid);<br>  ...<br>  <span class="hljs-comment">/* prepare_trx就是透传的all，代表正常事务提交和隐式提交，对于autocommit模式下</span><br><span class="hljs-comment">     的单语句，只要不是BEGIN语句，就当作单语句事务提交 */</span><br>  <span class="hljs-keyword">if</span> (prepare_trx || (!<span class="hljs-built_in">thd_test_options</span>(thd, OPTION_NOT_AUTOCOMMIT | OPTION_BEGIN))) &#123;<br>    <span class="hljs-comment">/* 内部简单检查后透传调用trx_prepare */</span><br>    <span class="hljs-type">dberr_t</span> err = <span class="hljs-built_in">trx_prepare_for_mysql</span>(trx);<br>    ...<br>  &#125;<br>  ...<br>&#125;<br></code></pre></td></tr></table></figure><p>可以看出该函数根据thd获得Innodb内部的事务对象，并且给它设置了XID，经过检查后调用trx_prepare_for_mysql函数，trx_prepare_for_mysql函数也有简单检查随后便直接透传调用了trx_prepare函数，在完成调用后设置事务状态为TRX_STATE_PREPARED即可，详情如下：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-function"><span class="hljs-type">static</span> <span class="hljs-type">void</span> <span class="hljs-title">trx_prepare</span><span class="hljs-params">(<span class="hljs-type">trx_t</span> *trx)</span> </span>&#123;<br>  <span class="hljs-type">lsn_t</span> lsn = <span class="hljs-number">0</span>;<br>  <span class="hljs-comment">/* trx-&gt;rsegs是分配给本事务Undo Segment的Rollback Segment们的关联信息，有两种分类：</span><br><span class="hljs-comment">   也就是两个if语句中的m_redo和m_noredo，m_redo是驻留在系统/Undo Tablespace</span><br><span class="hljs-comment">     中的Rollback Segment，需要在redo log中进行持久化；m_noredo则是驻留在临时</span><br><span class="hljs-comment">     Tablespace中的Rollback Segment，不需要持久化，所以第一个有分配Redo Log中</span><br><span class="hljs-comment">     的LSN，第二个没有。 */</span><br>  <span class="hljs-keyword">if</span> (trx-&gt;rsegs.m_redo.rseg != <span class="hljs-literal">nullptr</span> &amp;&amp; <span class="hljs-built_in">trx_is_redo_rseg_updated</span>(trx)) &#123;<br>    <span class="hljs-comment">/* 将对应Rollback Segment中本事务的Insert Undo Log和Update Undo Log设置为准备状态 */</span><br>    lsn = <span class="hljs-built_in">trx_prepare_low</span>(trx, &amp;trx-&gt;rsegs.m_redo, <span class="hljs-literal">false</span>);<br>  &#125;<br>  <span class="hljs-keyword">if</span> (trx-&gt;rsegs.m_noredo.rseg != <span class="hljs-literal">nullptr</span> &amp;&amp; <span class="hljs-built_in">trx_is_temp_rseg_updated</span>(trx)) &#123;<br>    <span class="hljs-built_in">trx_prepare_low</span>(trx, &amp;trx-&gt;rsegs.m_noredo, <span class="hljs-literal">true</span>);<br>  &#125;<br><br>  <span class="hljs-comment">/* 设置事务状态为prepared */</span><br>  <span class="hljs-built_in">trx_sys_mutex_enter</span>();<br>  trx-&gt;state.<span class="hljs-built_in">store</span>(TRX_STATE_PREPARED, std::memory_order_relaxed);<br>  trx_sys-&gt;n_prepared_trx++;<br>  <span class="hljs-built_in">trx_sys_mutex_exit</span>();<br><br>  ... <span class="hljs-comment">/* 读已提交及其之下隔离级别的需要释放间隙锁 */</span><br>    <br>  <span class="hljs-comment">/* Redo Log刷盘，但是之前binlog作为协调者准备时设置了HA_IGNORE_DURABILITY，</span><br><span class="hljs-comment">     组提交为了避免频繁刷盘，在5.7版本中修改了组提交的flush阶段，在prepare阶段不</span><br><span class="hljs-comment">     再让线程各自执行flush redolog操作，而是推迟到组提交的flush阶段。 */</span><br>  <span class="hljs-keyword">if</span> (lsn &gt; <span class="hljs-number">0</span>) &#123;<br>    <span class="hljs-built_in">trx_flush_logs</span>(trx, lsn);<br>  &#125;<br>&#125;<br></code></pre></td></tr></table></figure><p>再进一步看一下trx_prepare_low函数的主体逻辑，这个函数主要将事务相关的Undo Log调整为prepare状态，关于Undo Log相关数据结构见<a href="https://www.yuque.com/captain32/space/ek259gm6zgrr5wrr?view=doc_embed">Undo Log基本数据结构</a>，该函数详情如下：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-function"><span class="hljs-type">static</span> <span class="hljs-type">lsn_t</span> <span class="hljs-title">trx_prepare_low</span><span class="hljs-params">(<span class="hljs-type">trx_t</span> *trx, <span class="hljs-type">trx_undo_ptr_t</span> *undo_ptr, <span class="hljs-type">bool</span> noredo_logging)</span></span><br><span class="hljs-function"></span>&#123;<br>  <span class="hljs-comment">/* 事务有Insert类型Undo Log或者Update类型Undo Log，需要把相应的Undo Log设置为准备状态 */</span><br>  <span class="hljs-keyword">if</span> (undo_ptr-&gt;insert_undo != <span class="hljs-literal">nullptr</span> || undo_ptr-&gt;update_undo != <span class="hljs-literal">nullptr</span>) &#123;<br>    <span class="hljs-type">mtr_t</span> mtr;<br>    <span class="hljs-comment">/* 事务Undo Log所在的Rollback Segment */</span><br>    <span class="hljs-type">trx_rseg_t</span> *rseg = undo_ptr-&gt;rseg;<br><br>    <span class="hljs-comment">/* 开启一个Mini Transaction(引擎层内部小事务)，后面涉及硬盘内容的更新 */</span><br>    <span class="hljs-built_in">mtr_start_sync</span>(&amp;mtr);<br><br>    <span class="hljs-keyword">if</span> (noredo_logging) &#123;<br>      <span class="hljs-comment">/* 临时Tablespace下不需要将Undo Log本身录入Redo Log，这里不让MTR写到Redo Log */</span><br>      <span class="hljs-built_in">mtr_set_log_mode</span>(&amp;mtr, MTR_LOG_NO_REDO);<br>    &#125;<br><br>    rseg-&gt;<span class="hljs-built_in">latch</span>();<br>    <span class="hljs-keyword">if</span> (undo_ptr-&gt;insert_undo != <span class="hljs-literal">nullptr</span>) &#123; <span class="hljs-comment">/* 处理Insert类型Undo Log */</span><br>      <span class="hljs-comment">//设置Undo Segment在文件、内存层面的状态，向Undo Log Header中写入xid</span><br>      <span class="hljs-built_in">trx_undo_set_state_at_prepare</span>(trx, undo_ptr-&gt;insert_undo, <span class="hljs-literal">false</span>, &amp;mtr);<br>    &#125;<br>    <span class="hljs-keyword">if</span> (undo_ptr-&gt;update_undo != <span class="hljs-literal">nullptr</span>) &#123; <span class="hljs-comment">/* 处理Update类型Undo Log */</span><br>      <span class="hljs-keyword">if</span> (!noredo_logging) &#123;<br>        <span class="hljs-comment">/* 设置Undo Log对象的m_gtid_storage字段为PREPARE_AND_COMMIT */</span><br>        <span class="hljs-built_in">trx_undo_gtid_set</span>(trx, undo_ptr-&gt;update_undo, <span class="hljs-literal">true</span>);<br>      &#125;<br>      <span class="hljs-comment">/* 与上面Insert类型类似，设置xid到Undo Log Header中 */</span><br>      <span class="hljs-built_in">trx_undo_set_state_at_prepare</span>(trx, undo_ptr-&gt;update_undo, <span class="hljs-literal">false</span>, &amp;mtr);<br>    &#125;<br>    rseg-&gt;<span class="hljs-built_in">unlatch</span>();<br>    <br>    <span class="hljs-comment">/* 其实是mtr.commit()，内部调用Command::execute()函数，正常情况下写Redo Log，</span><br><span class="hljs-comment">       同时更新m_commit_lsn */</span><br>    <span class="hljs-built_in">mtr_commit</span>(&amp;mtr);<br><br>    <span class="hljs-keyword">if</span> (!noredo_logging) &#123; <span class="hljs-comment">/* 如果Undo Log本身变更需要录入Redo Log，获取lsn */</span><br>      <span class="hljs-comment">/* 获取m_commit_lsn，也就是刚才提交的小事务在Redo Log中的位置 */</span><br>      <span class="hljs-type">const</span> <span class="hljs-type">lsn_t</span> lsn = mtr.<span class="hljs-built_in">commit_lsn</span>();<br>      <span class="hljs-keyword">return</span> lsn;<br>    &#125;<br>  &#125;<br><br>  <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><h3 id="提交阶段"><a href="#提交阶段" class="headerlink" title="提交阶段"></a>提交阶段</h3><p>MYSQL_BIN_LOG::commit函数大部分内容是在为最终组提交做准备工作，首先对于非空的stmt_cache(单语句事务使用)和trx_cache(显式事务使用)，会视情况添加XID Event，并且调用finalize方法标记写入Binlog结束，来为之后组提交的flush做准备。当binlog cache完成了写入，就意味着所有需要持久化的内容都已经在cache中，之后就可以调用ordered_commit函数(下一小节详细介绍)开始组提交，详情如下：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-function">TC_LOG::enum_result <span class="hljs-title">MYSQL_BIN_LOG::commit</span><span class="hljs-params">(THD *thd, <span class="hljs-type">bool</span> all)</span> </span>&#123;<br>  Transaction_ctx *trn_ctx = thd-&gt;<span class="hljs-built_in">get_transaction</span>();<br>  my_xid xid = trn_ctx-&gt;<span class="hljs-built_in">xid_state</span>()-&gt;<span class="hljs-built_in">get_xid</span>()-&gt;<span class="hljs-built_in">get_my_xid</span>();<br>  <span class="hljs-type">bool</span> stmt_stuff_logged = <span class="hljs-literal">false</span>;<br>  <span class="hljs-type">bool</span> trx_stuff_logged = <span class="hljs-literal">false</span>;<br>  ...<br>  <span class="hljs-comment">/* 获得本线程的binlog cache manager */</span><br>  binlog_cache_mngr *cache_mngr = <span class="hljs-built_in">thd_get_cache_mngr</span>(thd);<br>  ...<br>  <span class="hljs-comment">/* 类似之前，用all区分非单语句事务和单语句事务 */</span><br>  Transaction_ctx::enum_trx_scope trx_scope = all ? Transaction_ctx::SESSION : Transaction_ctx::STMT;<br>  <br>  <span class="hljs-comment">/* 单语句事务的binlog cache非空，则需要提交 */</span><br>  <span class="hljs-keyword">if</span> (!cache_mngr-&gt;stmt_cache.<span class="hljs-built_in">is_binlog_empty</span>()) &#123;<br>    <span class="hljs-comment">/* 设置commit parent为最近提交的事务 */</span><br>    trn_ctx-&gt;<span class="hljs-built_in">store_commit_parent</span>(m_dependency_tracker.<span class="hljs-built_in">get_max_committed_timestamp</span>());<br>    <span class="hljs-comment">/* finalize函数会标记flags.finalized为true，意味着事务完成binlog cache</span><br><span class="hljs-comment">       的写入，只有经过这一步之后才可以将内容从cache刷到文件 */</span><br>    <span class="hljs-keyword">if</span> (cache_mngr-&gt;stmt_cache.<span class="hljs-built_in">finalize</span>(thd)) <span class="hljs-keyword">return</span> RESULT_ABORTED;<br>    <span class="hljs-comment">/* 设置stmt_stuff_logged为true，用于后续判断，可以进入组提交流程 */</span><br>    stmt_stuff_logged = <span class="hljs-literal">true</span>;<br>  &#125;<br><br>  <span class="hljs-comment">/* ending_trans就是简单判断all为true(正常事务、隐式提交)，或者是单语句事务，</span><br><span class="hljs-comment">     也即需要提交的事务。trx_stuff_logged会在本if末尾处设置为true供后续判断 */</span><br>  <span class="hljs-keyword">if</span> (!cache_mngr-&gt;trx_cache.<span class="hljs-built_in">is_binlog_empty</span>() &amp;&amp; <span class="hljs-built_in">ending_trans</span>(thd, all) &amp;&amp; !trx_stuff_logged) &#123;<br>    <span class="hljs-comment">/* 再一次判断，就是正常事务、隐式提交、单语句事务三种 */</span><br>    <span class="hljs-type">const</span> <span class="hljs-type">bool</span> real_trans = (all || !trn_ctx-&gt;<span class="hljs-built_in">is_active</span>(Transaction_ctx::SESSION));<br>    <br>    <span class="hljs-keyword">if</span>() ... <span class="hljs-comment">//外部XA事务相关    </span><br>    <span class="hljs-comment">/* 原子DDL直接finalize binlog cache */</span><br>    <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span> ((is_atomic_ddl = cache_mngr-&gt;trx_cache.<span class="hljs-built_in">has_xid</span>())) &#123;<br>      <span class="hljs-keyword">if</span> (cache_mngr-&gt;trx_cache.<span class="hljs-built_in">finalize</span>(thd, <span class="hljs-literal">nullptr</span>)) <span class="hljs-keyword">return</span> RESULT_ABORTED;<br>    &#125;<br>    <span class="hljs-comment">/* 为事务将xid event记入binlog cache，并且进行finalize */</span><br>    <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span> (real_trans &amp;&amp; xid &amp;&amp; trn_ctx-&gt;<span class="hljs-built_in">rw_ha_count</span>(trx_scope) &gt; <span class="hljs-number">1</span> &amp;&amp; !trn_ctx-&gt;<span class="hljs-built_in">no_2pc</span>(trx_scope)) &#123;<br>      Xid_log_event <span class="hljs-built_in">end_evt</span>(thd, xid);<br>      <span class="hljs-keyword">if</span> (cache_mngr-&gt;trx_cache.<span class="hljs-built_in">finalize</span>(thd, &amp;end_evt)) <span class="hljs-keyword">return</span> RESULT_ABORTED;<br>    &#125;<br>    <span class="hljs-comment">/* 其他情况补一个COMMIT在binlog cache的最后并finalize */</span><br>    <span class="hljs-keyword">else</span> &#123;<br>      Query_log_event <span class="hljs-built_in">end_evt</span>(thd, <span class="hljs-built_in">STRING_WITH_LEN</span>(<span class="hljs-string">&quot;COMMIT&quot;</span>), <span class="hljs-literal">true</span>, <span class="hljs-literal">false</span>, <span class="hljs-literal">true</span>, <span class="hljs-number">0</span>, <span class="hljs-literal">true</span>);<br>      <span class="hljs-keyword">if</span> (cache_mngr-&gt;trx_cache.<span class="hljs-built_in">finalize</span>(thd, &amp;end_evt)) <span class="hljs-keyword">return</span> RESULT_ABORTED;<br>    &#125;<br>    <span class="hljs-comment">/* 设置为true用于之后进入组提交 */</span><br>    trx_stuff_logged = <span class="hljs-literal">true</span>;<br>  &#125;<br><br>  <span class="hljs-comment">/* 根据之前设置的stmt_stuff_logged和trx_stuff_logged，开启组提交 */</span><br>  <span class="hljs-keyword">if</span> (stmt_stuff_logged || trx_stuff_logged) &#123;<br>    ...<br>    <span class="hljs-comment">/* 组提交 */</span><br>    <span class="hljs-keyword">if</span> (<span class="hljs-built_in">ordered_commit</span>(thd, all, skip_commit)) <span class="hljs-keyword">return</span> RESULT_INCONSISTENT;<br>    ... <span class="hljs-comment">/* 外部XA事务相关 */</span><br>  &#125; <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span> (!skip_commit) &#123;<br>    <span class="hljs-keyword">if</span> (trx_coordinator::<span class="hljs-built_in">commit_in_engines</span>(thd, all))<br>      <span class="hljs-keyword">return</span> RESULT_INCONSISTENT;<br>  &#125;<br><br>  <span class="hljs-keyword">return</span> RESULT_SUCCESS;<br>&#125;<br></code></pre></td></tr></table></figure><h1 id="组提交"><a href="#组提交" class="headerlink" title="组提交"></a>组提交</h1><p>当有多个事务并发执行时，redo log是可以穿插记录不同事务所做修改的，但是binlog不允许不同事务穿插记录，所以就需要每个事务先将自己所有的event记录到独占的binlog cache中，直到提交的时候，将binlog cache中所有内容作为一个整体刷到binlog文件中，这样就避免了不同事务日志的穿插。</p><p>事务是有多个的，但是binlog文件只有一个，因此这些事务之间也存在竞争，最简单的想法就是用一把锁保护binlog文件，持有锁的事务才可以将自己的binlog cache刷到binlog文件中，这也是5.6 版本之前的做法，在2PC前需要获得这把锁，提交完成后再释放锁，因此同一时刻只能有一个事务在2PC流程中，这显然大大降低了并发度。</p><p>5.6版本引入了组提交，组提交将2PC的提交阶段分为了三步：</p><ol><li>flush stage：多个线程按进入的顺序将 binlog 从 cache 写入文件（不刷盘）</li><li>sync stage：对 binlog 文件做 fsync 操作（多个线程的 binlog 合并一次刷盘）</li><li>commit stage：各个线程按顺序做InnoDB commit操作(ha_commit_low)，这里没有刷盘</li></ol><p>这三步每一步都有锁进行保护，各自维护自己的队列，其中第2步是提升性能的关键，合并刷盘可以让各个事务日志刷盘的均摊IO消耗大大降低。并且因为锁的粒度减小，三步可以并发执行，形成了流水线一样的流程。</p><p>组提交可以大幅提升多事务提交的性能，但是2PC的性能瓶颈也从commit阶段转移到了prepare阶段，原因是prepare阶段每个事务都对Redo Log进行了刷盘，因此5.7版本将这一步后推(也就有了之前Prepare阶段设置HA_IGNORE_DURABILITY)到了commit阶段的flush stage，于是新的flush stage不光要完成binlog cache写入文件，还要完成Redo Log的刷盘，就相当于为Redo Log做了一次组写入。于是2PC的提交阶段变为：</p><ol><li>flush stage：完成Redo Log刷盘，多个线程按进入的顺序将 binlog 从 cache 写入文件（不刷盘）</li><li>sync stage：对 binlog 文件做 fsync 操作（多个线程的 binlog 合并一次刷盘）</li><li>commit stage：各个线程按顺序做InnoDB commit操作(ha_commit_low)，在Redo Log中给事务添加Commit标记，这个不要求刷盘，因为不影响故障恢复</li></ol><p>接下来看一下组提交ordered_commit的详细实现：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">MYSQL_BIN_LOG::ordered_commit</span><span class="hljs-params">(THD *thd, <span class="hljs-type">bool</span> all, <span class="hljs-type">bool</span> skip_commit)</span> </span>&#123;<br>  ... <span class="hljs-comment">/* 备库回放需要等待的Stage 0，略过 */</span><br><br>  <span class="hljs-comment">/* Stage 1: 进入flush stage */</span><br>  <span class="hljs-keyword">if</span> (<span class="hljs-built_in">change_stage</span>(thd, Commit_stage_manager::BINLOG_FLUSH_STAGE, thd, <span class="hljs-literal">nullptr</span>, &amp;LOCK_log)) &#123;<br>    <span class="hljs-comment">/* 非leader线程醒来后直接finish即可，leader已经帮自己做了所有事 */</span><br>    <span class="hljs-keyword">return</span> <span class="hljs-built_in">finish_commit</span>(thd);<br>  &#125;<br>  ...<br>  <span class="hljs-comment">/* 处理flush stage queue中所有事务，包括刷盘Redo Log，分配事务GTID，把所有事务的</span><br><span class="hljs-comment">   binlog cache写到binlog的内存Buffer中 */</span><br>  flush_error = <span class="hljs-built_in">process_flush_stage_queue</span>(&amp;total_bytes, &amp;do_rotate, &amp;wait_queue);<br>  <span class="hljs-keyword">if</span> (flush_error == <span class="hljs-number">0</span> &amp;&amp; total_bytes &gt; <span class="hljs-number">0</span>) &#123;<br>    <span class="hljs-comment">/* 最终调用的是m_binlog_file-&gt;m_pipeline_head-&gt;flush()，这里才真正把</span><br><span class="hljs-comment">      binlog内存Buffer中内容刷到操作系统Page Cache中 */</span><br>    flush_error = <span class="hljs-built_in">flush_cache_to_file</span>(&amp;flush_end_pos);<br>  &#125;<br>  ... <span class="hljs-comment">/* 设置binlog end pos、错误处理等 */</span><br>  <br><br>  <span class="hljs-comment">/* Stage 2: 进入sync stage */</span><br>  <span class="hljs-keyword">if</span> (<span class="hljs-built_in">change_stage</span>(thd, Commit_stage_manager::SYNC_STAGE, wait_queue, &amp;LOCK_log, &amp;LOCK_sync)) &#123;<br>    <span class="hljs-comment">/* 非leader线程醒来后直接finish即可，leader已经帮自己做了所有事 */</span><br>    <span class="hljs-keyword">return</span> <span class="hljs-built_in">finish_commit</span>(thd);<br>  &#125;<br>  <span class="hljs-comment">/* get_sync_period函数其实就是sync_binlog参数，sync_counter是有多少组进入到</span><br><span class="hljs-comment">     该stage的计数，sync_counter+1的语义就是算上目前这组是否达到了sync_binlog的</span><br><span class="hljs-comment">     要求，在之后sync_binlog_file函数里面会再判断一次，用于决定是否sync。例如</span><br><span class="hljs-comment">     sync_binlog=1代表每一组都达到要求得sync，sync_binlog=1000代表每1000组sync</span><br><span class="hljs-comment">     一次，特例是sync_binlog=0，会直接跳过sync。</span><br><span class="hljs-comment">     在这里if判断是为了binlog_group_commit_sync_no_delay_count和</span><br><span class="hljs-comment">     opt_binlog_group_commit_sync_delay两个参数，分别决定了启动sync所需的事务</span><br><span class="hljs-comment">     数量和最大等待时间，故意等待是为了增加备库回放的并行度。特例sync_binlog=0也会</span><br><span class="hljs-comment">     在这里被留下来等待。效果就是flush阶段的多个组合并成了一个组，一起刷盘。 */</span><br>  <span class="hljs-keyword">if</span> (!flush_error &amp;&amp; (sync_counter + <span class="hljs-number">1</span> &gt;= <span class="hljs-built_in">get_sync_period</span>()))<br>    Commit_stage_manager::<span class="hljs-built_in">get_instance</span>().<span class="hljs-built_in">wait_count_or_timeout</span>(<br>        opt_binlog_group_commit_sync_no_delay_count,<br>        opt_binlog_group_commit_sync_delay, Commit_stage_manager::SYNC_STAGE);<br><br>  <span class="hljs-comment">/* 获得sync stage队列，并且清空队列，使得新的一组可以开始组装 */</span><br>  final_queue = Commit_stage_manager::<span class="hljs-built_in">get_instance</span>().<span class="hljs-built_in">fetch_queue_acquire_lock</span>(<br>      Commit_stage_manager::SYNC_STAGE);<br>  <span class="hljs-comment">/* binlog进行sync，其中sync_binlog_file会再次判断sync_counter是否达到</span><br><span class="hljs-comment">     sync_binlog要求，满足才进行sync，否则跳过 */</span><br>  <span class="hljs-keyword">if</span> (flush_error == <span class="hljs-number">0</span> &amp;&amp; total_bytes &gt; <span class="hljs-number">0</span>) &#123;<br>    std::pair&lt;<span class="hljs-type">bool</span>, <span class="hljs-type">bool</span>&gt; result = <span class="hljs-built_in">sync_binlog_file</span>(<span class="hljs-literal">false</span>);<br>    sync_error = result.first;<br>  &#125;<br>  <span class="hljs-comment">/* 根据需要更新binlog文件在内存中的end pos */</span><br>  <span class="hljs-keyword">if</span> (update_binlog_end_pos_after_sync &amp;&amp; flush_error == <span class="hljs-number">0</span> &amp;&amp; sync_error == <span class="hljs-number">0</span>) &#123;<br>    ... <span class="hljs-comment">/* 获取final_queue存的这组事务中的最大end_pos并更新binlog的end pos */</span><br>  &#125;<br>  <span class="hljs-comment">/* 用于等会儿释放锁 */</span><br>  leave_mutex_before_commit_stage = &amp;LOCK_sync;<br><br>  <span class="hljs-comment">/* 判断是否需要commit order，不需要的话就直接让各个事务自己commit就好了，不维持顺序 */</span><br>  <span class="hljs-keyword">if</span> ((opt_binlog_order_commits || Clone_handler::<span class="hljs-built_in">need_commit_order</span>()) &amp;&amp;<br>      (sync_error == <span class="hljs-number">0</span> || binlog_error_action != ABORT_SERVER)) &#123;<br>    <span class="hljs-comment">/* Stage 3: 进入commit stage */</span><br>    <span class="hljs-keyword">if</span> (<span class="hljs-built_in">change_stage</span>(thd, Commit_stage_manager::COMMIT_STAGE, final_queue, leave_mutex_before_commit_stage, &amp;LOCK_commit)) &#123;<br>      <span class="hljs-comment">/* 非leader线程醒来后直接finish即可，leader已经帮自己做了所有事 */</span><br>      <span class="hljs-keyword">return</span> <span class="hljs-built_in">finish_commit</span>(thd);<br>    &#125;<br>    <span class="hljs-comment">/* 获得commit stage队列，并且清空队列，使得新的一组可以开始组装 */</span><br>    THD *commit_queue = Commit_stage_manager::<span class="hljs-built_in">get_instance</span>().<span class="hljs-built_in">fetch_queue_acquire_lock</span>(Commit_stage_manager::COMMIT_STAGE);<br>    ...<br>    <span class="hljs-comment">/* 在Innodb引擎中逐个提交事务，维持了顺序 */</span><br>    <span class="hljs-built_in">process_commit_stage_queue</span>(thd, commit_queue);<br>    <span class="hljs-built_in">mysql_mutex_unlock</span>(&amp;LOCK_commit);<br>    <span class="hljs-comment">/* 主备半同步相关 */</span><br>    <span class="hljs-built_in">process_after_commit_stage_queue</span>(thd, commit_queue);<br>    final_queue = commit_queue;<br>  &#125; <span class="hljs-keyword">else</span> &#123;<br>    <span class="hljs-keyword">if</span> (leave_mutex_before_commit_stage) <span class="hljs-comment">/* 释放sync stage时获得的锁 */</span><br>      <span class="hljs-built_in">mysql_mutex_unlock</span>(leave_mutex_before_commit_stage);<br>    ...<br>  &#125;<br><br>  <span class="hljs-comment">/* 通知队列中所有等待的线程，设置每个线程的thd-&gt;tx_commit_pending标志，</span><br><span class="hljs-comment">     follower线程被唤醒后会调用finish_commit，如果发现事务没有提交，会</span><br><span class="hljs-comment">     调用ha_commit_low函数，此时就不能保证commit的顺序了，finish_commit</span><br><span class="hljs-comment">     函数中的工作大多与process_commit_stage_queue函数对每个事务的处理</span><br><span class="hljs-comment">     重叠，如果已经进行过就不做了。 */</span><br>  Commit_stage_manager::<span class="hljs-built_in">get_instance</span>().<span class="hljs-built_in">signal_done</span>(final_queue);<br>  (<span class="hljs-type">void</span>)<span class="hljs-built_in">finish_commit</span>(thd);<br><br>  ... <span class="hljs-comment">/* 根据Flush Stage设置的rotate_var进行binlog文件的rotate */</span><br>&#125;<br></code></pre></td></tr></table></figure><h2 id="Stage切换"><a href="#Stage切换" class="headerlink" title="Stage切换"></a>Stage切换</h2><p>在ordered_commit函数中，Stage与Stage之间需要调用change_stage函数来进行切换，内部调用的是Commit_stage_manager::enroll_for函数，Commit_stage_manager维护了不同Stage的队列，每个队列的队首线程会主导该阶段所有队列中成员的提交工作，其他线程会挂起等待被唤醒即可，下面看看enroll_for函数：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-function"><span class="hljs-type">bool</span> <span class="hljs-title">Commit_stage_manager::enroll_for</span><span class="hljs-params">(StageID stage, THD *thd,</span></span><br><span class="hljs-params"><span class="hljs-function">                                      <span class="hljs-type">mysql_mutex_t</span> *stage_mutex,</span></span><br><span class="hljs-params"><span class="hljs-function">                                      <span class="hljs-type">mysql_mutex_t</span> *enter_mutex)</span> </span>&#123;<br>  <span class="hljs-built_in">lock_queue</span>(stage);<br>  <span class="hljs-comment">/* 首个入队的是leader，为True，上个Stage的leader进入新Stage也可能变为follower */</span><br>  <span class="hljs-type">bool</span> leader = m_queue[stage].<span class="hljs-built_in">append</span>(thd);<br>  ... <span class="hljs-comment">/* COMMIT_ORDER_FLUSH_STAGE与备库回放相关，先无视 */</span><br>  <span class="hljs-built_in">unlock_queue</span>(stage);<br><br>  ...<br><br>  <span class="hljs-comment">/* 释放上个Stage获得的锁 */</span><br>  <span class="hljs-keyword">if</span> (stage_mutex &amp;&amp; need_unlock_stage_mutex) <span class="hljs-built_in">mysql_mutex_unlock</span>(stage_mutex);<br><br>  <span class="hljs-comment">/* 不是leader的线程需要休眠，等待完成提交后被leader唤醒即可 */</span><br>  <span class="hljs-keyword">if</span> (!leader) &#123;<br>    <span class="hljs-built_in">CONDITIONAL_SYNC_POINT_FOR_TIMESTAMP</span>(<span class="hljs-string">&quot;before_follower_wait&quot;</span>);<br>    <span class="hljs-built_in">mysql_mutex_lock</span>(&amp;m_lock_done);<br>    <span class="hljs-keyword">while</span> (thd-&gt;tx_commit_pending) &#123; <span class="hljs-comment">/* 在ordered_commit函数末尾sigal_done函数中被设置 */</span><br>      <span class="hljs-built_in">mysql_cond_wait</span>(&amp;m_stage_cond_binlog, &amp;m_lock_done);<br>    &#125;<br>    <span class="hljs-built_in">mysql_mutex_unlock</span>(&amp;m_lock_done);<br>    <span class="hljs-keyword">return</span> <span class="hljs-literal">false</span>;<br>  &#125;<br><br>  <span class="hljs-comment">/* leader需要获得下个Stage的锁 */</span><br>  <span class="hljs-keyword">if</span> (leader &amp;&amp; enter_mutex != <span class="hljs-literal">nullptr</span>) &#123;<br>    ...<br>    <span class="hljs-keyword">if</span> (need_lock_enter_mutex)<br>      <span class="hljs-built_in">mysql_mutex_lock</span>(enter_mutex);<br>    <span class="hljs-keyword">else</span><br>      <span class="hljs-built_in">mysql_mutex_assert_owner</span>(enter_mutex);<br>  &#125;<br><br>  ...<br><br>  <span class="hljs-keyword">return</span> leader;<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="Flush-Stage"><a href="#Flush-Stage" class="headerlink" title="Flush Stage"></a>Flush Stage</h2><p>看下process_flush_stage_queue函数，其内对Redo Log做了刷盘，给每个事务分配了GTID，进而生成了GTID Event，形成了一个事务Binlog内容的完全体，最终把组内所有成员的Binlog Cache内容都写到Binlog的内存Buffer中，外部调用者稍后就会把Buffer内容flush到操作系统的Page Cache中，详情如下：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">MYSQL_BIN_LOG::process_flush_stage_queue</span><span class="hljs-params">(<span class="hljs-type">my_off_t</span> *total_bytes_var, <span class="hljs-type">bool</span> *rotate_var, THD **out_queue_var)</span> </span>&#123;<br>  ...<br>  <span class="hljs-comment">/* 这个函数里会获取flush stage的线程队列，并且清空它使得可以继续组装新的组，</span><br><span class="hljs-comment">     在函数的最后会调用ha_flush_logs函数最终执行innobase_flush_logs函数，</span><br><span class="hljs-comment">     将Redo Log写操作系统Page Cache，innodb_flush_log_at_trx_commit为1时</span><br><span class="hljs-comment">     才会真正刷盘 */</span><br>  THD *first_seen = <span class="hljs-built_in">fetch_and_process_flush_stage_queue</span>();<br>  <br>  <span class="hljs-comment">/* 为组内事务自动分配gtid */</span><br>  <span class="hljs-built_in">assign_automatic_gtids_to_flush_group</span>(first_seen);<br>  <br>  <span class="hljs-comment">/* 将每个线程的binlog cache内容写到binlog文件内存Buffer中 */</span><br>  <span class="hljs-keyword">for</span> (THD *head = first_seen; head; head = head-&gt;next_to_commit) &#123;<br>    <span class="hljs-comment">/* 将current_thd换成head，作用域内仿佛head自己在执行 */</span><br>    <span class="hljs-function">Thd_backup_and_restore <span class="hljs-title">switch_thd</span><span class="hljs-params">(current_thd, head)</span></span>;<br>    <span class="hljs-comment">/* 最终调用的是m_binlog_file-&gt;m_pipeline_head-&gt;write()，只是写到</span><br><span class="hljs-comment">       binlog的内存Buffer中，外部调用者后续会把Buffer内容flush到Page Cache */</span><br>    std::pair&lt;<span class="hljs-type">int</span>, <span class="hljs-type">my_off_t</span>&gt; result = <span class="hljs-built_in">flush_thread_caches</span>(head);<br>    total_bytes += result.second;<br>    <span class="hljs-keyword">if</span> (flush_error == <span class="hljs-number">1</span>) flush_error = result.first;<br>  &#125;<br><br>  *out_queue_var = first_seen;<br>  *total_bytes_var = total_bytes;<br>  <span class="hljs-comment">/* 如果binlog文件大小超过最大限制，需要准备rotate */</span><br>  <span class="hljs-keyword">if</span> (total_bytes &gt; <span class="hljs-number">0</span> &amp;&amp;<br>      (m_binlog_file-&gt;<span class="hljs-built_in">get_real_file_size</span>() &gt;= (<span class="hljs-type">my_off_t</span>)max_size ||<br>       <span class="hljs-built_in">DBUG_EVALUATE_IF</span>(<span class="hljs-string">&quot;simulate_max_binlog_size&quot;</span>, <span class="hljs-literal">true</span>, <span class="hljs-literal">false</span>)))<br>    *rotate_var = <span class="hljs-literal">true</span>;<br>  <span class="hljs-keyword">return</span> flush_error;<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="Sync-Stage"><a href="#Sync-Stage" class="headerlink" title="Sync Stage"></a>Sync Stage</h2><p>看下sync_binlog_file函数，内容很简单，就是sync刷盘，如下：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-function">std::pair&lt;<span class="hljs-type">bool</span>, <span class="hljs-type">bool</span>&gt; <span class="hljs-title">MYSQL_BIN_LOG::sync_binlog_file</span><span class="hljs-params">(<span class="hljs-type">bool</span> force)</span> </span>&#123;<br>  <span class="hljs-type">bool</span> synced = <span class="hljs-literal">false</span>;<br>  <span class="hljs-type">unsigned</span> <span class="hljs-type">int</span> sync_period = <span class="hljs-built_in">get_sync_period</span>();<br>  <span class="hljs-comment">/* 用于决定是否sync，其中get_sync_period函数其实就是sync_binlog参数，</span><br><span class="hljs-comment">     sync_counter是有多少组进入到该stage的计数。例如sync_binlog=1代表</span><br><span class="hljs-comment">     每一组都达到要求得sync，sync_binlog=1000代表每100组sync一次，特例</span><br><span class="hljs-comment">     是sync_binlog=0，会直接跳过sync */</span><br>  <span class="hljs-keyword">if</span> (force || (sync_period &amp;&amp; ++sync_counter &gt;= sync_period)) &#123;<br>    sync_counter = <span class="hljs-number">0</span>; <span class="hljs-comment">//重置sync_counter</span><br>    <span class="hljs-comment">/* 调用m_binlog_file-&gt;sync()将binlog内容从操作系统的Page Cache刷盘 */</span><br>    <span class="hljs-keyword">if</span> (<span class="hljs-built_in">DBUG_EVALUATE_IF</span>(<span class="hljs-string">&quot;simulate_error_during_sync_binlog_file&quot;</span>, <span class="hljs-number">1</span>,<br>                         m_binlog_file-&gt;<span class="hljs-built_in">is_open</span>() &amp;&amp; m_binlog_file-&gt;<span class="hljs-built_in">sync</span>())) &#123;<br>      ...<br>    &#125;<br>    synced = <span class="hljs-literal">true</span>;<br>  &#125;<br>  <span class="hljs-keyword">return</span> std::<span class="hljs-built_in">make_pair</span>(<span class="hljs-literal">false</span>, synced);<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="Commit-Stage"><a href="#Commit-Stage" class="headerlink" title="Commit Stage"></a>Commit Stage</h2><p>首先看下process_commit_stage_queue函数，里面内容比较简单，就是按顺序把每个事务在InnoDB中提交，如下：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">MYSQL_BIN_LOG::process_commit_stage_queue</span><span class="hljs-params">(THD *thd, THD *first)</span> </span>&#123;<br>  <span class="hljs-comment">/* 将queue中所有事务在引擎中提交 */</span><br>  <span class="hljs-keyword">for</span> (THD *head = first; head; head = head-&gt;next_to_commit) &#123;<br>    ...<br>    <span class="hljs-comment">/* 将thd换成head，作用域内仿佛head自己在执行 */</span><br>    <span class="hljs-function">Thd_backup_and_restore <span class="hljs-title">switch_thd</span><span class="hljs-params">(thd, head)</span></span>;<br>    <span class="hljs-comment">/* real_commit在刚进入ordered_commit函数时被设置，就是传入的all参数</span><br><span class="hljs-comment">       区分非单语句事务和单语句事务 */</span><br>    <span class="hljs-type">bool</span> all = head-&gt;<span class="hljs-built_in">get_transaction</span>()-&gt;m_flags.real_commit;<br>    <span class="hljs-comment">/* 非外部XA事务，最终透传参数走到ha_commit_low函数 */</span><br>    ::<span class="hljs-built_in">finish_transaction_in_engines</span>(head, all, <span class="hljs-literal">false</span>);<br>  &#125;<br><br>  <span class="hljs-comment">/* 更新GTID状态，把事务们拥有的gtid转为已执行，详见“浅析MySQL GTID” */</span><br>  gtid_state-&gt;<span class="hljs-built_in">update_commit_group</span>(first);<br><br>  <span class="hljs-keyword">for</span> (THD *head = first; head; head = head-&gt;next_to_commit) &#123;<br>    <span class="hljs-function">Thd_backup_and_restore <span class="hljs-title">switch_thd</span><span class="hljs-params">(thd, head)</span></span>;<br>    <span class="hljs-keyword">auto</span> all = head-&gt;<span class="hljs-built_in">get_transaction</span>()-&gt;m_flags.real_commit;<br>    <span class="hljs-comment">/* 对于binlog和innodb分别调用binlog_set_prepared_in_tc函数(啥都没做)</span><br><span class="hljs-comment">       和innobase_set_prepared_in_tc函数，设置为已准备，应该和外部XA事务相关 */</span><br>    trx_coordinator::<span class="hljs-built_in">set_prepared_in_tc_in_engines</span>(head, all);<br>    <span class="hljs-comment">/* 该事务已提交，减少prepared状态XID计数 */</span><br>    <span class="hljs-keyword">if</span> (head-&gt;<span class="hljs-built_in">get_transaction</span>()-&gt;m_flags.xid_written) <span class="hljs-built_in">dec_prep_xids</span>(head);<br>  &#125;<br>&#125;<br></code></pre></td></tr></table></figure><h3 id="innobase-commit"><a href="#innobase-commit" class="headerlink" title="innobase_commit"></a>innobase_commit</h3><p>在ha_commit_low函数中，会调用所有登记被修改的引擎的commit函数，对于binlog而言就是binlog_commit函数，这个函数啥也没做，对于Innodb而言就是innobase_commit函数，接下来看看这个函数，该函数内会为事务添加Commit标记，如下：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-function"><span class="hljs-type">static</span> <span class="hljs-type">int</span> <span class="hljs-title">innobase_commit</span><span class="hljs-params">(handlerton *hton, THD *thd, <span class="hljs-type">bool</span> commit_trx)</span></span><br><span class="hljs-function"></span>&#123;<br>  ...<br>  <span class="hljs-keyword">if</span> (will_commit) &#123;<br>    <span class="hljs-keyword">if</span> (!read_only) &#123;<br>      ... <span class="hljs-comment">/* 进入commit状态，维护Innodb提交线程并发度 */</span><br><br>      <span class="hljs-comment">/* 获得事务在binlog中的提交位置 */</span><br>      ulonglong pos;<br>      <span class="hljs-built_in">thd_binlog_pos</span>(thd, &amp;trx-&gt;mysql_log_file_name, &amp;pos);<br>      trx-&gt;mysql_log_offset = <span class="hljs-built_in">static_cast</span>&lt;<span class="hljs-type">uint64_t</span>&gt;(pos);<br><br>      <span class="hljs-comment">/* 为了组提交统一刷盘，设置事务为稍后刷盘，在后续innobase_commit_low中被用到 */</span><br>      trx-&gt;flush_log_later = <span class="hljs-literal">true</span>;<br>    &#125;<br><br>    ... <span class="hljs-comment">/* 持久化gtid相关 */</span><br><br>    <span class="hljs-comment">/* 基本透传，调用栈trx_commit_for_mysql-&gt;trx_commit-&gt;trx_commit_low */</span><br>    <span class="hljs-built_in">innobase_commit_low</span>(trx);<br><br>    <span class="hljs-keyword">if</span> (!read_only) &#123;<br>      <span class="hljs-comment">/* 设置稍后刷盘为false，trx_commit_complete_for_mysql函数中进行刷盘 */</span><br>      trx-&gt;flush_log_later = <span class="hljs-literal">false</span>;<br>      ... <span class="hljs-comment">/* 离开commit状态，维护Innodb提交线程并发度 */</span><br>    &#125;<br><br>    <span class="hljs-comment">/* 设置事务不再是在2PC中注册的事务 */</span><br>    <span class="hljs-built_in">trx_deregister_from_2pc</span>(trx);<br><br>    <span class="hljs-comment">/* 将提交阶段新产生的Redo Log写入操作系统Page Cache，DDL操作一定刷盘，</span><br><span class="hljs-comment">       DML操作由于之前设置了HA_IGNORE_DURABILITY会直接返回跳过刷盘，不会</span><br><span class="hljs-comment">       因为刷盘而阻塞。其实这一步对于事务完整性已经影响不大，恢复的时候可以拿</span><br><span class="hljs-comment">       着XID去binlog找是否已提交即可。 */</span><br>    <span class="hljs-keyword">if</span> (!read_only) &#123;<br>      <span class="hljs-built_in">trx_commit_complete_for_mysql</span>(trx);<br>    &#125;<br><br>  &#125; <span class="hljs-keyword">else</span> &#123;<br>    ... <span class="hljs-comment">/* 不提交事务，只标记单语句结束 */</span><br>  &#125;<br>   ...<br>&#125;<br></code></pre></td></tr></table></figure><p>在trx_commit_low函数中主要调用了trx_write_serialisation_history和trx_commit_in_memory两个函数。trx_write_serialisation_history函数中分别处理了事务的Insert类型Undo Log和Update类型Undo Log，包括设置状态为TRX_UNDO_CACHED(首页使用率不足3&#x2F;4)、TRX_UNDO_TO_FREE(Insert类型)、TRX_UNDO_TO_PURGE(Update类型)三者之一，根据设置的状态将Undo Log对应的内存对象插入相应链表。随后的trx_commit_in_memory函数会将这些mtr内的操作提交，接下来看一下trx_commit_in_memory函数：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-function"><span class="hljs-type">static</span> <span class="hljs-type">void</span> <span class="hljs-title">trx_commit_in_memory</span><span class="hljs-params">(<span class="hljs-type">trx_t</span> *trx, <span class="hljs-type">const</span> <span class="hljs-type">mtr_t</span> *mtr, <span class="hljs-type">bool</span> serialised)</span></span><br><span class="hljs-function"></span>&#123;<br>  trx-&gt;must_flush_log_later = <span class="hljs-literal">false</span>;<br>  trx-&gt;ddl_must_flush = <span class="hljs-literal">false</span>;<br><br>  <span class="hljs-keyword">if</span> (<span class="hljs-built_in">trx_is_autocommit_non_locking</span>(trx)) &#123;...&#125; <span class="hljs-comment">/* 隐喻着只读 */</span><br>  <span class="hljs-keyword">else</span> &#123; <span class="hljs-comment">/* 读写事务 */</span><br>    <span class="hljs-comment">/* 内部将trx-&gt;state设置为TRX_STATE_COMMITTED_IN_MEMORY，</span><br><span class="hljs-comment">       从trx_sys的事务链表将本事务摘除，本事务的提交已被其他事务可见 */</span><br>    <span class="hljs-built_in">trx_release_impl_and_expl_locks</span>(trx, serialised);<br>    ...<br>  &#125;<br><br>  ... <span class="hljs-comment">/* gtid持久化相关 */</span><br><br>  <span class="hljs-keyword">if</span> (mtr != <span class="hljs-literal">nullptr</span>) &#123;<br>    <span class="hljs-comment">/* 清理本事务的insert类型undo log，在commit后再也用不着，</span><br><span class="hljs-comment">       update类型的得留着MVCC使用 */</span><br>    <span class="hljs-keyword">if</span> (trx-&gt;rsegs.m_redo.insert_undo != <span class="hljs-literal">nullptr</span>) &#123;<br>      <span class="hljs-built_in">trx_undo_insert_cleanup</span>(&amp;trx-&gt;rsegs.m_redo, <span class="hljs-literal">false</span>);<br>    &#125;<br>    <span class="hljs-keyword">if</span> (trx-&gt;rsegs.m_noredo.insert_undo != <span class="hljs-literal">nullptr</span>) &#123;<br>      <span class="hljs-built_in">trx_undo_insert_cleanup</span>(&amp;trx-&gt;rsegs.m_noredo, <span class="hljs-literal">true</span>);<br>    &#125;<br><br>    <span class="hljs-type">lsn_t</span> lsn = mtr-&gt;<span class="hljs-built_in">commit_lsn</span>();<br>    <span class="hljs-keyword">if</span> (lsn == <span class="hljs-number">0</span>) &#123;...&#125;<br>    <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span> (trx-&gt;flush_log_later) &#123; <span class="hljs-comment">/* innobase_commit中已设置为true，命中该分支 */</span><br>      trx-&gt;must_flush_log_later = <span class="hljs-literal">true</span>;<br>      <span class="hljs-comment">/* 存下来ddl_operation，等会会在trx_init函数中被设为false */</span><br>      trx-&gt;ddl_must_flush = trx-&gt;ddl_operation;<br>    &#125;<br>    <span class="hljs-keyword">else</span> &#123;...&#125;<br>    trx-&gt;commit_lsn = lsn;<br><br>    <span class="hljs-comment">/* 唤醒被挂起的主线程，类似的purge线程这些后台线程也起来工作 */</span><br>    <span class="hljs-built_in">srv_active_wake_master_thread</span>();<br>  &#125;<br><br>  <span class="hljs-comment">/* 减少分配给事务的Rollback Segment的引用计数 */</span><br>  <span class="hljs-keyword">if</span> (trx-&gt;rsegs.m_redo.rseg != <span class="hljs-literal">nullptr</span>) &#123;<br>    <span class="hljs-type">trx_rseg_t</span> *rseg = trx-&gt;rsegs.m_redo.rseg;<br>    rseg-&gt;trx_ref_count--;<br>    trx-&gt;rsegs.m_redo.rseg = <span class="hljs-literal">nullptr</span>;<br>  &#125;<br>  ...<br>  <span class="hljs-comment">/* 会设置trx一部分字段，例如trx-&gt;id=0，后续trx_commit_complete_for_mysql判断</span><br><span class="hljs-comment">     会据此判断是否刷盘Redo Log */</span><br>  <span class="hljs-built_in">trx_init</span>(trx);<br>  ...<br>&#125;<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>MySQL</category>
      
      <category>Server</category>
      
      <category>Transaction Layer</category>
      
    </categories>
    
    
    <tags>
      
      <tag>MySQL</tag>
      
      <tag>两阶段提交</tag>
      
      <tag>组提交</tag>
      
      <tag>读源码</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>分支一致性 CT</title>
    <link href="/2023/10/15/%E5%88%86%E6%94%AF%E4%B8%80%E8%87%B4%E6%80%A7-CT/"/>
    <url>/2023/10/15/%E5%88%86%E6%94%AF%E4%B8%80%E8%87%B4%E6%80%A7-CT/</url>
    
    <content type="html"><![CDATA[<p>本文依据<a href="https://research.swtch.com/tlog">Transparent Logs for Skeptical Clients</a>整理，主要介绍了<a href="https://certificate.transparency.dev/">Certificate Transparency</a>如何以日志的形式公开管理证书，避免传统PKI体系下CA(Certificate Authority)被攻击导致证书不可信的问题</p><h2 id="预备知识"><a href="#预备知识" class="headerlink" title="预备知识"></a>预备知识</h2><ul><li><a href="https://yeasy.gitbook.io/blockchain_guide/05_crypto/cert">数字证书</a></li><li><a href="https://yeasy.gitbook.io/blockchain_guide/05_crypto/pki">PKI体系</a></li><li><a href="https://yeasy.gitbook.io/blockchain_guide/05_crypto/merkle_trie">Merkle 树结构</a></li></ul><h2 id="PKI体系问题"><a href="#PKI体系问题" class="headerlink" title="PKI体系问题"></a>PKI体系问题</h2><p>PKI体系下由CA给各个域名对应的服务器签发证书，客户端(比如浏览器)通过信任CA，根据CA的公钥验证服务器返回证书签名的有效性，来确认服务器证书的有效性，进而相信服务器。CA在PKI体系下至关重要，浏览器一般信任了超过100个CA，会有以下安全性问题：</p><ul><li>不同CA的安全防范能力参差不齐，木桶原理，最弱的CA遭受黑客攻击会给用户带来安全问题</li><li>每个CA由工作人员维护，不能保证所有工作人员可信</li><li>CA无法避免受到政府的管控，掺入政府的意图</li></ul><p>CA的不安全性会使得“假证书”的情况频发，比如一个黑客的服务器绑定了Google的域名，并利用不安全的CA获得了证书，那么客户端就会相信这个黑客的服务器就是Google的服务器，导致后续安全问题。并且在现有PKI体系下，这样的“假证书”问题很难第一时间发现。</p><h2 id="Certificate-Transparency"><a href="#Certificate-Transparency" class="headerlink" title="Certificate Transparency"></a>Certificate Transparency</h2><p><a href="https://certificate.transparency.dev/howctworks/">How CT works</a>给出了CT的证书签发的替代方案，如下图所示。在CT中，依然有CA的存在，CA需要将证书登记到日志服务器，日志服务器上的日志只允许Append不断增长，其中每个日志项是一个签发的证书。这个日志是公开可见的，客户端可以检查服务器发送的证书是否在日志中来验证证书的合法性，每个公司(例如Google)可以设立专门的Monitor来定期扫描日志来检查是否假冒自己公司的证书。整个流程将PKI体系下CA存的内容变得透明了，外部请求可以直接访问日志，使得审计更加方便，能更快发现“假证书”。当发现“假证书”后，由于日志是只允许添加的，CA可以将“假证书”放入一个回收列表，每次确认证书合法性时先检查一下是否在回收列表中。<br><img src="/2023/10/15/%E5%88%86%E6%94%AF%E4%B8%80%E8%87%B4%E6%80%A7-CT/%E5%9B%BE1.png" alt="图1" title="图1"><br>提供一个全局可见、透明的日志是一个美好的愿想，但是如何保证被攻陷的CA不会篡改日志？同时全局的日志将会是很大的一个文件，如何快速检查某个日志项在日志中？接下来将介绍Merkle树，并且介绍CT如何利用Merkle树构建这么一个全局可见、不可篡改的日志，提供了<strong>分支一致性</strong>(即一个客户端看到了一个分支的话，将始终在这个分支上前进，无法再合并到其他分支)。</p><p>在CA、日志服务器被攻陷的情况下，CA、日志服务器可能给Client发送假证书，并且营造出假日志来欺骗Client，同时给Monitor呈现另一份日志让Monitor发现了不了假证书，实现瞒天过海。CT保证在这种情况下，<strong>Client可以通过和Monitor交流发现自己被欺骗(能检测到分支的不同)<strong>，并且能保留CA的犯罪证据，进而使CA失信。在惩罚之下，CA、日志服务器就会尽可能的遵守规则，</strong>呈现出全局可见并且只有一个分支的日志</strong>。</p><h2 id="Merkle树"><a href="#Merkle树" class="headerlink" title="Merkle树"></a>Merkle树</h2><p>Merkle树的每个叶子节点是实际存储内容(CT中就是日志项，即证书)的哈希值，每个内部节点是其两个子节点内容拼接后的哈希值，如此形成一棵树。为了方便，先暂时考虑叶子节点数量为2的幂的情况，这样树将会是满二叉树，如下图所示具有16个叶子结点的Merkle树：<br><img src="/2023/10/15/%E5%88%86%E6%94%AF%E4%B8%80%E8%87%B4%E6%80%A7-CT/%E5%9B%BE2.png" alt="图2" title="图2"><br>在上图例子中，有16个日志项，所以有16个(0-15)叶子结点，树高5层(0-4)，每一层节点数量依次减半。用h(L,K)来表示层级L的第K个节点存的哈希值，有：</p><ul><li>叶子节点是第K个记录的哈希值$h(0,K)&#x3D;H(record\ K)$</li><li>内部节点是两个子节点拼接后的哈希值$h(L,K)&#x3D;H(h(L-1,2K),h(L-1,2K+1))$</li></ul><p><strong>Merkle树结构可以用来快速判断某个日志项是否在日志中</strong>，比如客户端需要判断B是否是日志中的第9个日志项，可以如下拆解：</p><p><img src="/2023/10/15/%E5%88%86%E6%94%AF%E4%B8%80%E8%87%B4%E6%80%A7-CT/%E5%85%AC%E5%BC%8F1.jpg"></p><p>其实就是检查$h(4,0)$和$H(h(3, 0), H(H(H(h(0, 8), H(B)), h(1, 5)), h(2, 3)))$是否相等，那么当客户端需要验证某个证书B是否是日志的第9个日志项时，服务端向客户端发送根节点$h(4,0)$，邻居节点$h(0,8)$、$h(1,5)$、$h(2,3)$、$h(3,0)$即可，客户端拿到后自行计算校验，校验通过就说明证书B真的在日志中，就可以选择相信证书。至于如何确认<strong>服务端要传哪些节点哈希值给客户端用于证明第K个日志项在日志中，就是将根节点和第K个日志项到根节点的路径中所有节点的邻居节点传给客户端</strong>，客户端就可以重新计算出根节点哈希值，就可以完成校验，比如下图。<br><img src="/2023/10/15/%E5%88%86%E6%94%AF%E4%B8%80%E8%87%B4%E6%80%A7-CT/%E5%9B%BE3.png" alt="图3" title="图3"></p><h2 id="日志结构设计"><a href="#日志结构设计" class="headerlink" title="日志结构设计"></a>日志结构设计</h2><p>上面介绍中，将日志项个数理想化为2的幂，这样的Merkle树是满二叉树，但是大部分情况下日志项个数都不是2的幂，所以为了方便起见，在日志设计上就需要考虑将日志拆分成若干个满二叉树组合成的实际二叉树。比如具有13个日志项的日志，就可以拆分成13&#x3D;8+4+1三个满二叉树，如下图所示。<br><img src="/2023/10/15/%E5%88%86%E6%94%AF%E4%B8%80%E8%87%B4%E6%80%A7-CT/%E5%9B%BE4.png" alt="图4" title="图4"><br>图中标记为x的节点是通过两个子满二叉树组合出的节点，比如$h(3,x)&#x3D;H(h(2,2),h(0,12))$，这种节点被CT称为<strong>临时节点</strong>，用数字标记的在某个满二叉树中的节点被称为<strong>永久节点</strong>。永久节点会被日志服务器存储在硬盘上，但是临时节点并不会被存起来也不会被服务器返回，因为临时节点后续会因为继续添加日志项而被改变，当需要返回临时节点对应的哈希值时返回其永久子节点即可。例如下图所示，长度为7的日志增长为长度为8时，第7个日志项的添加，使Merkle树多了$h(0,7)$节点，它又与$h(0,6)$组合出了$h(1,3)$，进一步与$h(1,2)$组合出$h(2,1)$，再随后组合出了新的根节点$h(3,0)$。在这个过程中，两个y标识的临时节点消失，添加了4个永久节点，形成了一课新的Merkle树。可以发现永久节点自创建后永远不会变但是临时节点只用于表示当前日志状态，日志的增长会使临时节点改变。<br><img src="/2023/10/15/%E5%88%86%E6%94%AF%E4%B8%80%E8%87%B4%E6%80%A7-CT/%E5%9B%BE5.png" alt="图5" title="图5"><br>所以CT设计中真正进行日志存储时，有几个只允许Append增长的文件：</p><ul><li>日志文件，这个文件中按顺序存了一个个日志项(证书)，存了真正的数据内容</li><li>索引文件，这个文件是对第一个文件的索引，记录了每个日志项在文件中的offset，方便读出某个日志项或者遍历日志项</li><li>Merkle树文件，这是一组文件，每个文件代表了一个level中<strong>永久节点</strong>的哈希值，比如图4中日志对应的level-2文件存的就是$h(2,0)$、$h(2,1)$、$h(2,2)$，level-1文件存的就只有$h(3,0)$</li></ul><p>当向日志中添加新日志项时，首先是向日志文件、索引文件中添加相应的日志项与索引。随后计算出该日志项的哈希值添加到Merkle树的level-0文件，当该哈希值可以和某个邻居节点配对时，计算出两者联合后的哈希值添加到level-1文件，以此类推更新上层文件，直到无法配对形成新节点。</p><h2 id="日志使用"><a href="#日志使用" class="headerlink" title="日志使用"></a>日志使用</h2><h3 id="日志项是否存在"><a href="#日志项是否存在" class="headerlink" title="日志项是否存在"></a>日志项是否存在</h3><p>客户端需要校验证书的有效性，需要检查日志项是否存在于日志中，客户端不能完全信任CA、日志服务器说某个日志项在不在日志中，而应该让其提供密码学安全的证明，证明日志项在日志中。与前文描述的满二叉树验证方式类似，服务器同样需要返回日志项对应叶子节点到根结点之间的路径的邻居节点，供客户端计算出根节点哈希值，对比服务器返回的根结点哈希值就可以知道日志项是否存在于日志中。比如下图中，验证证书B是否是第9个日志项，服务器需要返回根节点$h(4,x)$，邻居节点$h(0,8)$、$h(1,5)$、$h(0,12)$、$h(3,0)$，其中$h(0,12)$是$h(2,2)$的邻居节点，虽然两者不在一个level。<br><img src="/2023/10/15/%E5%88%86%E6%94%AF%E4%B8%80%E8%87%B4%E6%80%A7-CT/%E5%9B%BE6.png" alt="图6" title="图6"><br>这个证明是密码学安全的，因为假如恶意日志服务器想证明不在日志中的假证书B，就需要伪造出其他邻居节点的哈希值来使得计算结果是根节点哈希值，但是哈希函数是单向的，这个伪造几乎不可能，所以只要服务器可以给出证明，那么客户端就可以相信证书B确实在日志中。<br>因为路径长度是$log(N)$的，所以可以用$log(N)$的传输信息、时间复杂度完成校验。</p><h3 id="前缀日志判断"><a href="#前缀日志判断" class="headerlink" title="前缀日志判断"></a>前缀日志判断</h3><p>因为CT中日志是单调增长的，所以<strong>任何日志项被添加到日志项后，不应该被修改、删除，所以任何旧版本日志都理应是新版本日志的前缀</strong>。前缀判断之所以重要，在于CA、日志服务器可能被黑客攻陷，假如它们在一次请求中给客户端维护了一个假日志，里面包含了假证书，骗完客户端后，在下一次请求中又给客户端发了真日志，客户端不加检查的话犯罪证据就被掩盖了。CT为了避免这种情况，需要<strong>客户端持久化最近一次CA发给它的日志根节点哈希值STH(Signed Tree Head)，这个哈希值是CA数字签名的，未来无法抵赖</strong>。在客户端需要更新的日志时，首先要验证自己已知的日志是不是新日志的前缀，如果不是就说明CA、日志服务器篡改了自己已知的日志，表明自己曾经信任的证书是不安全的！</p><p><strong>前缀判断还可以用于客户端和Monitor之间的交流</strong>，因为恶意CA、日志服务器可能给客户端、Monitor维护两份日志，一方面提供恶意服务给客户端，一方面应对Monitor的检查。所以客户端和Monitor之间的交流是重要的，但是同一时间两者见到的日志可能是不同版本，无法直接比较根节点哈希值，所以<strong>通常检查其中一个日志是否是另一个日志的前缀来判断两者看到是不是同一份日志</strong>。</p><p>一个版本的日志，可以由其Merkle树的根节点哈希值STH唯一标识，所以已知两个版本日志的STH，目标是证明旧版本日志是新版本日志的前缀，有下图示例。已知前7个(0-6)日志项组成日志的STH为$T_7$，前13个(0-12)日志项组成日志的STH为$T_{13}$，两个Merkle树中的临时节点分别由y、x代指。服务端需要返回哪些节点哈希值才可以证明$T_7$是$T_{13}$的前缀？<br><img src="/2023/10/15/%E5%88%86%E6%94%AF%E4%B8%80%E8%87%B4%E6%80%A7-CT/%E5%9B%BE7.png" alt="图7" title="图7"><br><strong>首先将旧版本的</strong>$T_7$<strong>进行拆解，因为不能用临时节点，所以需要拆解成尽可能少的永久节点(即所有子满二叉树的根节点)<strong>，有$T_7 &#x3D; H(h(2, 0), H(h(1, 2), h(0, 6)))$，即上图中蓝圈中的节点。</strong>随后再拆解新版本的</strong>$T_{13}$<strong>，在拆解时需要完全使用旧版本的拆解结果(即蓝圈节点)，并且再配上尽可能少的额外的永久节点</strong>，即上图中红圈中的节点，因此有：</p><p><img src="/2023/10/15/%E5%88%86%E6%94%AF%E4%B8%80%E8%87%B4%E6%80%A7-CT/%E5%85%AC%E5%BC%8F2.jpg"></p><p>所以服务器为了证明$T_7$是$T_{13}$的前缀，需要返回蓝圈节点以及红圈节点，<strong>客户端利用蓝圈节点计算出</strong>$T_7’$<strong>与已知的</strong>$T_7$<strong>对比，利用蓝圈节点计算出</strong>$T_{13}’$<strong>与已知的</strong>$T_{13}$<strong>对比，两者都完全一致就说明</strong>$T_7$<strong>是</strong>$T_{13}$<strong>的前缀</strong>。</p><p>这个证明同样是密码学安全的，因为假如恶意日志服务器想造假，证明某个旧版本日志是某个不同的新版本日志的前缀，蓝圈节点可以直接返回计算出旧版本日志的STH，但是需要伪造出红圈节点的哈希值使得配合蓝圈节点后计算结果是新版本日志的STH，哈希函数是单向的，这个伪造几乎不可能，所以只要服务器可以给出证明，那么客户端就可以相信旧版本日志是新版本日志的前缀。</p><p>因为蓝圈节点是将旧版本日志长度M拆分成多个2的幂，所以蓝圈节点个数是$log(M)$的。红圈节点则是搭配蓝圈节点组成新版本的根节点，配合蓝圈节点的红圈节点数量是$log(N)$的。所以可以用$log(M)+log(N)&#x3D;log(MN)$的传输信息、时间复杂度完成校验。</p><h3 id="认证流程"><a href="#认证流程" class="headerlink" title="认证流程"></a>认证流程</h3><p>有了前两个校验的理论基础后，就可以完整描述客户端遇到一个新证书B的认证流程：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python">validate(bits B <span class="hljs-keyword">as</span> record R): <span class="hljs-comment"># 验证数据块B是否是第R个日志项</span><br>    <span class="hljs-keyword">if</span> R ≥ cached.N: <span class="hljs-comment"># R比当前客户端见过的日志长度cached.N大，这里是为了保证见过的日志足够新可以用于校验新证书</span><br>        N, T = server.Latest() <span class="hljs-comment"># 客户端需要找服务器获取最新的日志长度N，新日志的根节点哈希值T</span><br>        <span class="hljs-keyword">if</span> server.TreeProof(cached.N, N) cannot be verified: <span class="hljs-comment"># 客户端找服务器要自己缓存中长度对应的日志是新日志的前缀的证明，服务端返回后，客户端自行计算新旧根节点哈希值，与已知的cached.T和T对比完成校验</span><br>            fail loudly<br>        cached.N, cached.T = N, T <span class="hljs-comment"># 更新见过的日志长度、根节点哈希值为新日志的长度、根节点哈希值</span><br>    <span class="hljs-keyword">if</span> server.RecordProof(R, cached.N) cannot be verified using B: <span class="hljs-comment"># 客户端找服务器要自己缓存长度对应的日志证明第R个日志项的证明，客户端利用B计算出根节点哈希值与自己的cached.T对比进行校验</span><br>        fail loudly<br>    accept B <span class="hljs-keyword">as</span> record R <span class="hljs-comment"># 所有校验通过，接受新证书B</span><br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>MIT6.824 分布式系统</category>
      
    </categories>
    
    
    <tags>
      
      <tag>分支一致性</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>分支一致性 SUNDR</title>
    <link href="/2023/10/15/%E5%88%86%E6%94%AF%E4%B8%80%E8%87%B4%E6%80%A7-SUNDR/"/>
    <url>/2023/10/15/%E5%88%86%E6%94%AF%E4%B8%80%E8%87%B4%E6%80%A7-SUNDR/</url>
    
    <content type="html"><![CDATA[<p>论文标题为《Secure Untrusted Data Repository (SUNDR)》，发表于2004年，这篇论文主要介绍了分布式文件系统SUNDR，考虑了服务器是恶意节点的情况，可以保证分支一致性。</p><h2 id="分支一致性"><a href="#分支一致性" class="headerlink" title="分支一致性"></a>分支一致性</h2><p>SUNDR实现了一个分布式文件系统，但是不像Frangipani那样主要关注如何组织存储，而是考虑了有恶意服务器的场景下如何安全的存储。在常规的分布式文件系统中，基本都默认存储服务器是安全可信的，但是在SUNDR的假设中，这些存储服务器可以是外部的，所以可以是不安全、恶意的。恶意的存储服务器会使得整个文件系统内容变的不可信。为了应对这种情况，SUNDR通过操作日志、公私钥签名等技术保证了<strong>分支一致性</strong>(Fork Consistency)。</p><p>在分支一致性的要求下，如果用户A的读操作收到恶意服务器篡改的日志，比如删除了之前用户B的某个修改操作，那么用户A后续获得的日志项中只要包含了用户B的操作，都可以揭示用户A被骗，因为两者不在一个分支但是现在又回到了一个分支。所以恶意服务器为了维持这个骗局，就需要专门为用户A维护这么一个篡改分支，这个篡改分支再也不可能和用户B所在的主分支合并，处在不同分支的用户之间的交互就可以揭露骗局，使得恶意服务器被发现。</p><p>简而言之，<strong>分支一致性通过密码学保证了日志一旦产生分支，这个分支就只能基于自己的日志内容增长，无法和其他分支合并。这使得所有用户只要是在同一分支中读日志，那么日志内容就是一致的。</strong></p><h2 id="样例场景"><a href="#样例场景" class="headerlink" title="样例场景"></a>样例场景</h2><p>多个开发者合作开发一个项目时，将代码文件都放在一个分布式文件系统中，有以下操作：</p><ol><li>开发者A修改了认证模块的代码文件auth.py，使得能登陆系统的用户范围缩减</li><li>开发者B知道认证模块修改完成后，修改了转账模块的代码文件bank.py，会给登陆系统的用户发钱</li><li>开发者C拉取代码并且在生产环境进行部署</li></ol><p>以上三个操作时间上顺序执行，如果存储服务器是安全可信的，那么这样的修改代码、部署代码理应没有问题。但是假设存储服务器是恶意的，服务器可以做两种程度的恶：</p><ul><li>返回任意内容：服务器直接给开发者C返回自己精心准备的有后门的代码，部署后黑客就可以利用后门攻破系统</li><li>选择性忽略修改(分支攻击)：在第3步中给开发者C只返回了开发者B对于bank.py的修改，但没有返回开发者A对于auth.py的修改，在部署这样的代码后，就会给不应收到钱的用户转账</li></ul><p>解决第一种返回任意内容的问题，可以在系统设计时要求用户对修改后的文件数字签名，用户读文件的时候需要检查签名的有效性，这样就保证了哪怕服务器是恶意的也不能随意返回内容。</p><p>解决第二种选择性忽略修改的问题，就需要达成分支一致性，使得开发者C如果看到了开发者B的修改，就说明两者在一个分支上，那么一定能看到在同一分支上更靠前的日志项记录的开发者A的修改。这就保证了恶意服务器无法选择性忽略修改。</p><h2 id="稻草人系统设计"><a href="#稻草人系统设计" class="headerlink" title="稻草人系统设计"></a>稻草人系统设计</h2><p>这一部分先给出为了达成分支一致性做的原始设计，具有较差的性能。首先SUNDR使用日志记录下文件系统的所有操作记录，每个日志项记录了一个操作，操作就是读操作(fetch)和写操作(modify)两种，存储服务器需要存下完整的日志，日志即是文件系统。用户在给日志添加日志项时需要签名，签名覆盖的内容不光是这个日志项中的内容，而是从日志开头到这个日志项的所有内容。</p><p><strong>因为所有日志项都有数字签名的需要，所以每个新用户都需要生成一对公私钥，管理员将用户的ID和公钥添加到这个共享文件系统的一个文件中，这样所有用户就可以知道其他用户的公钥，进而能够验证其他用户的数字签名的有效性。</strong>这也是后续升级系统中公钥的管理方式。</p><p>在稻草人系统中，客户端每个读写操作有以下流程：</p><ol><li>给文件系统加互斥锁，这使得操作全局有序</li><li>客户端找服务器下载所有日志，从头到尾校验日志项签名的合法性，因为每个日志项的签名的覆盖范围都是从开头到本日志项，经过检查后可以保证日志的连续性，没有任何空洞</li><li>客户端重放日志得到文件系统当前状态，进行读写操作，完成后将操作也作为日志项添加到日志中，再给整体日志进行签名，随后发送给服务器</li><li>释放锁</li></ol><p>读操作不会修改文件系统为什么也需要加入日志？<strong>SUNDR要求每个客户端记录下自己最近一次操作，在进行新的操作时，需要额外检查日志中是否有自己最近一次操作，没有的话就说明自己的分支被恶意服务器切换或者本次选择的服务器太旧了</strong>。所以<strong>如果不把读操作加入日志，一个只读的客户端就无法发现恶意服务器给自己切换分支，也不能发现服务器过时。</strong></p><p>在这样的设计中，恶意服务器无法返回任意内容，只能返回经过签名的日志项，由于每个签名覆盖了之前的所有日志项，所以无法返回有空洞的日志。恶意服务器能够做的攻击就是<strong>从某个日志项开始给不同的用户呈现出不同的日志</strong>，使他们在各自所看见的日志上工作，比如下图所示：<br><img src="/2023/10/15/%E5%88%86%E6%94%AF%E4%B8%80%E8%87%B4%E6%80%A7-SUNDR/%E5%9B%BE1.png" alt="图1" title="图1"><br>两个用户看到的日志从第4个日志项开始产生分叉，由于两个分支无法再合并(每个签名覆盖之前所有日志项)，所以在恶意服务器上两个分支将永远分开，A再也看不到B的操作，B再也看不到A的操作。</p><p>为了能够尽快检测出这样的攻击，可以<strong>引入用户间的通信(Gossip)，周期性交流自己最近一次操作的信息，就可以较快检测出分支攻击</strong>。比如上图用户B在后续日志中迟迟见不到用户A的最近一次操作，根据所知信息(比如日志项索引)就可以判断出自己被恶意服务器欺骗。<strong>为了实现用户间通信，可以在文件系统中维护若干文件，记录各个用户的登录、登出以及联络方式，用户就可以找其他在线用户进行信息交换。</strong></p><p>还有一个方法是<strong>安排可信的时间戳用户(timestamp box)，该用户拥有一个文件，每5秒该用户就对这个文件进行一次修改操作。其他用户就可以通过是否定期观察到时间戳用户的修改操作知道自己是否还在主分支上。</strong>如果观察不到，就说明自己要么被恶意服务器分支攻击了，要么时间戳用户宕机了。</p><h2 id="串行版SUNDR"><a href="#串行版SUNDR" class="headerlink" title="串行版SUNDR"></a>串行版SUNDR</h2><p>在稻草人系统中，有两个主要的性能问题：</p><ol><li>所有读写操作需要加全局锁，串行化，不允许并发操作</li><li>每次客户端请求需要下载全部日志，检查所有日志项，重放日志自行构建文件系统当前状态，既浪费带宽又耗时</li></ol><p>在本节中将针对第2个缺陷给出解决方案，但是所有操作仍需要加全局锁完全串行。为了避免每次请求下载所有日志，重建文件系统，SUNDR将摒弃“日志即文件系统”的设计，而是<strong>服务器维护了文件系统的状态，具体的元数据、数据块都通过签名、哈希等密码学手段保证服务器无法篡改数据，同时引入版本向量来记录每个用户、群组最后一次操作的版本来避免分支攻击</strong>。这样就可以直接读写数据块，而避免了下载、检查、重放日志的过程。</p><h3 id="数据结构"><a href="#数据结构" class="headerlink" title="数据结构"></a>数据结构</h3><p>为了避免服务器随意篡改数据，SUNDR设计了下图所示的数据结构来存储数据。<br><img src="/2023/10/15/%E5%88%86%E6%94%AF%E4%B8%80%E8%87%B4%E6%80%A7-SUNDR/%E5%9B%BE2.png" alt="图2" title="图2"><br>在SUNDR文件系统中，真正的存储可以看作一个kv存储，一个数据结构的内容是value的话，它的哈希值就是key，所以可以通过任意数据结构的哈希值找到该数据结构。<strong>之所以要存哈希值和数据内容两部分，就是为了防止恶意服务器随意篡改数据，通过哈希值可以检测出来。</strong></p><p>所有文件、目录都有自己的inode。文件inode中(例如上图$i_4$)存了元数据以及所有数据块的哈希值，通过数据块的哈希值可以真正读到数据块。目录inode中(例如上图$i_6$)存了目录下所有文件名到拥有它的用户、群组的i-table相应元素的映射，比如上图“locore.S”就是用户$u_2$的第5个文件，“main.c”就是群组$g$的第4个文件。</p><p>为了进行权限控制，即拥有相应权限的用户、群组才可以读写相应的文件。SUNDR中有用户(user)和群组(group)两个权限控制单元，一个用户对应了一个i-handle，是用户i-table内容的哈希值，一个群组也对应了一个i-handle，是群组i-table内容的哈希值。用户i-table中存了所有他拥有的文件inode的哈希值，通过这些哈希值就可以找到对应文件的inode。群组i-table中存每个元素也是代表了群组拥有权限的文件，只不过更像是二级索引，存的是该文件是哪个用户i-table的第几个元素，比如上图中群组$g$的第3个文件就指向了$u_2$的第4个文件，这样就可以间接访问到数据。一个用户不光可以使用自己的i-handle访问有权限的文件，还可以使用自己所在群组的i-handle访问有权限的文件。</p><p>需要注意的是由于数据内容的变化会使得它的哈希值发生变化，那么由于写操作一个文件数据块的变更就会连锁的导致引用它的数据结构的哈希值发生变化。SUNDR将i-table设计成B树，每个节点的哈希值变化一直向上影响到根节点的哈希值变化，i-handle就是根节点的哈希值。所以用户、群组的i-handle不是一成不变的。<strong>一个用户i-handle的变化表明其下所有文件内容存在变化！所以i-handle本质上是对文件系统当前状态快照的哈希。</strong></p><h3 id="版本向量"><a href="#版本向量" class="headerlink" title="版本向量"></a>版本向量</h3><p>数据结构中利用对数据结构内容求哈希值来避免恶意服务器篡改数据内容，但还有个问题没解决，就是还没有达成分支一致性。为了解决这个问题，SUNDR引入了<strong>版本向量，记录文件系统中所用用户、群组最后一次操作的版本号</strong>，在客户端和服务器每一次操作交互时都需要获得版本向量并且修改版本向量。版本向量被存在版本结构体(version structure)中，会在每次客户端和服务器交互时传输，如下图所示。每个用户、群组都有自己的版本结构体，其中包括自己的i-handle、所属的所有群组的i-handle、版本向量、自己对该版本结构体的签名。<br><img src="/2023/10/15/%E5%88%86%E6%94%AF%E4%B8%80%E8%87%B4%E6%80%A7-SUNDR/%E5%9B%BE3.png" alt="图3" title="图3"><br>在串行版SUNDR中，每个客户端都保留了自己上次操作的版本结构体，客户端的每个读写操作有以下流程：</p><ol><li>给文件系统加互斥锁，这使得操作全局有序</li><li>客户端找服务器下载所有用户、群组的版本结构体，被称为VSL(version structure list)</li><li>客户端检查自己上次操作的版本结构体在VSL中</li><li>客户端给VSL中所有版本结构体进行排序，两个版本结构体的顺序性由其中中的版本向量$v_a$和$v_b$决定，$v_a\leq v_b$需要满足$v_a$中每一个版本号都小于等于$v_b$中对应的版本号。假如存在两个版本向量既没有$v_a\leq v_b$也没有$v_b\leq v_a$，那就说明<strong>发生了不一致</strong>！</li><li>客户端读写相应的数据块，如果是写操作修改数据块的话还需要重新计算哈希值，向上一直影响到所有引用它的i-handle，那么也需要在新的版本结构体中更新i-handle。</li><li>客户端需要将版本结构体中的版本向量更新为自己所见到的最大值，随后将自己的版本号+1，将自己所属的所有群组的版本号+1</li><li>客户端给修改好的版本结构体签名并且发给服务器</li><li>释放锁</li></ol><p>在这样的流程下，就保证了如果出现了下图这种分叉的情况，A和B的分支(4和5之后)将永远无法合并，互相通信就可以发现恶意服务器的分支攻击，进而达到了分支一致性。<br><img src="/2023/10/15/%E5%88%86%E6%94%AF%E4%B8%80%E8%87%B4%E6%80%A7-SUNDR/%E5%9B%BE4.png" alt="图4" title="图4"><br><strong>版本向量中每个用户、群组的版本号本质上是记录了用户、群组最后一次操作的编号。</strong>只用记录每个用户、群租的最后一次操作而不是像稻草人系统中的日志一样记录所有操作的原因就是，<strong>每个用户在执行新操作时都会检查自己上一次操作是否在VSL中，根据传递性就保证了自己所有的操作都已经被反映在文件系统中了。</strong></p><h2 id="并发版SUNDR"><a href="#并发版SUNDR" class="headerlink" title="并发版SUNDR"></a>并发版SUNDR</h2><p>现在还剩一个性能问题，就是每个操作都需要加全局锁来给操作进行全局排序，串行化使得性能大打折扣。现在的目标是让读操作仅在它要读的文件正在被写时才需要等待，进化为文件级别的读写锁。大概总结一下就是，SUNDR在处理请求的流程中添加了一步预修改，客户端把自己要执行的操作记录下来并签名然后发给服务器，诚实的服务器会把所有请求排序，给分配相应的版本号然后把所有在本请求之前的待处理请求发送给客户端，客户端会自行检查冲突，如果没有统一文件上的写操作，就可以直接读写，把结果再返回给服务器，否则就是等待冲突操作完成然后再进行自己的操作。</p>]]></content>
    
    
    <categories>
      
      <category>MIT6.824 分布式系统</category>
      
    </categories>
    
    
    <tags>
      
      <tag>读论文</tag>
      
      <tag>分支一致性</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>因果一致性 COPS</title>
    <link href="/2023/10/15/%E5%9B%A0%E6%9E%9C%E4%B8%80%E8%87%B4%E6%80%A7-COPS/"/>
    <url>/2023/10/15/%E5%9B%A0%E6%9E%9C%E4%B8%80%E8%87%B4%E6%80%A7-COPS/</url>
    
    <content type="html"><![CDATA[<p>论文标题为《Don’t Settle for Eventual: Scalable Causal Consistency for Wide-Area Storage with COPS》，发表于2011年，这篇论文主要介绍了一个满足因果一致性(Causal Consistency)并且支持冲突处理(Incorporating Convergent Conflict Handling)的存储系统COPS，以及支持读事务的升级版本COPS-GT。因果一致性是一个有趣的研究方向，但很少被真正应用到现实系统中(比如嵌套的因果关系如同一颗树，根结点的阻塞导致所有子树等待)。</p><h2 id="Causal-一致性"><a href="#Causal-一致性" class="headerlink" title="Causal+一致性"></a>Causal+一致性</h2><p>论文首先定义了Causal+一致性，即<strong>满足因果一致性并且支持冲突处理</strong>(例如最后写胜利)。<br>首先定义一下<strong>因果关系</strong>，整个系统被抽象为支持put和get两种操作，那么因果关系可以被归类为以下三种情况：</p><ul><li>同一个线程里假如操作a发生在操作b之前，那么有$a\rightarrow b$，即a为因，b为果。</li><li>如果一个get操作b读到了之前某个put操作a写入的值，同样有$a\rightarrow b$。</li><li>因果关系具有传递性，若$a\rightarrow b$且$b\rightarrow c$，那么有$a\rightarrow c$。</li></ul><p><img src="/2023/10/15/%E5%9B%A0%E6%9E%9C%E4%B8%80%E8%87%B4%E6%80%A7-COPS/%E5%9B%BE1.png" alt="图1" title="图1"><br>上图反映了三种情况对应的因果关系。因果关系其实是为了符合使用者的直觉，符合因果一致性其实是为了解决如下这样的问题。</p><blockquote><p>Client 1：</p><ol><li>put(photo) 上传了一张照片</li><li>put(album) 相册中添加了指向该照片的引用</li></ol><p>Client 2:</p><ol><li>get(album) 读取相册列表</li><li>get(photo) 读取引用指向的照片</li></ol><p>符合直觉的应该是Client 2如果成功读取了新的相册列表，那么一定可以读取到新上传的照片，所以就要求了Client 1两个put操作的顺序，第1步一定要发生在第2步之前。</p></blockquote><p>正如前文所述，Causal+一致性由因果一致性和冲突处理两部分组成，如下：</p><ul><li>因果一致性要求任何get操作读到的值<strong>遵循因果关系所定义的顺序</strong>，例如上图中put(x,4)必须是在put(y,2)之后发生，不存在client读到x&#x3D;4后又读到x&#x3D;1的情况。但因果关系其实只进行了<strong>局部排序</strong>，例如上图中put(x,3)和put(x,4)之间并不存在严格的先后关系，这就说明这两个操作是<strong>并发</strong>的，也就是<strong>冲突</strong>的，它们的先后关系是不明确的。</li><li>由于因果关系的局部排序带来的操作<strong>冲突</strong>，为了能给冲突操作定义出**全局排序(最终一致)<strong>，所以就有了Causal+一致性的第二部分——冲突收敛处理，最常见的冲突处理就是最后写者胜(虽然面对累加这种情况不适用)。COPS采用了Lamport时间戳解决这个问题，因为不同机器时间戳存在误差的问题，不能绝对相信机器生成的时间戳，所以使用Lamport时间戳，该时间戳由两部分组成</strong>&lt;逻辑时间戳，机器id&gt;<strong>，每个机器为新操作生成的逻辑时间戳是</strong>max(本机物理时间戳，所见过操作逻辑时间戳最大值+1)**，这就使得冲突操作根据Lamport时间戳可以分出先后，并且物理时间走的慢的机器可以通过见到的最大逻辑时间戳跟上时间走的快的机器。</li></ul><p>至此，论文标题中的<strong>可扩展性</strong>也就浮现出来了，该设计并不像很多一致性设计一样，需要一个包含所有分片(shard)的全局log日志(扩展性的瓶颈)给出所有操作的全局顺序，而是通过因果关系提供的局部排序外加冲突处理提供的补足排序，使得所有操作达到全局排序的目标。由于没有log日志和去中心化的设计，使得系统具有可扩展性，不需要发送、交换日志，也不需要一个中心节点调控。</p><h2 id="系统设计"><a href="#系统设计" class="headerlink" title="系统设计"></a>系统设计</h2><p><img src="/2023/10/15/%E5%9B%A0%E6%9E%9C%E4%B8%80%E8%87%B4%E6%80%A7-COPS/%E5%9B%BE2.png" alt="图2" title="图2"><br>COPS给客户端提供的接口就是简单的put和get，加强版COPS-GT则额外提供了支持多key读事务的get_trans。COPS的设计是为了多个数据中心之间的一致性，每个<strong>client在本地的数据中心进行读写</strong>，本地数据中心完成写操作后，会将写请求放在Repl Queue，有额外的后台线程异步地将请求发给其他数据中心进行同步。在一个数据中心内，每个值也会在多台机器上进行复制容错，预期网速很快，采用的是链式复制(Chain Replication)，但是COPS关心的是<strong>数据中心之间的复制</strong>，要确保符合Causal+一致性。</p><p>在存储节点中(上图中央蓝色部分)，COPS记录的每一个单元是&lt;key,value,version&gt;，key和value是KV存储的基本单元，version则是put该&lt;key,value&gt;时存储节点赋予它的版本号也就是之前介绍的Lamport时间戳。由于COPS-GT要支持更为复杂的读事务，所以在存储时要记录下多个版本，并且记录每个版本的所有依赖(因果关系所定义的)，因此COPS-GT为每个key记录的是一系列的&lt;value,version,deps&gt;。</p><h3 id="接口说明"><a href="#接口说明" class="headerlink" title="接口说明"></a>接口说明</h3><p>Client是客户端，Client Library是运行在客户端的基础库，用于和存储节点进行交互(put_after、get_by_vers)以及管理上下文状态(Client Context)，Client Library暴露给用户的接口就是简单的put、get、get_trans，分别用于写单个&lt;key,value&gt;、读单个key、事务读多个key。而这些用户操作在Client Library这里会进行一些丰富，以put_after和get_by_vers两个接口与存储节点进行交互，下面简单介绍一下put_after、get_by_vers和dep_check三个API：</p><ul><li>put_after被用于Client Library向本地数据中心写入&lt;key,value&gt;，同时被用于数据中心之间的同步。put_after的参数和返回形式为<code>⟨bool,vers⟩ ← put_after (key, val, [deps], nearest, vers=null)</code>，参数中的deps被COPS-GT用于传依赖(因果)关系，nearest被COPS用于传依赖关系，vers在数据中心之间同步时会被设置为分配的版本；返回中的vers则是客户端提交put操作后存储节点给该操作分配的版本号。</li><li>get_by_vers被用于读指定版本的&lt;key,value&gt;，其参数和返回形式为<code>⟨value, version, [deps]⟩ ← get_by_vers (key, version=LATEST)</code>，参数中的version字段只有在COPS-GT的读事务中被使用，其他情况默认是读最新版本；返回中的deps也是只有在COPS-GT中被使用，用于客户端上下文状态的维护。当读取指定version数据无法被满足时，将会阻塞等待直到数据版本得到满足。</li><li>dep_check被用于检查依赖关系是否满足，当进行put_after时，每个&lt;key,value&gt;被commit前都需要检查其deps(或者nearest)是否已经满足，这里就用到了dep_check，本质就是从本数据中心把该版本&lt;key,value&gt;每一个依赖的key(可能因为在不同分片存在不同的存储节点)读指定版本，看是否满足读结果版本&gt;&#x3D;指定的version，当所有检查完成，才可以进行一个新的&lt;key,value&gt;的commit。</li></ul><h3 id="客户端状态管理"><a href="#客户端状态管理" class="headerlink" title="客户端状态管理"></a>客户端状态管理</h3><p>从上文介绍因果关系可知，从一个线程中操作的先后关系会导致因果关系，所以在客户端需要维护一个上下文状态(Client Context)用于管理单线程中产生的因果关系。这一部分工作由Client Library完成，其中COPS和COPS-GT所需要管理的状态不太一样。</p><ul><li>先介绍COPS-GT，Client Context就是一张表，每个元素是一个&lt;key,version,deps&gt;元组，每当使用get读到一个&lt;key,value&gt;对，就向表中添加一个对应的元组，其中version、deps信息都是get_by_vers接口顺带返回的。当要进行put操作时，为了让存储节点知道该&lt;key,value&gt;依赖哪些前置的因，就需要将Client Context表中每一个key对应的最新版本以及其依赖构成的全集都加入到本次put的deps中，作为一个参数调用put_after接口传给存储节点，用于后续dep_check接口判断依赖性；随着put_after成功完成，会返回存储节点给本次put赋予的version，同样需要把&lt;key,version,deps&gt;加入到Client Context表中。总结一下就是，<strong>所有的读写操作都会以&lt;key,version,deps&gt;的形式存放在Client Context中</strong>，带来的无限增长的空间问题会在后文垃圾回收阶段给出解决方案。</li><li>COPS由于不需要支持读事务，所需维护的上下文状态就少很多，同样也是维护一张表，每个元素是一个&lt;key,version&gt;对，当get获得&lt;key,value&gt;时，会将相应的&lt;key,version&gt;加入表中；当进行put操作时，同样需要把Client Context中记录的所有&lt;key,version&gt;作为本次put的依赖以put_after借口的nearest参数传给存储节点，并在put_after返回得到版本后，把Client Context清空，存入返回的&lt;key,version&gt;。</li></ul><p>可以看出COPS的Client Context不维护每个版本key的依赖deps，同时每次put操作后都会清空已有context，这是<strong>利用了因果关系的传递性</strong>，如下图所示，z4依赖所有其他操作，但是在dep_check检查依赖性时，只需要检查y1和v6这两个最近依赖即可通过传递性检查到其他所有的依赖。所以put后清空Client Context的用意就是本次put已经依赖了前面所有操作，后续的put只需要依赖本次put就可以了。get获得的结果也要加入到Client Context中是因为类似于图1中$put(y,2)\rightarrow get(y,2)$这种跨线程的因果关系也需要考虑在内。<br><img src="/2023/10/15/%E5%9B%A0%E6%9E%9C%E4%B8%80%E8%87%B4%E6%80%A7-COPS/%E5%9B%BE3.png" alt="图3" title="图3"></p><h3 id="读事务"><a href="#读事务" class="headerlink" title="读事务"></a>读事务</h3><p>读事务相比于读单个key复杂在需要进行多个key在同一时刻的快照读，以下面这个情景为例。</p><blockquote><p>Client 1: get(ACL)&#x3D;”open” &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; get(album)<br>Client 2: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; put(ACL,”close”) &nbsp;&nbsp;&nbsp;&nbsp; put(album)</p><p>这个场景中，Client 1去获取其他用户的相册内容，首先进行权限验证，由于是open公开相册，所以有了进一步的获取相册内容。但是不巧的是，在两个get操作中间，相册的拥有者将相册改为私密，并且向相册里面添加了私密照片，这就导致了Client 1使用了旧的权限信息获得了新的不符合权限的照片。</p><p>读事务就是为了将多个get操作打包到同一个时间点一起完成，进而避免这种“前朝的剑斩现朝的官”的问题。</p></blockquote><p>为了在同一时刻读取多个key，这就要求了&lt;key,value&gt;的<strong>多版本存储</strong>(是不是挺像MVCC)，这也就是为什么COPS-GT要求存储节点存储键值对的多个版本。下面介绍COPS-GT是如何利用两轮get_by_vers完成读事务get_trans。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_trans</span>(<span class="hljs-params">keys</span>):<br><span class="hljs-comment"># 第一轮，获取所有key的最新版本</span><br>    <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> keys<br>results[k] = get_by_vers(k, LATEST)<br><span class="hljs-comment"># 根据因果性，计算出每个key所需的版本(存在ccv中)，以最高的为准</span><br><span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> keys<br>ccv[k] = <span class="hljs-built_in">max</span>(ccv[k], results[k].vers) <br><span class="hljs-keyword">for</span> dep <span class="hljs-keyword">in</span> results[k].deps <span class="hljs-comment"># 更新所有依赖的key的version到最新</span><br><span class="hljs-keyword">if</span> dep.key <span class="hljs-keyword">in</span> keys<br>ccv[dep.key] = <span class="hljs-built_in">max</span>(ccv[dep.key], dep.vers)<br><br><span class="hljs-comment"># 第二轮，根据第一轮算出的每个key所需的版本，获取指定版本</span><br><span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> keys<br><span class="hljs-keyword">if</span> ccv[k] &gt; results[k].vers<br>results[k] = get_by_vers(k, ccv[k])<br><br><span class="hljs-comment"># 收尾，更新结果到Client Context中，返回事务结果</span><br>update_context(results, ctx_id)<br><span class="hljs-keyword">return</span> extract_values(results)<br></code></pre></td></tr></table></figure><p>到这里就可以解释<strong>为什么COPS-GT需要记录下历史所有操作结果在Client Context中</strong>？假设在某个线程中按顺序发生了put(x2)，put(y2)，put(z2)三个操作(这里下标假设是key的版本version)，那么z2是依赖x2和y2的，但在实行COPS的最近依赖规则后，z2只需要依赖y2。假设三个值正在数据中心之间进行同步，被同步的数据中心正在进行一次读事务。在第一轮中，先读到x1，随后同步的dep_check检查完成$x_2\rightarrow y_2 \rightarrow z_2$被依次commit，随后读到了z2。由于z2只依赖了y2，所以不会对x1版本仅为1产生质疑，就没有了读事务的第二轮，这导致了读到x1和z2这组不符合因果一致性的结果，所以这就要求了能够支持读事务的COPS-GT不可以将all deps省略为nearest deps。</p><h3 id="垃圾回收"><a href="#垃圾回收" class="headerlink" title="垃圾回收"></a>垃圾回收</h3><h4 id="旧版本数据回收"><a href="#旧版本数据回收" class="headerlink" title="旧版本数据回收"></a>旧版本数据回收</h4><p>因为COPS-GT需要支持读事务，所以需要保存一些旧版本数据，但永久保留将会导致存储节点存储需求无穷上涨，所以需要确定旧版本数据何时可以删除。COPS-GT规定了读事务的超时时间(5秒)，所以一个旧版本数据在被存放超过5秒后，需要它的读事务已经完成或超时结束，后续的读事务使用新版本即可，那么该旧版本数据就可以被清除了。</p><h4 id="存储节点依赖关系回收"><a href="#存储节点依赖关系回收" class="headerlink" title="存储节点依赖关系回收"></a>存储节点依赖关系回收</h4><p>因为COPS-GT支持读事务，第一轮需要返回每个key的deps，所以在存储节点需要记录下每个key的依赖项deps。何时这个依赖项可以被删除呢？COPS-GT规定假设z2是依赖x2的，当z2被所有数据中心commit了并且持续了读事务超时时间(5秒)即可将依赖项删除。</p><ul><li>首先解释为什么需要满足z2被所有数据中心commit，假设数据中心A和B断连，A中已经把z2依赖x2这一信息删除，那么当数据中心A和B重连后，在数据中心B中z2直接可以commit无需等待x2，这时候如果发生了读事务读取x、z就会产生不一致。但是这样的限制会导致当产生网络分区时，该条件始终无法达成。</li><li>再解释一下为什么需要超过5秒才可以删除，假设读事务第一轮读了x1，随后完成$x_2 \rightarrow z_2$被依次commit，并且删除了依赖关系，接着读到了z2。由于没有了$x_2 \rightarrow z_2$的依赖关系，所以不会对x1版本仅为1产生质疑，就没有了读事务的第二轮，这导致了读到x1和z2这组不符合因果一致性的结果。等待5秒是为了保证第一轮读到x1、z2的读事务在整个事务时间范围内可以看到$x_2 \rightarrow z_2$的依赖关系，从而可以第二轮更新x1。</li></ul><h4 id="Client-Context回收"><a href="#Client-Context回收" class="headerlink" title="Client Context回收"></a>Client Context回收</h4><p>当存储节点依赖关系被回收时，会给一个never-depend的标记，这个标记会通过get_by_vers操作返回给Client Library，通过这个标记，就可以将Client Context中该key的所有依赖都从表中删掉，因为never-depend标记也根据因果关系先后发生，所以可以被安全删除。COPS不同数据中心的存储节点会每隔一段时间商议出一个全局检查点时间，代表每个数据中心都一定走过了这个版本，那么比这个全局检查点时间还旧的版本数据就可以从Client Context中删除了。</p>]]></content>
    
    
    <categories>
      
      <category>MIT6.824 分布式系统</category>
      
    </categories>
    
    
    <tags>
      
      <tag>读论文</tag>
      
      <tag>因果一致性</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>高并发缓存架构 Memcached</title>
    <link href="/2023/10/15/%E9%AB%98%E5%B9%B6%E5%8F%91%E7%BC%93%E5%AD%98%E6%9E%B6%E6%9E%84-Memcached/"/>
    <url>/2023/10/15/%E9%AB%98%E5%B9%B6%E5%8F%91%E7%BC%93%E5%AD%98%E6%9E%B6%E6%9E%84-Memcached/</url>
    
    <content type="html"><![CDATA[<p>论文标题为《Scaling Memcache at Facebook》，发表于2013年，这篇论文主要介绍了Facebook应对高并发的缓存架构Memcached，该缓存架构是经过现实的高并发场景验证过的，同时可以满足最终一致性和“写后读”一致性。</p><h2 id="接口定义"><a href="#接口定义" class="headerlink" title="接口定义"></a>接口定义</h2><p>Memcache对外暴露三个简单的接口get(k)、set(k,v)、delete(k)，使用示例见下图，分别对应客户端(web server)读、写两个场景。</p><ul><li>当客户端需要得到一个key对应的value时，首先会尝试从缓存get，成果获取则直接返回；若缓存中get失败，才真正去访问持久化存储的DB，并且将得到的结果(可能会进一步加工)set到缓存中，方便之后读取，避免频繁访问持久化存储DB。</li><li>当客户端需要修改DB中的内容时，首先会向DB发送修改请求，完成后计算出影响到的&lt;key,value&gt;对，然后将对应的过期&lt;key,value&gt;从缓存中delete掉。</li></ul><p><img src="/2023/10/15/%E9%AB%98%E5%B9%B6%E5%8F%91%E7%BC%93%E5%AD%98%E6%9E%B6%E6%9E%84-Memcached/%E5%9B%BE1.png" alt="图1" title="图1"></p><h2 id="整体架构"><a href="#整体架构" class="headerlink" title="整体架构"></a>整体架构</h2><p><img src="/2023/10/15/%E9%AB%98%E5%B9%B6%E5%8F%91%E7%BC%93%E5%AD%98%E6%9E%B6%E6%9E%84-Memcached/%E5%9B%BE2.png" alt="图2" title="图2"><br>Facebook服务端整体架构如上图所示，首先整体上分为两个Region(比如西海岸、东海岸)，Region是地理上的远距离分布，所以中间的网络延迟最优情况下也只能是距离除以光速，所以让东海岸用户访问西海岸的服务器是很耗时的，拆分Region的目的就是为了让<strong>用户就近访问</strong>。但并不是在东海岸和西海岸分别设立数据中心提供服务就可以解决问题了，因为两岸的用户也是有联系的，并不是直接隔离开的，所以需要共用相同的持久化存储内容，这就带来了一致性的问题，Facebook面对这个问题的解决办法还是简单直接的，直接将Region区分出Master和Slave，这种<strong>主从结构</strong>保证了从Region存储集群完全接受主Region存储集群的拷贝(上图最下面的箭头)，自己不具备写存储的能力。</p><p>在每个Region中，最底层有一个持久化存储集群(在Facebook中是Mysql数据库)，其上是若干个Front-End集群，每个Front-End集群中是一系列无状态的Web服务器和一系列Memcache服务器(经过分片，比如三个Memcache服务器分别负责A-H，I-P，Q-Z的分片)，Memcache的存在可以拦截掉许多Web服务器访问存储集群的请求，并且Memcache服务器将数据存放在内存中，读取性能是远高于读持久化存储的。</p><p>既然Web服务器无状态，可以直接扩容，<strong>为什么一个Region中要分出那么多Front-End集群？</strong>Facebook是这么考量的：</p><ul><li>由于扩容了Web服务器，那么原本Memcache服务器的压力将会增大，所以也需要同比例扩容，由前面描述可知每个Memcache服务器有自己负责的分片，扩了Memcache服务器的数量会让每个Memcache服务器负责的分片key范围变小(比如原来负责A-H，现在负责A-C)，但是在缓存系统中经常遇到热key的问题，大量的请求集中在少数key上，这会导致扩容的Web服务器对于热key请求量更高，但是负责热key的Memcache服务器还是只有一台，并没有跟着扩机器数量而扩容。所以Facebook选择了分出多个Front-end集群，在每个集群中都有自己的Memcache服务器冗余负责一些分片，来<strong>达到有多个备份在不同集群中处理热key的效果，虽然这会带来一些一致性上的麻烦</strong>。</li><li>还有一点就是假如分片很细碎，由于经常一个页面加载需要访问很多key，细碎的分片导致Web服务器需要给很多个分片的Memcache服务器发送请求，无法批量请求，所以大范围的分片更有利于批量请求，并且减少了网络拥塞的情况。</li><li>最后就是划分出一个又一个集群，每个集群有自己的比如网络相关的配置，当遇到故障的时候，可以最小化影响范围，不会导致全Region的失效。</li></ul><h2 id="Region内设计"><a href="#Region内设计" class="headerlink" title="Region内设计"></a>Region内设计</h2><h3 id="缓存一致性"><a href="#缓存一致性" class="headerlink" title="缓存一致性"></a>缓存一致性</h3><p>正如刚才所说，Facebook Memcached的架构设计中，每个Front-End集群中都有相应的冗余Memcache服务器维护相同的分片缓存，既然有了冗余，就会带来一致性的问题。如图1右图所示，当某个Front-End集群中的某个Web服务器完成了一次对持久化存储的修改后，可以通过delete操作将自己集群中Memcache服务器的对应&lt;key,value&gt;设置为失效，但是如何让其他集群中Memcache服务器的对应&lt;key,value&gt;失效？广播的话会导致Web服务器的负载加重，并且加重整个网络中的负载。<br><img src="/2023/10/15/%E9%AB%98%E5%B9%B6%E5%8F%91%E7%BC%93%E5%AD%98%E6%9E%B6%E6%9E%84-Memcached/%E5%9B%BE3.png" alt="图3" title="图3"><br>如上图所示，Memcached的设计中将所有Mysql更新操作以Commit Log的形式记录下来，存储服务器中有一个专门的McSqueal线程解析Commit Log，将对应的delete请求通过Mcrouter路由到关联的Memcache服务器上(更像是批处理)，来达到使对应&lt;key,value&gt;失效的目的，这个解决方案显然仅仅是<strong>最终一致性</strong>，过期缓存的失效有一定的延后性，但对于Facebook来说是可以接受的。</p><h3 id="租约"><a href="#租约" class="headerlink" title="租约"></a>租约</h3><p>Memcached用租约机制巧妙的解决了set过期数据、惊群效应两个问题，分别考虑以下两个情况：</p><blockquote><ol><li>Client 1从缓存get(k)失败，去找持久化存储get(k)&#x3D;v1</li><li>Client 2向持久化存储写入&lt;k,v2&gt;，并在缓存中进行了delete(k)</li><li>Client 1向缓存set(k,v1)        &#x2F;&#x2F;设置了过期数据，并且有可能永久驻留，违反最终一致性！</li></ol></blockquote><blockquote><ol><li>一系列Client同时向缓存请求get(k)，都miss了</li><li>这些Client接着一起转向持久化存储进行get(k)           &#x2F;&#x2F;惊群效应！</li></ol></blockquote><p>当Client访问缓存miss的时候，Memcached会给Client一个租约(lease)，在规定时间内Client可以凭租约将从持久化存储获得的数据设置到缓存中，当对一个key发出租约后，后续的miss请求会被通知等待一会儿再来试(已经有人替你去访问持久化存储啦)，通过这样的拦截可以有效解决惊群效应。再看租约如何解决set过期数据的问题，对一个key的delete操作会令该key发出的租约失效，这样之前拿到租约的Client就无法在之后使用旧租约set数据了，这样就避免了set过期数据，保证了最终一致性！</p><h3 id="冷启动"><a href="#冷启动" class="headerlink" title="冷启动"></a>冷启动</h3><p>假如在一个Region内新创建了一个Front-End集群，刚开始时该集群内的Memcache服务器都是空的，访问这个集群的请求将会直接去读DB，激增的请求量很可能把DB打垮，所以这种情况是不应该出现的。为了快速使冷缓存追上Region内其他集群的热缓存，处于这个热身(Warmup)阶段的集群，允许Web服务器在本集群缓存访问失败后，将请求转发给隔壁集群的热缓存，并将结果也set到本集群的冷缓存中，这样尽可能避免了直接访问DB，并且让冷缓存可以慢慢追上热缓存。<br>但这样的设计会破坏最终一致性，比如这样的情况：</p><blockquote><ol><li>初始时冷集群和热集群缓存中都存了&lt;k,v1&gt;</li><li>冷集群中的Web服务器修改了持久化存储为&lt;k,v2&gt;，所以delete掉了缓存中的&lt;k,v1&gt;</li><li>冷集群接到一个新请求get(k)，因为处于热身阶段，自己的缓存没有找到就去隔壁热集群get(k)，假设热集群因为缓存同步的延后性还没有接到delete请求，所以返回了&lt;k,v1&gt;，冷集群会返回&lt;k,v1&gt;并且(可能无休止的)存在自己的缓存里，违反最终一致性！</li></ol></blockquote><p>为了避免跨集群读取+缓存同步延后性叠加产生的永久不一致，Memcached规定处于热身阶段的集群假如delete掉缓存中的&lt;k,v&gt;对，那么在之后2s内不能进行从隔壁热集群拷贝过来&lt;k,v&gt;对的操作，Facebook预期缓存同步的延迟不会超过2s，所以可以保证本集群接受的delete请求一定已经同步到了隔壁集群，避免了永久不一致的发生。</p><h3 id="故障恢复"><a href="#故障恢复" class="headerlink" title="故障恢复"></a>故障恢复</h3><p>假如Memcache服务器崩溃、掉线，该集群指向该服务器的请求都将超时失败，这些请求会被转发到DB导致请求量激增。为了避免这种情况，Memcached分配了一些命名为Gutter的服务器，维护起一个能存所有key范围的缓存池，这个Gutter缓存池就像是兜底条款一样。当Web服务器发现有些Memcache服务器掉线后，会将这部分请求转发给Gutter缓存池，它的存在可以拦截很多转向DB的请求，作为最后一道防线，之后就需要等待故障Memcache服务器恢复即可。</p><h2 id="Region间一致性"><a href="#Region间一致性" class="headerlink" title="Region间一致性"></a>Region间一致性</h2><p>Region间的一致性其实是依靠图2所示的存储集群(Mysql数据库)进行主从复制，从Region接到诸如binlog这种包含了更新操作的日志时，也会更新到自己的持久化存储集群，也同样有图3所示的McSqueal线程将过期缓存delete掉。这种主从架构是简单易于理解的，但是会牺牲写的性能，因为只有主Region可以受理写请求，所以东海岸的用户也需要把写请求发送到西海岸的主Region，并等待Region之间的同步，可以使自己从当地Region读到自己的写结果。但这样的设计也存在违背“写后读”一致性的情况：</p><blockquote><ol><li>属于从Region的用户发起写请求给到主Region，主Region完成请求后返回成功，从Region会delete掉相应缓存</li><li>用户发现写成功，发起读请求到本地从Region读刚才自己写的结果，假设主从同步还未完成，用户访问缓存失败后从持久化存储读到的是之前的老数据，违背了“写后读”一致性！</li></ol></blockquote><p>为了避免远方写+本地读导致的“写后读”不一致，Memcached规定从Region的用户发起写请求到主Region成功后，需要在从Region中delete掉相应缓存并且加上“remote”标记，当用户接着本地读的时候会从缓存发现标记，随后将读请求转交给主Region来获得最新的数据，进而满足“写后读”一致性。“remote”标记则会在主从同步完成后被McSqueal线程清理掉。</p><h2 id="优化手段"><a href="#优化手段" class="headerlink" title="优化手段"></a>优化手段</h2><ul><li>批量并行请求缓存，Web服务器整理出所有需要的&lt;key,value&gt;，并行的批量的发给对应的Memcache服务器。</li><li>使用UDP(不需要创建连接，无序、快速)进行get操作，TCP(需要创建连接，有序、可靠)进行set操作，在进行set操作时，Web服务器将请求转发给路由服务Mcrouter，该服务专职跟Memcache服务器交互，可以复用TCP连接。</li><li>对不同类型的key进行调研，比如&lt;key,value&gt;大小，使用频繁程度，划分出不同种类的缓存池，特定key走特定池。同时有些非热key没必要在多个Front-End集群缓存中冗余，所以提供一个Region级别的缓存池存放这些key(只存一份，节省内存)，可以给所有Front-End集群共用。</li></ul>]]></content>
    
    
    <categories>
      
      <category>MIT6.824 分布式系统</category>
      
    </categories>
    
    
    <tags>
      
      <tag>读论文</tag>
      
      <tag>缓存一致性</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>分布式事务 FaRM</title>
    <link href="/2023/10/15/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1-FaRM/"/>
    <url>/2023/10/15/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1-FaRM/</url>
    
    <content type="html"><![CDATA[<p>论文标题为《No compromises: distributed transactions with consistency, availability, and performance》，发表于2015年，这篇论文主要介绍了Microsoft的一个研究模型FaRM，利用两种特殊硬件，以及两阶段提交2PC和乐观并发控制OCC的结合来完成高性能的分布式事务，整个设计适用于单个数据中心，并发事务较少的情况。</p><h2 id="特殊硬件"><a href="#特殊硬件" class="headerlink" title="特殊硬件"></a>特殊硬件</h2><h3 id="非易失性内存-Non-volatile-DRAM"><a href="#非易失性内存-Non-volatile-DRAM" class="headerlink" title="非易失性内存(Non-volatile DRAM)"></a>非易失性内存(Non-volatile DRAM)</h3><p>FaRM之所以快，很大程度上与它使用了非易失性内存NVRAM有关，传统的存储系统在完成读、写操作时经常需要和硬盘打交道，但是硬盘相比于内存慢了很多。所以FaRM选择直接不用硬盘，而是用内存提供读写服务，但是内存的缺陷就在于断电重启之后存储内容清空，所以FaRM设计了内存的供能系统，与机器分开独立供电。当集群或者机器断电时，NVRAM的电源可以再支撑一段时间，在这段时间里会把NVRAM中的数据转储到硬盘之中，方便之后恢复，以此达到非易失性内存的目标。<strong>NVRAM解决了硬盘写性能这个瓶颈</strong>。</p><h3 id="RDMA-Remote-Direct-Memory-Access-网卡"><a href="#RDMA-Remote-Direct-Memory-Access-网卡" class="headerlink" title="RDMA(Remote Direct Memory Access)网卡"></a>RDMA(Remote Direct Memory Access)网卡</h3><p>通常内部系统之间的网络交互都使用RPC调用，RPC调用需要CPU参与，经过TCP等一系列协议栈从网卡NIC发出去，整个过程由一系列的系统调用、消息复制、中断，并且在接收方收到时也需要同样的逆操作，导致整体性能差。为了解决网络IO的性能问题，FaRM使用了kernel bypass技术和RDMA网卡，kernel bypass允许应用程序直接和网卡进行交互，没有中间这些协议栈以及系统调用，RDMA网卡则允许直接访问远端内存，无需远端CPU参与，这是单边的通信协议，不需要双边参与，既加快了访问速度也减少了双方的CPU消耗。<strong>RDMA解决了网络IO性能这个瓶颈</strong>。</p><h2 id="整体架构"><a href="#整体架构" class="headerlink" title="整体架构"></a>整体架构</h2><p><img src="/2023/10/15/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1-FaRM/%E5%9B%BE1.png" alt="图1" title="图1"><br>一个FaRM集群由一台CM(configuration manager，上图Machine D，很像HDFS中Master的角色)和若干台提供读写服务的机器(上图Machine A、B、C，很像Slave的角色)。CM负责了整个集群Region的管理，比如一个Region在哪些机器上存着，它的主节点是哪台机器，CM承担起了整个集群的元数据管理职责。可以从图中右下部分看到FaRM还利用Zookeeper管理了整个集群的配置信息，包括一个单调递增的配置id、集群中的机器、集群中的CM，Zookeeper是用于整个集群基本信息的管理，在配置稳定的情况下本身不参与集群活动，很像是Zookeeper在HDFS集群中的角色。</p><p>FaRM对外暴露的是一个很大的全局地址空间，外部应用可以将FaRM看作一个有极大地址空间的内存。为了管理这么大的地址空间，FaRM把地址空间每2GB拆出来一个Region，每一个Region会在f+1台机器上进行冗余，所以正常情况下，每个Region会有1台主节点和f台备份从节点，可以达到容错f台机器故障的效果。Region的复制完全是主从复制，在主节点健康的情况下，将仅仅有主节点提供该Region的读写服务。在FaRM的设计中，一台机器的NVRAM上可以存250个Region，有些是自己做主节点，有些不是，可以达到整体的负载均衡。</p><p>当需要新建一个Region的时候，会是一个2PC(两阶段提交)的过程，CM会充当协调者(Coordinator)的身份，首先根据不同机架等容错相关的角度以及负载均衡的考量选出f+1台机器存新的Region，并且决定主节点。之后便开启了2PC的过程，首先向f+1台机器发送prepare消息，让他们做好存新Region的准备，当收到所有f+1台机器肯定的回复后再发送commit消息并且建立起新的Region id到机器的映射。</p><p>CM以外的每台机器除了要负责Region的本地读写、远端读以外，还需要专门预留一片空间存放其他机器发来的事务记录日志、消息，这一部分内存的写直接由远端机器RDMA写，RDMA网卡本身会有自己的ACK消息返回，不需要本机器的CPU参与，本机器只需要有个线程定期扫这些日志、队列进行处理即可，对于远端发送方来说是一个异步的过程。</p><h2 id="分布式事务"><a href="#分布式事务" class="headerlink" title="分布式事务"></a>分布式事务</h2><p>FaRM的分布式事务是2PC+乐观并发控制(OCC，Optimistic Concurrency Control)的思想，OCC的核心在于事务使用的对象不加锁，整个过程不写对象，直到最后验证要写的、要读的对象全部和开始时一样，才会写对象并且提交。与悲观并发控制的不同就在于过程中尽可能不加锁，FaRM整体上是OCC的思想，虽然还是对事务要写的对象上了锁，因为毕竟分布式事务的写操作外加上之后的2PC并不是原子的，所以需要加锁，但不给只读对象上锁，在FaRM的分布式事务设计中有多次远端读，这可以充分的利用RDMA网卡，所以可以达到很好的性能。</p><p>正如刚才描述的OCC，本质思想其实是Compare and Set，与原来一样那就代表没冲突，就可以写。所以FaRM为每个存储对象都赋予了64bit的单调递增版本号，用于比较对象前后是否被修改过，同样为了满足FaRM锁待修改对象的需要，版本号的最高位用于标示该对象是否正在一个事务中等待被修改。<br><img src="/2023/10/15/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1-FaRM/%E5%9B%BE2.png" alt="图2 事务流程图" title="图2 事务流程图"><br><img src="/2023/10/15/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1-FaRM/%E5%9B%BE3.png" alt="图3 事务日志记录类别表" title="图3 事务日志记录类别表"><br>整个分布式事务流程是一个2PC+OCC的过程，应用层的分布式事务发起者会充当协调者的身份。FaRM采用的是预写策略，在Execute Phase，协调者会把事务需要的所有对象远端读过来，然后在本机完成整个事务需要的修改，这里就已经把事务结果预处理并缓存好了。接下来便是事务提交Commit Phase的过程：</p><ol><li>首先协调者会向所有待修改对象所在Region的主节点发送LOCK记录，记录中包含了事务id，事务所修改的所有Region的id，待修改对象的地址、版本、值。各个Region的主节点处理到这个记录时，会尝试原子的(compare and set)将自己负责的对应版本的对象锁起来，假如没有全部成功，那就解锁返回失败，否则返回成功。这一步其实是OCC验证了待修改对象的版本还和之前一样，至于上锁是为了防止后续这段处理时间中突然被改了，这保证了多个事务之间的顺序。同时这一步也做了2PC第一阶段的准备工作。</li><li>当第1步收到所有待修改Region主节点的成功返回后，就会开始OCC验证只读对象的版本是否和之前一样。所以这一步直接远端读只读对象对比即可，但凡版本不一样或者被加了锁(同样会导致版本号不一样)，都会使得整个事物失败。这一步结束后其实就将事务涉及的所有对象的OCC检查都完成了。</li><li>这一步协调者会向所有待修改Region的备份节点发送COMMIT-BACKUP记录，这个记录的内容和第一步发给主节点的LOCK记录其实是一样的，与第一步不同的是，这一步只管把记录发到(收到了对面RDMA网卡的ACK)，不用管是否记录已经被处理。这一步完成后，所有的备份节点就已经持久化了事务的提交记录，之后会被处理更新到具体对象上。这一步像是对于备份节点2PC准备、提交的二合一，因为备份节点不需要考虑锁的问题，也没有啥准备工作，直接提交就可以了。</li><li>这一步就是2PC的第二阶段——提交，协调者会发送COMMIT-PRIMARY记录给所有待修改Region的主节点，只要有其中一个主节点ACK了协调者的RDMA写，该记录不需要被真正处理，协调者就可以认为此次事务已经完成提交，最终一定会被更新到存储内容中，因此就可以向上层客户端返回成功了。之所以可以如此自信，是因为COMMIT-PRIMARY记录只要被送达到一个待修改Region的主节点那里持久化在事务日志里，那么该Region所有主备节点都持久化了COMMIT-BACKUP或者COMMIT-PRIMARY记录，已经可以容忍f台机器故障，所以保证了事务提交这个决定的持久化。这一步就完成了2PC的第二阶段，当然记录的处理是异步的，真正反映到修改的对象上需要等异步线程完成处理，以及增加版本号、释放相关的锁。</li><li>这一步就是收尾工作了，跟整个事务的正确性其实关系不大，当所有COMMIT记录都确保送到之后，会懒发送TRUNCATE记录，表明哪些事务id的记录项可以被删掉，用于维护服务器的事务日志占的大小。</li></ol><p>补充一个例子，可以充分说明VALIDATION的作用。</p><blockquote><p>x、y初始为0<br>事务T1:<br>&nbsp;&nbsp;if x &#x3D;&#x3D; 0:<br>&nbsp;&nbsp;&nbsp;&nbsp;y &#x3D; 1<br>事务T2:<br>&nbsp;&nbsp;if y &#x3D;&#x3D; 0:<br>&nbsp;&nbsp;&nbsp;&nbsp;x &#x3D; 1<br>两个事务的并发结果(0,1)、(1,0)或者双双终止得到(0,0)都是可以接受的，但是(1,1)是错误的。没有对只读对象进行VALIDATION就可能导致(1,1)的错误结果！</p></blockquote><h2 id="故障恢复"><a href="#故障恢复" class="headerlink" title="故障恢复"></a>故障恢复</h2><h3 id="故障检测"><a href="#故障检测" class="headerlink" title="故障检测"></a>故障检测</h3><p>FaRM利用租约(其实也是心跳)检测机器故障，CM赋予其他所有机器租约，同时其他机器也都赋予CM租约，每隔一小段时间就需要更新租约，来保证感知机器的存活状态。</p><h3 id="配置更新"><a href="#配置更新" class="headerlink" title="配置更新"></a>配置更新</h3><p>当有机器宕机、掉线的时候，就需要找Zookeeper修改集群配置信息了，期间还会涉及Region的重新分配。正如上文描述，FaRM的故障检测本质上是心跳检测，当CM发现某个机器失去心跳的时候会发起配置更新的流程；当普通机器发现CM失去心跳的时候，它首先会联系CM备份机去进行配置更新，假如过了一长段时间并没有配置更新，那么该机器就会自己启动配置更新的流程，并任命自己为CM。下面根据配置更新流程图介绍整个流程。<br><img src="/2023/10/15/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1-FaRM/%E5%9B%BE4.png" alt="图4" title="图4"></p><ol><li>首先，CM根据心跳检查察觉哪些机器宕机了，这一步只是怀疑。</li><li>为了调查有多少机器宕机，CM向所有非怀疑机器发读请求，得到存活的机器列表。</li><li>CM根据第2步的结果将新的配置信息提交给Zookeeper，完成配置信息的更新。</li><li>CM把之前宕机的机器负责的Region重新安排够f+1台冗余，假如宕机的是主节点，就提拔一个备份节点当主节点。这一步完成后CM就不会再从宕机节点读数据了。</li><li>CM将新配置信息NEW-CONFIG发给每台机器。</li><li>非CM机器处理到新配置信息时，会停止从宕机节点读写数据，并且开辟空间承接自己要新负责的Region，屏蔽外部请求，并且回复CM，假如CM发生了变更，则会跟新的CM进行心跳检查。</li><li>CM收到所有机器的回复后，等到前一个配置所有租约都过期，会向所有机器下发提交新配置的命令NEW-CONFIG-COMMIT，所有机器接到后就可以开始处理外部请求了。至此完成配置更新。</li></ol><h3 id="事务恢复"><a href="#事务恢复" class="headerlink" title="事务恢复"></a>事务恢复</h3><p><img src="/2023/10/15/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1-FaRM/%E5%9B%BE5.png" alt="图5" title="图5"></p><ol><li><p>事务恢复和之前的配置更新有穿插的部分。当某个Region的主节点挂掉了，新的被提拔为主节点的备份节点还不能立马开始提供读写服务，需要将所有状态(包括事务的锁)都恢复的和挂掉的主节点一样，所以需要先屏蔽所有请求。</p></li><li><p>当机器接到了新配置提交的命令后，会把机器上所有的已有未处理的日志处理完，这一步是所有机器互不干扰在自己机器上并行完成的。</p></li><li><p>什么样的事务需要恢复？首先事务的Commit Phase需要横跨配置更新，并且以下三个条件在新配置中至少满足其一(其实就是判读事务的所有参与机器有没有宕机)，其他的事务其实不受影响，无需恢复：</p><ol><li>待修改Region的副本机器集发生变更</li><li>只读Region的主节点发生变更</li><li>事务协调者被移除</li></ol><p> 这里的判断是在第2步处理日志时进行的，所需要判断以上3个条件的信息是由CM发过来的(CM可以发现一个Region的映射关系是否发生变化)。每个Region的备节点都会将自己发现的需要恢复的事务id以及相关信息组成NEED-RECOVERY消息发给主节点。主节点会把所有备节点和自己发现的待恢复的事务整理出来，由于主节点可能已经截断删除了某些待恢复事务的日志项，所以需要找对应的备节点取回对应的日志项。</p></li><li><p>主节点有了所有待恢复事务的id，也有了所有需要的日志项，就可以根据日志把所有需要上锁的对象上锁(新选的主节点才需要)。如此新主节点便已经恢复了之前宕机主节点的状态，所以该Region可以开始对外提供读写服务，停止屏蔽请求。</p></li><li><p>主节点根据之前备节点NEED-RECORVERY消息里面与最终整理的待恢复事务差集可以知道备节点丢失了哪些事务的日志项(其实就是COMMIT-BACKUP记录)。主节点会给每个备节点发送相应的记录，与自己当前拥有的LOCK记录相同。</p></li><li><p>恢复事务的时候假如原本的协调者被移除了新配置，FaRM使用一致性哈希决定新的协调者，否则沿用原来的。跟该事务相关的所有待修改Region的主节点会根据自己Region的事务日志情况进行投票：</p><ol><li>本Region存在副本看到了COMMIT-PRIMARY，就投commit-primary</li><li>本Region存在副本看到了COMMIT-BACKUP，并且没有看到ABORT，就投commit-backup</li><li>本Region存在副本看到了LOCK，并且没有看到ABORT，就投lock</li><li>没有该事务任何日志，投truncated或unknown</li><li>其他情况投abort</li></ol></li><li><p>协调者收集所有投票之后，根据投票情况会做出决定，并且最后也会发出截断日志的消息：</p><ol><li>收到了commit-primary，决定提交该事务(保证了与之前的提交决定一致)</li><li>收到了commit-backup，并且其他所有投票不存在abort，决定提交该事务(之前没有决定提交还是终止，但是已经通过VALIDATE到了COMMIT-BACKUP这一步，那么就尽可能地提交该事务)</li><li>否则决定终止该事务(保证了与之前的终止决定一致)</li></ol></li></ol><p>抛开截断日志不看，FaRM事务恢复的正确性就在于一个分布式事务的所有参与Region里，单凡有一个主节点活下来保留了COMMIT-PRIMARY，那么整个事务就可以不违背之前的决定成功提交。但凡看到一个ABORT，同样也可以不违背之前的决定成功终止。对于出现了COMMIT-BACKUP但是没看到COMMIT-PRIMARY和ABORT的情况，那就代表事务已经通过了OCC检查，但是有可能恰巧所有的COMMIT-PRIMARY记录丢失了，所以应该尽可能地让各个新主节点恢复锁现场，完成对该事务的提交，不违背对外部的承诺。</p><p>对于不符合3.abc条件的事务，那其实说明该事务的所有参与机器都没有故障，不需要额外干扰，让他们自己推进就好，所以不用进行事务恢复。</p>]]></content>
    
    
    <categories>
      
      <category>MIT6.824 分布式系统</category>
      
    </categories>
    
    
    <tags>
      
      <tag>读论文</tag>
      
      <tag>分布式事务</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>分布式事务 Spanner</title>
    <link href="/2023/10/15/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1-Spanner/"/>
    <url>/2023/10/15/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1-Spanner/</url>
    
    <content type="html"><![CDATA[<p>论文标题为《Spanner: Google’s Globally-Distributed Database》，发表于2013年，这篇论文主要介绍了Google的全球级别数据库Spanner，使用了原子钟和GPS实现高精度分布式时间系统，利用高精度的时间戳确定事务的先后顺序，并通过Paxos共识协议备份容错+2PC两阶段提交完成分布式事务。</p><h2 id="整体架构"><a href="#整体架构" class="headerlink" title="整体架构"></a>整体架构</h2><p><img src="/2023/10/15/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1-Spanner/%E5%9B%BE1.png" alt="图1" title="图1"><br>Spanner的架构设计是覆盖全球级别的，每一个Spanner部署被Google命名为一个universe，所以在一个universe中有一个universemaster，其实就是一个查看整个部署各种信息的控制台；还有一个placement driver，周期性地与spanserver们联系，找到需要迁移的数据，并且完成迁移，迁移是源于负载均衡或者复制的约束条件发生变化。</p><p>Spanner中包含了很多Zone，Zone是物理意义上的不同集群，可以被管理员添加或移出universe，是数据进行冗余复制的单元(比如要求3个备份，就需要在3个不同的Zone备份)。每个Zone里则像是一个BigTable部署，zonemaster负责分配数据到spanserver上，location proxy负责将用户请求路由到对应spanserver上，spanserver负责真正的数据存储以及读写服务。</p><h2 id="spanserver设计"><a href="#spanserver设计" class="headerlink" title="spanserver设计"></a>spanserver设计</h2><p><img src="/2023/10/15/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1-Spanner/%E5%9B%BE2.png" alt="图2" title="图2"><br>Spanner对外提供的服务如同一个多版本KV存储，以<code>(key:string, timestamp:int64) → string</code>这样的映射得到对应时间戳的(key,value)，Spanner在设计上对这些键值对进行了分区，每100-1000个组成一个tablet，类似BigTable中Region使用LSM树文件的形式存在GFS中，tablet则以B树文件的形式存在Colossus(Google内部GFS的后续升级版)中。</p><p>为了能达到多备份容错的目标，tablet需要在跨多个数据中心的机器上进行备份，所以有一层Paxos层，该层需要达成的共识就是tablet的内容，因为Paxos是强一致性共识协议，就保证了tablet在多个副本中的一致性。每个tablet备份所在的这几个机器组成了维护这个tablet的Paxos Group，因为Paxos共识协议要求有leader的存在，所以这几个机器中也会有一个leader。tablet的写需要从leader这里发起，读则只用选一个足够新的replica即可。由于leader需要维护事务相关的锁表，所以Spanner的实现中Paxos尽量不频繁切换leader，使用大概10s左右的租约，避免频繁leader切换的代价。Spanner在设计时考虑到有很多事务是分钟级的，假如使用乐观并发控制OCC这种模式，很容易一直重做，所以还是选用了悲观的加锁这种形式。</p><p>再上层便是在Paxos leader之上对于事务的设计，事务涉及的数据有可能是仅在本Group之内的，也可能是跨Group的，无论是哪种都需要锁表的存在，leader在事务开始时给对应对象上锁，事务结束后再解锁。单Group的事务是简单直接的，可以由本Group的leader直接完成。跨多Group的事务则需要多个Group的leader协作完成两阶段提交2PC的过程，2PC需要一个协调者来统筹整个事务提交过程，所以所有参与的Group leader中有一个会被选为协调者coordinator leader，并且和其他Group的participant leader交流推进2PC。各个leader在这个过程中会将所有操作、已知信息记录在Paxos的log中，可以保证故障恢复，其实也解决了原始版2PC单点故障无法恢复的问题。</p><h2 id="TrueTime-API"><a href="#TrueTime-API" class="headerlink" title="TrueTime API"></a>TrueTime API</h2><p><img src="/2023/10/15/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1-Spanner/%E5%9B%BE3.png" alt="图3" title="图3"><br>从前面描述可以知道，Spanner会存同一个key的多个时间戳版本，时间戳的准确性在Spanner中十分重要，不但与(key,value)的时间戳有关，还关系到Spanner想要达到的强(外部)一致性，用物理意义上的时间自动为各个操作排序。在一个分布式系统中，统一所有机器的时间戳是很难的，或多或少都会与真实时间有偏差，所以每一台机器自身的硬件时钟是不足以信任的。为了避免这样的问题，Spanner利用原子钟和GPS构建了一套误差很小的时间系统，在每一个数据中心都有一个time master，该master要么自身装备了原子钟，要么通过GPS从其他多个master获得当前时间来校准自己的时间。每个数据中心的所有机器则跟自己数据中心的time master交流校准自己的时间，凭此Spanner构建了误差很小的时间系统，并开放了上图所示的三个API。与常规的时间API不同，TT.now()返回的是一个很小的时间范围，保证当前现实世界时间戳一定处于这个范围内，after()和before()则用于判断某个时间戳是否已经确定成为过去或者确定还未到来。</p><h2 id="事务"><a href="#事务" class="headerlink" title="事务"></a>事务</h2><p>Spanner会给事务分配一个标识该事务发生时间(论文中的Start规则)的时间戳，任何事务的先后顺序是依据其时间戳决定的，这个顺序需要与现实发生时间相符，并且每个事务的时间戳一定不相同。对于并发的事务，系统的表现仿佛是依次顺序执行所有事务，对于一个读事务，它看到的值应该是其开始时间戳之前的写事务的结果，不能看到任何其开始时间戳之后的写事务的结果。</p><h3 id="读写事务"><a href="#读写事务" class="headerlink" title="读写事务"></a>读写事务</h3><p>在Spanner的一个读写事务中，完整流程如下：</p><ol><li>客户端先找所有相关tablet对应的Paxos Group读到该事务需要的所有最新的(key,value)，并且是要上读锁的，一直持续到整个分布式事务结束</li><li>客户端将读到的(key,value)缓存在本地，进行事务中的操作，最终结果也缓存在本地</li><li>客户端开启2PC，选择其中一个Group的leader作为协调者，然后把第2步的缓存结果分发到各个participant leader</li><li>根据是否是协调者，不同leader接收到客户端缓存结果时有不同的行动：<ol><li>非协调者的participant leader先把要修改的key都上写锁，然后挑选一个比自己日志中出现过的最大时间戳还要大的时间戳$T_p$，同时会向Paxos日志中加一个记录此次修改的prepare日志项，该日志项的时间戳就是$T_p$</li><li>协调者coordinator leader收到消息时会先记录一下当前的时间$T_n&#x3D;TT.now().latest$，随后同样把对应的key上写锁，并且收集其他participant leader刚给这个事务分配的准备时间戳$T_p$，之后协调者选一个比$T_n$、所有$T_p$、自己日志中出现过的最大时间戳都大的时间戳$S$，时间戳$S$将作为这个事务的时间戳(论文中的<strong>Start规则</strong>)，最后便是向Paxos添加一个时间戳为$S$的日志项，内容为本次修改</li></ol></li><li>为了保证<strong>一定要在现实时间</strong>$S$<strong>之后才能看到对应的修改(即读请求无法读到未来发生的修改)<strong>，所以coordinator leader要求自己Group中的所有副本都需要等(论文中的</strong>Commit Wait规则</strong>)到$TT.after(S)$为真时才可以apply这个日志项到上层状态之中。当等待完成后，coordinator leader就会给客户端以及其他participant leader发送时间戳$S$并通知提交结果，其他的participant leader将这个结果记录在日志中，时间戳为$S$，随后就可以根据刚才的prepare日志项将变更apply到上层状态，时间戳版本为$S$，最后释放锁即可。</li></ol><h3 id="只读事务"><a href="#只读事务" class="headerlink" title="只读事务"></a>只读事务</h3><h4 id="读请求处理"><a href="#读请求处理" class="headerlink" title="读请求处理"></a>读请求处理</h4><p>前面说了，tablet上的写请求必须找Paxos Group的leader，但是读请求可以找足够新的副本机器，那么如何判断足够新这个条件？</p><p>Spanner规定了每个机器的安全时间(Safe Time)$T_{safe}$，其实很直接的一个想法，每个机器需要记录下自己apply的最新的日志项的时间戳，假如读请求附带的时间戳小于等于$T_{safe}$，那就代表自己足够新，可以应对该读请求；反之则需要阻塞一段时间直到$T_{safe}$大于等于读请求的时间戳。</p><h4 id="单tablet读事务"><a href="#单tablet读事务" class="headerlink" title="单tablet读事务"></a>单tablet读事务</h4><p>对于读事务而言，最合适的时间戳其实就是当前时间，但是正如之前的描述，假如一个tablet很久没有写请求，其实每个机器的$T_{safe}$是陈旧的，会阻塞住读请求直到足够新。所以对于单tablet(即单Paxos Group)的读事务，Spanner会直接选择当前Paxos Group的最后一个已提交日志项的时间戳作为本读事务的时间戳，以此得到最快的响应，但这样其实牺牲了外部一致性，比如某个写事务的时间戳处于$T_{safe}$和当前时间之间，只不过还没有来得及apply，那这部分修改本应被读到却被跳过了。</p><h4 id="多tablet读事务"><a href="#多tablet读事务" class="headerlink" title="多tablet读事务"></a>多tablet读事务</h4><p>因为跨了多个Paxos Group，每个Group中最后一个已提交日志项的时间戳很可能不一样，按理说可以取最大(最新)的一个时间戳作为最大的时间戳，但这需要多一轮网络IO。所以为了方便以及外部一致性起见，Spanner使用当前时间$TT.now().latest$作为读事务的时间戳(论文中的<strong>Start规则</strong>)。</p><h3 id="外部一致性证明"><a href="#外部一致性证明" class="headerlink" title="外部一致性证明"></a>外部一致性证明</h3><p>由于Spanner中的每个事务都被分配了一个时间戳，这个时间戳并不是严格的现实时间戳，但需要满足的是，两个事务分配的时间戳需要与实际发生时间戳的先后关系相同。一个读写事务$T_i$有两个关键事件$e_{i}^{start}$和$e_{i}^{commit}$，分别代表事务在现实中的客户端认为的开始(客户端开始事务)与提交(客户端收到已提交这个回复)。一个只读事务$T_i$有一个关键事件$e_{i}^{start}$，代表事务在现实中客户端认为的开始。所以现实与Spanner顺序的一致性需要保证对于写事务$T_1$和紧随其后的另一个事务$T_2$，需要有$t_{abs}(e_1^{commit})&lt;t_{abs}(e_2^{start})\rightarrow s_1&lt;s_2$成立，即现实中如果客户端认为$T_1$已经提交成功后，另一个事务$T_2$开始，那么要保证两个事务的Spanner时间戳满足$T_1$的时间戳$s_1$小于$T_2$的时间戳$s_2$，即Spanner中发生顺序与现实一致。论文中给出了如下证明，其中$t_{abs}(e)$代表事件$e$发生时刻的现实时间戳：</p><p><img src="/2023/10/15/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1-Spanner/%E5%85%AC%E5%BC%8F1.jpg"></p><p>这个证明是基于两个读写事务$T_1$和$T_2$的Spanner时间戳顺序与现实时间戳顺序保持一致，每一行分别在阐述：</p><ol><li>因为有Commit Wait规则，所以$T_1$在现实中返回给客户端成功提交那个时刻的现实时间戳一定是大于分配的时间戳$s_1$</li><li>第二行就是要证明的断言的前提条件</li><li>第三行是指因果关系，$t_{abs}(e_{2}^{server}$)代表了$T_2$中coordinator leader接到客户端提交请求(即前文“读写事务”第4步)的时间，它一定比客户端开始事务的时间晚</li><li>第四行是读写事务$T_2$选择时间戳$s_2$需要遵守的规则(见前文“读写事务”第4.b步，保证大于等于$TT.now().latest$，这里调用$TT.now().latest$一定是接到客户端请求之后，所以也是具有因果关系，但也是Start规则的一部分，所以有$t_{abs}(e_2^{server}) \leq TT.now().latest \leq s_2$)</li><li>由传递性可得最终需要证明的$s_1&lt;s_2$</li></ol><p>上面证明了两个读写事务符合$t_{abs}(e_1^{commit})&lt;t_{abs}(e_2^{start})\rightarrow s_1&lt;s_2$，对于写事务$T_1$后跟随一个读事务$T_2$，证明其实更加简单：</p><p><img src="/2023/10/15/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1-Spanner/%E5%85%AC%E5%BC%8F2.jpg"></p><p>对于读事务$T_2$而言，它的Start规则规定$s_2&#x3D;TT.now().latest$，代表赋予读事务的时间戳$s_2$一定比当前现实时间戳大，所以一定有$t_{abs}(e_2^{start}) \leq TT.now().latest \leq s_2$，从而可以证明$t_{abs}(e_1^{commit})&lt;t_{abs}(e_2^{start})\rightarrow s_1&lt;s_2$成立。</p><h2 id="精确时间的重要性"><a href="#精确时间的重要性" class="headerlink" title="精确时间的重要性"></a>精确时间的重要性</h2><p>为了说明准确的时间为什么那么重要，看一下下面的例子：</p><blockquote><p>1.如果发起读事务的机器时间很不精准，时间戳过小：</p><p>写事务 T0 @  0: Wx1 C<br>写事务 T1 @ 10: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Wx2 C<br>读事务 T2 @  5: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Rx1</p><p>(横向代表现实时间先后顺序，@后的数字代表该事务分配到的Spanner时间戳，C代表提交)</p><p>读事务T2的时间戳由于所在发起机器时间走的过慢，仅为5，虽然在现实中它发生的最晚，这会导致它只能读到T0写的1，读不到T1写的2，这与现实中的顺序是违背的，即违背了$t_{abs}(e_1^{commit})&lt;t_{abs}(e_2^{start})\rightarrow s_1&lt;s_2$，损害了外部一致性！</p><p>2.如果发起读事务的机器时间很不精准，时间戳过大：</p><p>  写事务 T0 @  0: Wx1 C<br>  写事务 T1 @ 10: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Wx2 C<br>  写事务 T2 @ 16: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Wx3 C<br>  读事务 T3 @ 15: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Rx————————2</p><p>读事务T3的时间戳由于所在发起机器时间走的过快，达到了15，虽然在现实中它发生的比T1早，但由于$T_{safe}$不满足它一直被阻塞，直到时间戳为16的T2提交后，读到了T1写的2，这里其实没有违背外部一致性，但是会让读事务阻塞很久，影响性能。</p></blockquote>]]></content>
    
    
    <categories>
      
      <category>MIT6.824 分布式系统</category>
      
    </categories>
    
    
    <tags>
      
      <tag>读论文</tag>
      
      <tag>分布式事务</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>存算分离 Aurora</title>
    <link href="/2023/10/15/%E5%AD%98%E7%AE%97%E5%88%86%E7%A6%BB-Aurora/"/>
    <url>/2023/10/15/%E5%AD%98%E7%AE%97%E5%88%86%E7%A6%BB-Aurora/</url>
    
    <content type="html"><![CDATA[<p>论文标题为《Amazon Aurora: Design Considerations for High Throughput Cloud-Native Relational Databases》，发表于2017年，这篇论文主要介绍了Amazon的AWS RDS Mysql应对云场景进行的数据库存算分离架构设计。</p><h2 id="原始Mysql"><a href="#原始Mysql" class="headerlink" title="原始Mysql"></a>原始Mysql</h2><p><img src="/2023/10/15/%E5%AD%98%E7%AE%97%E5%88%86%E7%A6%BB-Aurora/%E5%9B%BE1.png" alt="图1" title="图1"><br>原始Mysql的主从备份流程如上图所示，数据库实例写远端硬盘EBS(1&amp;4步)、同AZ内主备硬盘同步(2&amp;5步)、跨AZ主备数据库实例同步(3步)都需要传输5种信息：</p><ol><li>redo log，里面每个日志项包含了对应数据页修改内容之前和之后的值</li><li>binlog，默认语句级别，记录了每一条修改数据的SQL语句，同时会抄一份给S3用于最差情况下的数据恢复；还可以设定为行级别，记录每一行数据修改的细节，会产生大量日志</li><li>真正修改好的数据页</li><li>为了避免3中数据页损坏无法察觉进行双写(校验)</li><li>元信息FRM文件</li></ol><p>之所以原始Mysql要传那么多东西是因为本意用于单机，在自己本地硬盘写5个内容，不需要走网络，所以不影响。但是在AWS的容灾设计中，需要将数据保存多份，导致图中每一步传输都需要走网络，并且1、3、4步是按序阻塞的，大大限制了Mysql的性能。</p><h2 id="LOG即DB"><a href="#LOG即DB" class="headerlink" title="LOG即DB"></a>LOG即DB</h2><p><img src="/2023/10/15/%E5%AD%98%E7%AE%97%E5%88%86%E7%A6%BB-Aurora/%E5%9B%BE2.png" alt="图2" title="图2"><br>如前文所述，原始Mysql主从同步要传输的数据太多了，大大限制Mysql的性能，Aurora创造性地提出存算分离，并且坚持了LOG即DB的理念，将数据库分为计算节点(对外提供前台读写服务)和存储节点(将日志持久化到存储中)。</p><p>在Aurora的设计中，前台有若干个无状态的计算节点，其中有一个主节点，一切写操作需要主节点发起，其他的从计算节点是只读节点，由于计算节点是无状态的，所以可以轻松的扩展。在进行写操作时，主节点仅把redo log传给所有的存储节点以及其他从计算节点，大大降低了网络传输数据量。</p><p>在计算节点之下是有状态的存储节点，所有存储节点呈现出如同一个硬盘的存储服务，Aurora为每一个数据段(根据硬盘范围拆分，10GB左右)分配了分布于3个AZ的6个存储节点，称为每个数据段的PG(Protection Group)，由于采用Quorum读写(R&#x3D;3，W&#x3D;4，一定程度容忍慢节点)，所以遇到AZ级别(2个存储节点宕机)的故障不影响读写，遇到AZ+1级别(3个存储节点宕机)的故障不影响读。存储节点接到主计算节点发来的日志项时持久化后就可以直接返回OK，这样计算节点可以快速地收集够4个OK来完成写请求的答复，至于真正apply日志项到数据页中则是异步后台任务完成。<br><img src="/2023/10/15/%E5%AD%98%E7%AE%97%E5%88%86%E7%A6%BB-Aurora/%E5%9B%BE3.png" alt="图3" title="图3"><br>上图详细介绍了存储节点对日志项全生命流程的处理：</p><ol><li>存储节点接到计算节点传来的日志项，先缓存在内存队列中(前台工作)</li><li>将内存中的日志项持久化到磁盘，随后ACK回应计算节点(前台工作，持久化确保会完成后续写操作，快速响应)</li><li>将日志项排序，可以发现其中的空洞(比如由于网络传输失败造成)</li><li>找同数据段PG的其他存储节点要缺失的日志项，补齐空洞</li><li>把日志项真正apply到数据页上(后台进行，会把日志项按先后挂到对应数据页上，在前台工作不忙的时候或者读请求急需时真正执行)</li><li>定期把日志项、更新后数据页传给S3备份</li><li>定期回收旧版本过期数据页、已apply的日志项，释放空间</li><li>定期CRC校验数据页，避免坏页</li></ol><h2 id="日志管理"><a href="#日志管理" class="headerlink" title="日志管理"></a>日志管理</h2><h3 id="专有名词定义"><a href="#专有名词定义" class="headerlink" title="专有名词定义"></a>专有名词定义</h3><ul><li>LSN，Log Sequence Number，Aurora会给每个日志项分配唯一的递增的序列号</li><li>VCL，Volume Complete LSN，存储服务(指的整体上的硬盘抽象)持久化的完整的连续的最大LSN，比如存储服务持久化了1-1007、1010-1015的日志，那么VCL是1007而不是1015，因为要求完整连续</li><li>SCL，Segment Complete LSN，与VCL类似，不过不是全局层面的，是数据段PG层面的，对于每个数据段(10GB)，负责该数据段的6个存储节点所持久化的完整的连续的最大LSN。由于LSN的分配是全局的，所以单个数据段日志项LSN并不一定连续，这里的连续判断是依靠每个日志项里记录的该数据段上一个日志项的LSN(其实就是指向前一个节点的指针)，保证了可以发现中间的空洞，存储节点之间Gossip补足gap也是通过交流SCL</li><li>CPL，Consistency Point LSNs，因为事务要求原子性，但在日志里面很可能是多个日志项，为了保证这些日志项要么全部apply，要么完全不apply，所以定义事务的最后一个Commit日志项的LSN为CPL。对于普通的单日志项修改，自己本身的LSN就直接是CPL</li><li>VDL，Volume Durable LSN，小于等于VCL的最大的CPL就是VDL，标识了当前数据库已经持久化的最大LSN</li><li>LAL，LSN Allocation Limit，用于控制写请求速率，给新的日志项分配的LSN需要小于VDL+LAL，目前LAL被设置为一千万</li></ul><h3 id="写"><a href="#写" class="headerlink" title="写"></a>写</h3><p>主计算节点接到写请求，会将写请求转成对应的一个或多个日志项，为每个日志项分配递增唯一的LSN，最后一个日志项会标记为CPL，主计算节点将日志项Quorum写到相应数据段对应的存储节点中，达到4个及以上回复就知道日志项已经被持久化，根据前面描述的规则就可以推进全局的VDL。</p><p>对于事务来说，一般会被拆成多个mini事务，每个mini事务由多个连续日志项组成，整体事务会在最后加一个Commit日志项，它的LSN也会是一个CPL。至于如何响应事务的提交与否，则是将所有进行中事务的CPL在主计算节点中记录下来，当VDL推进到大于等于某个事务的CPL时就可以向客户端返回成功了，是个异步的过程。</p><h3 id="读"><a href="#读" class="headerlink" title="读"></a>读</h3><p>每个计算节点有自己的缓存池，里面存了一些最新版本的数据页，当读请求命中时就直接返回，否则会引发计算节点找存储节点读相关数据页，这个读并不是Quorum读，而是只找一个足够新的存储节点读出来数据页即可。计算节点根据读请求到来时的VDL为读请求赋予版本号，检查相应存储节点的SCL是否大于等于VDL(即读请求的版本号)，就知道存储节点是不是足够新，就可以保证不读过时数据。</p><p>存储节点中数据页的存储形式是，每个数据页上挂着一串它自己的尚未apply的日志项，当读请求到来时，存储节点根据要求的版本(VDL)，在数据页上apply所有LSN&lt;&#x3D;VDL的日志项(为了原子性，进行中的事务除外)并将数据页返回，注意并不是将存储节点上原本数据页替换掉，因为旧版本的数据可能还在被其他读请求、事务使用。</p><p>那么什么时候可以将挂在数据页上的日志项apply到数据页然后垃圾回收掉？需要在旧版本的数据不可能被使用时。主计算节点会与所有其他计算节点沟通得到每个数据段的当前读请求中最低的版本号PGMRPL(Protection Group Min Read Point LSN)，然后主计算节点通知相应数据段的存储节点，就可以把所有LSN&lt;&#x3D;PGMRPL的日志项apply到数据页中，随后将这些日志项垃圾回收掉，并且把旧版本数据页也扔掉。</p><p>在图3可以看出，主计算节点还会将日志传给备计算节点，这些备计算节点也会用这些日志信息维护自己缓存池中的数据页。当备计算节点收到日志项时，若对应数据页没有在自己缓存中，会直接抛弃，否则就和存储节点一样把日志项挂在对应数据页上。当VDL向前行进时，计算节点会将LSN&lt;&#x3D;VDL的日志项(非事务，事务的所有日志项需要原子地进行)apply到数据页上，这样就保证了缓存池中数据页的新鲜。Aurora描述备计算节点与主计算节点有小于20ms的延迟。</p><h3 id="故障恢复"><a href="#故障恢复" class="headerlink" title="故障恢复"></a>故障恢复</h3><p>Aurora由于日志即数据库的设计，不需要检查点，在故障恢复时，存储节点并不需要将所有状态恢复到故障前的样子(对应传统的redo阶段)，而是直接重启即可，通过读请求、后台任务就可以将日志项apply到数据页上，没有什么需要恢复的状态。计算节点则会联系所有存储节点，以Quorum读的形式计算出整体的VDL，随后告知所有存储节点删掉所有LSN&gt;VDL的日志项，并且将开始点&lt;&#x3D;VDL，但尚未Commit的事务补上undo日志项(相当于取消进行中的未Commit事务，对应传统的undo阶段)。当redo、undo两阶段完成后，就完成了故障恢复。</p>]]></content>
    
    
    <categories>
      
      <category>MIT6.824 分布式系统</category>
      
    </categories>
    
    
    <tags>
      
      <tag>读论文</tag>
      
      <tag>MySQL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>协调服务 Zookeeper</title>
    <link href="/2023/10/15/%E5%8D%8F%E8%B0%83%E6%9C%8D%E5%8A%A1-Zookeeper/"/>
    <url>/2023/10/15/%E5%8D%8F%E8%B0%83%E6%9C%8D%E5%8A%A1-Zookeeper/</url>
    
    <content type="html"><![CDATA[<p>论文标题为《ZooKeeper: Wait-free coordination for Internet-scale systems》，发表于2010年，这篇论文主要介绍了Yahoo实现的著名协调服务Zookeeper，可以方便地被用来实现主节点选举、配置管理、元信息管理、群组管理、简单状态同步、分布式锁等等。</p><h2 id="总览"><a href="#总览" class="headerlink" title="总览"></a>总览</h2><p><img src="/2023/10/15/%E5%8D%8F%E8%B0%83%E6%9C%8D%E5%8A%A1-Zookeeper/%E5%9B%BE1.png" alt="图1" title="图1"><br>Zookeeper的目标是帮助应用程序管理元数据，使用目录树结构(实际每个znode存完整路径，更像KV存储，但是对外展示目录树形式)管理znode，便于用户、应用程序理解以及使用，但并不是一个以存储数据为目标的文件系统。</p><p>在Zookeeper的设计中有两种znode，分别是<strong>常规(Regular)znode</strong>和<strong>临时(Ephemeral)znode</strong>，临时znode可以被Client显式删除，或者在Client与Zookeeper会话结束时自动删除。为了方便管理应用程序的配置信息，所有znode都允许存储一些数据(往往都是简短的配置信息)，并且允许所有常规znode拥有子znode，临时znode没有目录功能。Zookeeper允许Client创建znode的时候加上sequential标志，意味着在当前目录下所有sequential znode都有一个递增的id，这个id会拼接到znode名字的最后。</p><p>Client可以在znode上加<strong>监视器</strong>，当znode发生变化时，Zookeeper会通知监视它的Client，注意监视器使用一次就失效，Client可以在被通知后再次加一个新的监视器。Zookeeper会管理自己与所有Client之间的会话，Client主动关闭或者会话如果超过一定时间没有交互，Zookeeper就会关闭会话，同时也会删除该Client的监视器。会话管理也是Zookeeper集群要达成共识的状态之一，所以Client切换具体连接的机器时，是可以无感切换的。</p><p>Zookeeper底层使用Zab共识协议，与Raft等共识协议类似，使得Zookeeper集群中的机器对应用层状态达成共识(强一致性)，所有写请求会转交给主节点leader完成，读请求则由备节点follower完成，所以可能会因为延迟返回旧数据，但在大多数场景下不关键。Zookeeper将所有操作实现为<strong>幂等</strong>的，所以可以处理重复请求，并且可以边apply请求边创建状态快照(fuzzy snapshot)，这个快照可能是不准确的，但是基于快照重放一遍启动快照时刻之后的日志就可以了，幂等保证了重复相同的操作没有影响。<br>Zookeeper对于一致性有两个保证：</p><ul><li><strong>全局线性写</strong>，即所有写请求都转发给Zookeeper主节点leader，分配了zxid具有全局顺序。</li><li>在单个Client看来，它的所有请求都是FIFO(先来后到)处理的，保证了单个Client的<strong>“写后读”一致性</strong>和<strong>不会回退</strong>。为了获得“写后读”一致性，Client会在发送读请求的时候发送自己已知的最大的zxid(不光有Client自己提写请求得到的zxid，Server给Client的所有回复都会带上Server上最大的zxid)，这样处理这个读请求的节点就知道一定要在zxid写请求被apply后再回复这个读请求。</li></ul><h2 id="客户端API"><a href="#客户端API" class="headerlink" title="客户端API"></a>客户端API</h2><p>Zookeeper支持以下这些API：</p><ul><li>create(path, data, flags)：在path指定的路径下创建一个znode，存入数据data，flags可以指定常规、临时、sequential</li><li>delete(path, version)：如果版本号正确，删除path指定的znode</li><li>exists(path, watch)：判断是否存在path指定的znode，布尔类型参数watch用来指定是否要监视该znode的变化</li><li>getData(path, watch)：与exists接口类似，不过还会获得znode的内容、版本、元信息等</li><li>setData(path, data, version)：如果版本号正确，将path指定的znode存的数据设置成data</li><li>getChildren(path, watch)：获得path指定的znode的所有子znode列表，布尔类型参数watch用来指定是否要监视子znode列表变化</li><li>sync(path)：在发表论文时，参数path没有用，sync用于让Client等待直到当前连接的机器(Zookeeper允许集群中所有机器对外提供读服务，所以可能有延迟)同步到最新状态。实现是处理sync请求的Zookeeper机器给leader提一个sync操作，等这个操作被同步到自己这里并且被apply后就可以继续响应Client的后续请求</li></ul><p>Zookeeper提供的这些API都有阻塞和非阻塞版本，Client使用非阻塞版本API时，可以发起请求不等回复就进行其他操作，虽然非阻塞API使得Client侧可以同时有多个进行中的请求，但是Zookeeper侧还是会按照Client发起的顺序按序执行。</p><h2 id="样例"><a href="#样例" class="headerlink" title="样例"></a>样例</h2><h3 id="动态配置管理"><a href="#动态配置管理" class="headerlink" title="动态配置管理"></a>动态配置管理</h3><p>多个机器上的进程同时关注某个配置znode，每个进程开始时读一下znode，并在其上加一个监视器，一旦有配置变更，进程就会得到提醒，接下来读一下新配置，并且再次加一个监视器即可。</p><h3 id="原子写"><a href="#原子写" class="headerlink" title="原子写"></a>原子写</h3><p>有很多时候，需要同时修改多个配置znode，要么全部修改完成，要么全部不修改。为了在Zookeeper中实现原子写，可以创建一个ready znode，当修改者要开始修改时，首先把ready znode删除，然后进行所有修改，完成后再创建ready znode。读者想要读这些配置znode的时候，需要先用exists确认ready znode的存在，存在才可以读配置znode，但是这样还会有问题，就是一开始ready znode存在，但是开始读配置znode的时候，另一个写者删掉了ready znode然后开始修改配置，这就会造成错误，所以读者在用exists确认ready znode存在时要加上一个监视器，当ready znode被删除时可以获得提醒，就放弃这次读。</p><h3 id="群组管理"><a href="#群组管理" class="headerlink" title="群组管理"></a>群组管理</h3><p>合作关系的多个进程往往需要知道都有哪些进程及其信息在群组中，使用Zookeeper可以创建一个znode $z_g$用于表示群组，每个进程启动的时候就在$z_g$下用sequential标志创建一个子znode，使用getChildren就可以获得群组内的所有成员，设置watch参数为true就可以监控群组成员的变化。</p><h3 id="锁"><a href="#锁" class="headerlink" title="锁"></a>锁</h3><p>多个进程之间往往需要分布式锁来协调对资源的使用，在Zookeeper中可以轻易实现锁，所有进程都尝试使用同一个path创建一个临时znode，创建成功就意味着获得了锁，当进程显式删除znode或者自己掉线导致会话失效删除这个临时znode，都可以看作释放锁。其他没有创建成功的进程，可以在znode上设置监视器，当znode被删除后，获得通知后就可以再次尝试创建(上锁)。</p><h3 id="锁-避免惊群效应"><a href="#锁-避免惊群效应" class="headerlink" title="锁(避免惊群效应)"></a>锁(避免惊群效应)</h3><p>上一个版本的锁会带来惊群效应的问题，当锁被释放后，大量监听的Client会被通知，然后一股脑尝试上锁。为了解决这个问题，可以配合使用sequential标志，一开始就确定znode的顺序(即获得锁的顺序)，每一个Client只监听前面那一个znode，每次释放锁只会通知后序的那一个Client，避免了惊群效应，如下：</p><blockquote><p><strong>Lock</strong></p><ol><li>n &#x3D; create(l + “&#x2F;lock-”, EPHEMERAL|SEQUENTIAL)   &#x2F;&#x2F;sequential保证了创建成功</li><li>C &#x3D; getChildren(l, false) &#x2F;&#x2F;用于判断是否上锁成功，即第一个创建了znode</li><li>if n is lowest znode in C, exit &#x2F;&#x2F;第一个创建znode，获得锁</li><li>p &#x3D; znode in C ordered just before n &#x2F;&#x2F;正好前面的那一个znode</li><li>if exists(p, true) wait for watch event &#x2F;&#x2F;假如正好前面的那一个znode没了，就说明轮到自己了</li><li>goto 2</li></ol><p><strong>Unlock</strong></p><ol><li>delete(n) &#x2F;&#x2F;删除znode，释放锁</li></ol></blockquote><h3 id="读写锁"><a href="#读写锁" class="headerlink" title="读写锁"></a>读写锁</h3><p>读写锁是根据上面的锁实现稍微修改的，写锁和之前的一致，读锁则改为监听其前的那一个写锁，不需要关注读锁，因为读锁是共享锁，如下：</p><blockquote><p><strong>Write Lock</strong></p><ol><li>n &#x3D; create(l + “&#x2F;write-”, EPHEMERAL|SEQUENTIAL) &#x2F;&#x2F;前缀wrtie</li><li>C &#x3D; getChildren(l, false)</li><li>if n is lowest znode in C, exit</li><li>p &#x3D; znode in C ordered just before n</li><li>if exists(p, true) wait for event</li><li>goto 2</li></ol><p><strong>Read Lock</strong></p><ol><li>n &#x3D; create(l + “&#x2F;read-”, EPHEMERAL|SEQUENTIAL) &#x2F;&#x2F;前缀read</li><li>C &#x3D; getChildren(l, false)</li><li>if no write znodes lower than n in C, exit &#x2F;&#x2F;只关注之前是否有写锁znode</li><li>p &#x3D; write znode in C ordered just before n &#x2F;&#x2F;正好前面的那一个写锁znode</li><li>if exists(p, true) wait for event &#x2F;&#x2F;假如正好前面的那一个写锁znode没了，就说明轮到之后的这些读锁了</li><li>goto 3</li></ol><p><strong>Unlock</strong></p><ol><li>delete(n) &#x2F;&#x2F;删除znode，释放锁</li></ol></blockquote><h3 id="屏障"><a href="#屏障" class="headerlink" title="屏障"></a>屏障</h3><p>屏障用于同步多个进程的步伐，一起进入屏障，一起走出屏障。在Zookeeper中，可以创建一个znode $z_b$，当进程准备进入屏障时在$z_b$下创建一个znode，当自己是最后一个进入准备状态的进程时，会在$z_b$下创建一个ready znode，所有进程通过exists检查ready znode是否存在就知道是否可以开始计算，当完成后就可以将自己刚刚创建的znode删除；为了知道什么时候可以离开屏障，可以创建一个exit znode，最后一个完成计算的进程删除掉exit znode，所有进程通过监听就可以知道可以离开屏障了。</p>]]></content>
    
    
    <categories>
      
      <category>MIT6.824 分布式系统</category>
      
    </categories>
    
    
    <tags>
      
      <tag>读论文</tag>
      
      <tag>协调服务</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>共识算法 Raft</title>
    <link href="/2023/10/15/%E5%85%B1%E8%AF%86%E7%AE%97%E6%B3%95-Raft/"/>
    <url>/2023/10/15/%E5%85%B1%E8%AF%86%E7%AE%97%E6%B3%95-Raft/</url>
    
    <content type="html"><![CDATA[<p>论文标题为《In Search of an Understandable Consensus Algorithm》，发表于2014年，这篇论文主要介绍了共识算法Raft，易懂并且提供强一致性语义，已经被广泛使用在了工业界分布式系统中。</p><h2 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h2><p><img src="/2023/10/15/%E5%85%B1%E8%AF%86%E7%AE%97%E6%B3%95-Raft/%E5%9B%BE1.png" alt="图1" title="图1"><br>为什么需要共识算法？因为为了容错，往往将同样的内容存在多个备份上，但是各种硬件、软件故障通常另多个备份陷入不一致的状态，为了保证每个备份上存的内容一致，并且在发生故障后可以恢复回故障前的状态，所以需要共识算法。</p><p>例如上图所示的kv存储，服务器上画出了三部分：共识模块、日志、状态机。状态机是真正的kv表，对外部Client提供读写服务就是基于状态机上的状态。共识模块则是共识算法工作的地方，目标就是为了让多个备份机器上的状态机保持一致，对于Raft而言，它将所有状态机支持的操作以一个个日志项的形式记录下来，共识模块需要共识的就是日志项，确保每个备份机器上都有完全相同的已提交日志记录，这样状态机基于日志进行状态变更时就可以保证各个备份上的状态一致。<br>Raft有以下保证：</p><ul><li>确保强一致性，确保任何情况下不会返回错误(违反一致性)的结果</li><li>只要Raft集群中超过一半的机器存活，就可以对外提供服务</li><li>不依赖时钟(像Spanner使用了特殊时钟硬件利用时间戳给操作全局排序)</li><li>一个外部请求产生的指令，只需要超过一半的机器达成共识，就可以产生回复，不需要等慢的那一小半</li></ul><h2 id="Raft算法"><a href="#Raft算法" class="headerlink" title="Raft算法"></a>Raft算法</h2><p>Raft集群会首先选出一个leader，leader会全权授理所有Client的请求，将请求转化为日志项，leader需要负责将日志项复制到所有follower上，并且在收到超过半数回复后确定可以提交日志项，告知所有follower可以将该日志项apply到状态机中。leader也可能会宕机，剩余的follower会因为长期未收到leader心跳发起新的选举，选出一个新leader。因此，Raft集群内主要就是两种RPC请求：</p><ul><li>RequestVote：某台机器发起新一轮选举，向其他机器请求选票</li><li>AppendEntries：leader向其他follower复制日志项，该请求还可以不包含任何日志项而用于leader发送心跳</li></ul><p>Raft算法各个机器需要维护的状态(state)、两个RPC请求、需遵守的规则如下图所示：<br><img src="/2023/10/15/%E5%85%B1%E8%AF%86%E7%AE%97%E6%B3%95-Raft/%E5%9B%BE2.png" alt="图2" title="图2"><br>按照上图的描述，Raft算法在任何时刻可以有以下保证：</p><ul><li>Election Safety(§5.2)：一个任期term中至多有一个leader</li><li>Leader Append-Only(§5.3)：leader不会修改、覆盖、删除自己的已有日志项，只会在自己已有日志项之后添加新的日志项</li><li>Log Matching(§5.3)：如果两台机器在某个相同索引的日志项具有相同的term任期号，那么该日志项及其之前的所有日志项完全一致</li><li>Leader Completeness(§5.4.1)：如果一个日志项被提交了，那么所有之后任期的leader的日志上都会有该日志项</li><li>State Machine Safety(§5.4.3)：如果一个日志项被某台机器apply了，所有其他机器都会在相同索引处apply同样的日志项</li></ul><h3 id="角色与任期-§5-1"><a href="#角色与任期-§5-1" class="headerlink" title="角色与任期(§5.1)"></a>角色与任期(§5.1)</h3><p>在Raft集群中机器处于三种角色中的一种：leader、follower、candidate，三种角色之间的切换如下图所示。<br><img src="/2023/10/15/%E5%85%B1%E8%AF%86%E7%AE%97%E6%B3%95-Raft/%E5%9B%BE3.png" alt="图3" title="图3"><br>在大多数情况下，集群中有一个leader，其余机器是follower。在follower长时间没有收到leader心跳的时候就会变成candidate发起选举，这一时期可能有多个candidate，但是这个时期是短暂的，目标就是选出一个leader。Raft将时间如下图所示划分成不确定长度的term，每一次选举都会使得term向前推进，如果有一个candidate赢得选举(获得超过一半的票)，那么就升级为leader，在这个term中负责处理外部的请求、同步日志项。如果如同下图t3发生了分票现象(多个candidate分别获得一些票，但每个都小于等于一半)，那么这个term就会没有leader，接着因为心跳超时再次递进到下一个term的选举。<br><img src="/2023/10/15/%E5%85%B1%E8%AF%86%E7%AE%97%E6%B3%95-Raft/%E5%9B%BE4.png" alt="图4" title="图4"><br>term在Raft算法中就如同<strong>逻辑时钟</strong>一样的存在，更高的term意味着在时间线上更新，所以在Raft算法中，每个日志项都会记录自己是在哪个term被添加到日志中的，<strong>每台机器也会在状态中持久化自己所认为的当前term(图2中的currentTerm)<strong>。在Raft的两个RPC接口中，无论是请求还是回复都会带上双方的currentTerm，使得机器可以尽快知道自己是否落伍。</strong>假如机器在交流中发现更高的term，就代表自己已经落伍，机器会直接切换为follower角色并更新自己的currentTerm。机器收到的具有更低term的请求都会被无视，机器会返回自己的currentTerm令其知道其已落伍。</strong></p><h3 id="Leader选举-§5-2"><a href="#Leader选举-§5-2" class="headerlink" title="Leader选举(§5.2)"></a>Leader选举(§5.2)</h3><p>当follower、candidate超过一定时间没有收到leader的心跳，就会递增自己的currentTerm，角色变为candidate发起新term的选举。它会把票投给自己，并且向所有其他机器发送RequestVote请求。Raft规定<strong>每个机器在每个term最多给一个candidate投票</strong>，投票者会把票投给第一个到来请求对应的candidate(为了一致性的考虑，还需要检查candidate是否够格，在§5.4.1介绍)，为了避免自己故障重启后忘记已经投了票而继续给其他candidate投票(这会导致总票数超过集群机器数，可能会选出多个leader，即得票都超过一半)，<strong>投票信息votedFor(图2中votedFor)需要持久化，记录本term给谁投了票，哪怕故障重启了也不会忘记</strong>。</p><p>candidate会保持现状直到以下三者之一发生：</p><ul><li>自己赢得选举(超过一半机器投票给自己)，自己转变为leader，给其他所有机器发送心跳</li><li>其他candidate成为leader，给自己发送了心跳，检查其term大于等于自己的currentTerm就认定其是leader，自己转变为follower</li><li>自己没有成为leader，并且超时没有收到leader心跳，维持candidate身份，再次递进currentTerm发起新一轮选举</li></ul><p>第三种情况大多是因为分票现象的发生，虽然不影响系统正确性，但是频繁的分票会使得很多轮选举才可以成功选出leader。<strong>为了避免频繁的分票现象，Raft令每台机器的心跳超时时间加入一部分随机因素</strong>，使得不同机器不会在同一时刻都变成candidate来分票，而是因为随机性有的机器先变成candidate早早地获得选票成为leader。</p><p>关于超时时间的设定，论文推荐满足<code>broadcastTime ≪ electionTimeout ≪ MTBF</code>这样的关系，≪意味着数量级上的小于。broadcastTime指的是机器之间网络IO的延时，它需要比选举超时时间小一个数量级，因为网络的波动经常会增加延迟，假如broadcastTime和electionTimeout比较接近，那一个网络波动就可能触发选举，使得选举过于频繁。MTBF指的是一台机器平均发生故障时间(通常为几个月或更多)，electionTimeout需要比MTBF小一个或几个数量级，假如electionTimeout和MTBF很接近，那么leader发生故障时，要等很久才会引发新的选举，这期间Raft集群无法做任何工作，所以需要能够更快应对leader的故障。综上，Raft根据broadcastTime(大约0.5-20ms)和MTBF(几个月或更多)给出了electionTimeout的选择，大概在10-500ms。</p><h3 id="日志复制-§5-3"><a href="#日志复制-§5-3" class="headerlink" title="日志复制(§5.3)"></a>日志复制(§5.3)</h3><p>当一个机器被选为leader后，就需要负责处理Client的所有请求，将请求转化为一系列指令以日志项的形式添加到日志中。leader通过AppendEntries接口将日志项同步给所有follower，<strong>当有超过一半的follower确认收到后，就可以认为日志项已提交</strong>，进而可以将日志项apply到状态机中并响应Client。对于那些没有收到回复的follower，leader会一直重试AppendEntries直到得到回复，leader虽然可以在超过一半确认时就回复Client，但最终目标一定是让所有follower都收到日志项。<br><img src="/2023/10/15/%E5%85%B1%E8%AF%86%E7%AE%97%E6%B3%95-Raft/%E5%9B%BE5.png" alt="图5" title="图5"><br>对于图1所示的kv存储，日志的样子大概如图5所示，每个日志项存了一个指令，并且日志项中还存了这个日志项是由哪个term的leader添加的。对于一个日志项而言，比较关键的是它的term和index(即在日志中的索引、位置)，index主要是标识了不同日志项的相对顺序，所有机器都是从前向后apply日志项。对于日志，Raft有以下保证：</p><ul><li><strong>如果两台机器在某个index的日志项具有相同的term，那么这两个日志项完全一样。</strong>该保证来自于leader在自己的任期(term)内在某个index最多添加一个日志项，并且这个日志项的位置index不会再做任何改变。</li><li><strong>如果两台机器在某个index的日志项具有相同的term，那么两台机器的日志在该日志项之前的部分也完全一样。</strong>该保证来自于leader在AppendEntries请求中会带上prevLogIndex和prevLogTerm两个参数，分别代表了当前复制日志项的前一个日志项的index和term。follower在接到请求后，需要先验证前一个日志项的index和term是否一致，验证通过后才可以将新的日志项添加到日志中，根据归纳法可知，前一个日志项保证了前前一个日志项的一致，所以就保证了当前日志项之前的所有日志项都一致。</li></ul><p>在大部分正常情况下，leader和follower的日志是如图5所示一样，一致但伴有延迟。但有时候故障的发生、网络分区会使得leader和follower在相同index的日志项不一致，如下图所示。与leader的日志相比，a和b只是有延迟，缺少一些日志项；c是第6个term的leader，index为11的日志项还没来得及复制给其他机器就宕机，在目前term沦为follower；d与c相似，两者都有多余的未提交日志项，不过d是第7个term的leader；e和f不光缺一些日志项，还多了一些未提交日志项，这种情况一般也是当选leader后没来得及给足够多的follower复制日志项就被网络分区了，导致自娱自乐了。<br><img src="/2023/10/15/%E5%85%B1%E8%AF%86%E7%AE%97%E6%B3%95-Raft/%E5%9B%BE6.png" alt="图6" title="图6"><br>为了解决图6所示的日志不一致的问题，Raft规定<strong>一切日志以当前leader为准，当前leader要做的就是把所有follower的日志同步成和自己一样，所以不需要对自己的日志做任何修改，只会基于自己的日志继续添加日志项</strong>。这样的规定是会覆盖一些follower上的日志项的，之所以可以覆盖这些日志项，是因为这些都是未提交的(没有对外产生影响)，这一部分的证明会在之后说明。</p><p>为了能够将follower的日志同步成和leader一样，如前文描述，AppendEntries请求中加了prevLogIndex和prevLogTerm两个参数，用于follower对比前一个日志项的index和term和leader是否一致。在不一致时，通过回复中的success字段告知leader，leader就会将要传递的日志项往前推一个，直到推到两者一致的日志项(<strong>删除多余的未提交日志项</strong>)，比如图6中leader给e传日志项时就会不断回退直到index为6的日志项。follower在检查通过后就会在对应index添加leader传来的日志项，并删除之后的日志项(<strong>若有，删除多余的未提交日志项</strong>)。</p><p>在具体实现中，leader的状态中需要维护图2所示的nextIndex[]和matchIndex[]两个数组，分别存了每个follower下一个需要复制的日志项index、每个follower已知复制完成最大的日志项Index，这两个数组都不需要持久化，遇到故障丢失就丢失了。在leader刚被选举完成后，会初始化两个数组，nextIndex[]的所有项都被初始化为leader日志中下一个日志项的index(以图6为例就是11)，matchIndex[]的所有项都被初始化为0。刚才说的<strong>不断回退就是指的nextIndex[]中对应元素的不断减1，复制成功的请求就会令nextIndex[]中对应元素加1</strong>，同时<strong>成功的回复会令matchIndex中对应follower已知复制完成最大的日志项Index向前推进</strong>。</p><p><strong>leader会根据matchIndex中每个follower的复制情况算出当前term(要求当前term的原因见§5.4.2)已提交的最新日志项</strong>(即matchIndex中超过一半元素都大于等于该日志项的Index)，并根据其index更新自己的commitIndex。leader会在AppendEntries请求中的leaderCommit字段填上自己的commitIndex，以此告知所有follower当前日志项的commit情况，所有小于等于leaderCommit的日志项都是已提交的，都可以apply到状态机中。每个机器的状态中都有图2所示的commitIndex和lastApplied两个字段，两个字段同样不需要持久化，初始设置为0，<strong>leader是根据matchIndex[]推进commitIndex，follower则是被动接受leader的leaderCommit更新到自己的commitIndex，lastApplied则代表当前机器已经将前lastApplied个日志项apply到自己的状态机</strong>。所有机器都会有线程检查lastApplied和commitIndex的大小，并持续的将(lastApplied,commitIndex]范围内的日志项apply到状态机中。机器故障后恢复其实就是从头apply日志项恢复状态机的状态。</p><p>刚才的描述可以发现，leader在AppendEntries失败时会以步长为1的速度回退nextIndex[]对应元素，其实可以让follower返回更多的失败信息，<strong>减少回退次数</strong>，下面是6.824 Morris给出的一种可能的实现：</p><blockquote><table><thead><tr><th></th><th><strong>Case 1</strong></th><th><strong>Case 2</strong></th><th><strong>Case 3</strong></th></tr></thead><tbody><tr><td><strong>S1</strong></td><td>4 5 5</td><td>4 4 4</td><td>4</td></tr><tr><td><strong>S2</strong></td><td>4 6 6 6</td><td>4 6 6 6</td><td>4 6 6 6</td></tr></tbody></table><p>S2是第6个term的leader，当S2给S1传送最后一个日志项时，AppendEntries请求中prevLogTerm为6，由于和自己的日志项不匹配，S1会拒绝该请求，S1的回复中包括：</p><ul><li>XTerm：如果有冲突，设置为S1上冲突日志项的term(比如Case 1中就是5)</li><li>XIndex：如果有冲突，设置为S1上隶属于XTerm的第一个日志项的index(比如Case 1中就是2)</li><li>XLen：S1日志的长度(比如Case 1中就是3)</li></ul><p>根据S1的回复，S2分三种情况进行处理，分别对应上面三个Case：</p><ul><li>Case 1：leader的日志中没有日志项的term等于XTerm，令nextIndex为XIndex(示例中为2)</li><li>Case 2：leader的日志中存在日志项的term等于XTerm，令nextIndex为leader的最后一个term为XTerm的日志项的下一个index(示例中为2)</li><li>Case 3：没有冲突，仅仅是follower有延迟，令nextIndex为XLen+1(示例中为2)</li></ul><p>所以在上面的3个示例中，都可以快速回退到第2个日志项，而不是一步一步的回退。</p></blockquote><h3 id="安全性-§5-4"><a href="#安全性-§5-4" class="headerlink" title="安全性(§5.4)"></a>安全性(§5.4)</h3><h4 id="选举限制-§5-4-1"><a href="#选举限制-§5-4-1" class="headerlink" title="选举限制(§5.4.1)"></a>选举限制(§5.4.1)</h4><p>Raft规定leader要将所有follower的日志同步成和自己一样，这个规定限制了数据的传输是leader到follower单向的。因此<strong>需要在leader选举时就选出足够新的leader</strong>，如果选比较过时的机器做leader很有可能就会丢失一些已提交日志项(比如图6中的b)。为了能够在选举时选出足够新的机器做leader，选举阶段candidate发送RequestVote请求时需要有两个字段lastLogIndex和lastLogTerm指明自己日志中最后一个日志项的index和term，<strong>机器在收到RequestVote请求时，需要确保candidate最后一个日志项不旧于自己最后一个日志项，才能给它投票</strong>。判断两个日志项的新旧首先比较term，term大的新，term相同的情况下，index大的新。</p><p>这个限制保证了赢得选举的leader的日志不落后于超过一半的机器。Raft中日志项已提交的前提条件就是超过一半的机器存了该日志项，所有已提交的日志项存在了超过一半的机器上，那么和给leader投票的那一大半机器一定有交集至少有一台机器，而leader的日志至少和它一样新，那么leader一定就有这个已提交日志项，那么就保证了<strong>新选出的leader就一定存了所有已提交的日志项</strong>。</p><h4 id="只能显式提交本term日志项-§5-4-2"><a href="#只能显式提交本term日志项-§5-4-2" class="headerlink" title="只能显式提交本term日志项(§5.4.2)"></a>只能显式提交本term日志项(§5.4.2)</h4><p>leader在上任后会将follower的日志同步成和自己一样，这期间也包括向follower同步之前term的日志项，但是<strong>哪怕之前term的日志项已经在超过一半的机器上备份了，leader也不能显式提交之前的term的日志项(即commitIndex设置为该日志项的index)，而是应该提交自己term的日志项顺带把之前term的日志项提交</strong>。之所以这样要求，先看下图所示的例子。<br><img src="/2023/10/15/%E5%85%B1%E8%AF%86%E7%AE%97%E6%B3%95-Raft/%E5%9B%BE7.png" alt="图7" title="图7"><br>上图(a)(b)(c)(e)是成功案例，上图(a)(b)(c)(d)给出了不加这样限制的失败样例：</p><ul><li>(a)中term为2，index为2的日志项(上图黄色)仅复制到S1和S2两台机器上后leader就宕机了。</li><li>(b)中S5为leader(可以获得S3、S4、S5的选票，超过一半)，term为3，仅在自己的日志上添加了index为2、term为3的日志项(上图蓝色)就宕机了。</li><li>(c)中S1成为leader(可以获得S1、S2、S3、S4的选票)，term为4，该leader将自己日志上index为2、term为2的日志项(上图黄色)复制给了S3，使得该日志项在超过一半的机器上备份，<strong>假设此时leader显式提交了这个日志项并且apply到状态机后对外部Client提供了服务，之后这个日志项(上图黄色)就绝对不应该被覆盖、删除</strong>。这个leader随后在自己的日志上添加了index为3、term为4的日志项(上图红色)就宕机了。</li><li>(d)中S5成为leader(可以获得S2、S3、S4、S5的选票)，term为5，该leader将自己日志上index为2、term为3的日志项(上图蓝色)复制给了所有follower，这就<strong>将(c)中已提交的日志项(上图黄色)覆盖掉了，相当于回退了已提交的日志项，违背一致性</strong>！</li></ul><p>之所以会产生这样的结果，是因为<strong>Raft中的日志项始终记录了自己被添加到日志时的term，而因为term在Raft中有着逻辑时钟的地位，选举、复制时低term对高term绝对服从</strong>。在这种情况下，<strong>现任leader假如显式提交了前任的日志项，虽然自己用高term身份做了提交这个操作，但没有给予这个日志项应有的保护(日志项的term还是之前的低term，比如上图c中黄色日志项)<strong>，在自己宕机后，</strong>下一任leader完全可以因为在相同index具有更高term的日志项(比如上图d中蓝色日志项)而覆盖掉现任leader提交的日志项</strong>。为了能够保护自己提交的日志项，Raft就限制了leader只能显式提交当前term的日志项(上图e中红色日志项)，由于Raft中两台机器上某个index日志项的一致隐喻着之前所有日志项的一致(§5.3)，所以就顺带提交了之前term的日志项(上图e中黄色日志项)。<strong>在这种顺带提交的保护下，上图(e)中S5根本无法赢得选举，因为日志比超过一半的机器旧(S1、S2、S3)，就不会覆盖掉已提交的日志项。</strong></p><p>在这样的限制条件下，假如新leader刚上任但是一直没有Client请求，就会无法确认之前的日志项是否已提交，所以更新不了CommitIndex也就无法推进apply日志项的工作，所以往往新leader上任后就添加一个包含空操作的日志项，这样可以快速的确认CommitIndex，顺带提交了之前的日志项。</p><h4 id="安全性证明-§5-4-3"><a href="#安全性证明-§5-4-3" class="headerlink" title="安全性证明(§5.4.3)"></a>安全性证明(§5.4.3)</h4><p>任何一台机器apply某个日志项的前提是该日志项已提交，而已提交的日志项会存在当前及之后所有leader的日志中，并且始终具有相同的index。这确保了机器apply某个日志项时，这个日志项及其之前的日志项都与leader相同，也会与未来的leader相同。所以这确保了<strong>如果一个日志项被某台机器apply了，所有其他机器都会在相同索引处apply同样的日志项</strong>。同时Raft要求机器从前向后apply已提交的日志项，所以不同机器apply日志项的顺序、内容都是完全一样的，保证了多个机器之间状态机的一致。</p><h2 id="集群变更"><a href="#集群变更" class="headerlink" title="集群变更"></a>集群变更</h2><p>Raft集群很可能碰到添加机器、减少机器、替换机器等变更需求，集群成员的变更一般会影响Raft对于大多数(超过一半)这个关键条件的判断，所以并不能是简单添加、删除机器就可以了，变更也需要以日志项的形式让集群中的机器达成共识。但是<strong>整个过程并不是对新配置</strong>$C_{new}$<strong>达成共识，就可以两阶段的将旧配置</strong>$C_{old}$<strong>切换为</strong>$C_{new}$。如下图所示，$C_{old}$中有三台机器，$C_{new}$中新添了两台机器，在同一时刻可以有两台机器(S1、S2)停留在旧配置形成大多数，三台机器切换到新配置(S3、S4、S5)形成大多数，这样可能会导致在集群中选出两个leader，分别遵循旧配置和新配置，使得Raft集群发生脑裂，有两个leader对外服务，导致不一致！<br><img src="/2023/10/15/%E5%85%B1%E8%AF%86%E7%AE%97%E6%B3%95-Raft/%E5%9B%BE8.png" alt="图8" title="图8"><br>为了让配置切换的过程不影响一致性，Raft将配置的切换设计成了下图所示的三阶段。需要注意的是，<strong>与常规日志项不同，机器收到集群配置的日志项时不需要考虑其是否被提交，就要遵守这个配置</strong>。下面介绍一下三个阶段：</p><ol><li>旧配置中的leader收到变更集群配置的请求后，会将当前配置$C_{old}$和新配置$C_{new}$联合成$C_{old,new}$作为新的日志项添加到日志中，并开始复制给$C_{old,new}$包含的所有机器。由于集群配置的日志项不考虑是否被提交就要遵守，所以leader会依据$C_{old,new}$来判断$C_{old,new}$及其之后的日志项是否被提交(之前的只需要满足$C_{old}$即可)。$C_{old,new}$是比较特殊的联合了两个配置的过渡配置，使用$C_{old,new}$判断已提交时，需要既满足$C_{old}$超过一半又要满足$C_{new}$超过一半，这被Raft称为<strong>联合共识</strong>。当leader根据联合共识的要求判定$C_{old,new}$已提交后，就进入了第二阶段。在第一阶段中有可能遇到leader宕机重新选举的情况，重新选的leader一定是旧配置中的机器，由于$C_{old,new}$复制进度不确定，选出来的leader可能具有$C_{old,new}$日志项也可能没有，如果没有的话就整体又维持在了旧配置，需要外部请求再次重试重新发起配置切换。</li><li>$C_{old,new}$日志项提交之后，leader会将$C_{new}$日志项添加到日志中，并且leader需要向$C_{new}$包含的机器发送该日志项。同样由于集群配置的日志项不考虑是否被提交就要遵守，所以leader通过$C_{new}$来判断$C_{new}$及其之后的日志项是否已提交(之前的需要满足$C_{old,new}$的联合共识)。当leader根据$C_{new}$来判断记录$C_{new}$的日志项已经提交，就进入了第三阶段。同样第二阶段中可能遇到leader宕机，由于$C_{new}$复制进度不确定，选出来的leader可能具有$C_{new}$日志项也可能没有，但如论如何leader都会有$C_{old,new}$日志项，可以再次将$C_{new}$日志项添加到日志中，确保了一定会进行配置切换。</li><li>$C_{new}$日志项提交之后，就可以认为配置切换已经完成，新配置中的机器可以依据新配置正常的工作，接下来就是收尾工作。所有不在新配置中的机器可以关停了。还有一种情况是第二阶段中的leader并不在$C_{new}$中，理应被踢出集群，当该leader完成了$C_{new}$日志项的提交(统计是否超过一半时不把自己算进去)工作后，就会选择自动退出，随后新集群会因为心跳超时选出自己的新leader，正式完成了配置切换。</li></ol><p><img src="/2023/10/15/%E5%85%B1%E8%AF%86%E7%AE%97%E6%B3%95-Raft/%E5%9B%BE9.png" alt="图9" title="图9"><br>给集群添加新机器的时候，开始时该机器是一张白纸，假如这时候就进行配置变更，新机器需要追很长时间的日志才能走到配置所在的日志项，追赶期间整个集群的工作都可能处于等待状态。为了避免因为配置更新带来可用性的损失，Raft规定新机器加入集群前要先“伴飞”一段时间，这期间不断追赶日志，但不参与任何投票，等日志追的差不多了，就可以开始配置变更，整个配置变更过程就会更快。</p><p>还有一个问题就是假如没有及时关闭在旧配置中但不在新配置中的机器，这些机器很可能不断的处于“心跳超时-增加term-发起选举”的循环，它发送给新配置中机器的RequestVote请求很容易引起新集群中不必要的选举。为了避免这些旧机器的骚扰，Raft规定机器收到leader心跳后的一段时间内无视RequestVote请求，这个一段时间被设置为心跳超时时间的最小值，所以只要leader和follower之间的心跳正常，就不会被这些RequestVote骚扰请求打搅。同时这并不影响正常的选举，当leader宕机后，没有了心跳信息，follower在超时后将不会无视RequestVote请求，可以正常进入选举流程。</p><h2 id="日志压缩"><a href="#日志压缩" class="headerlink" title="日志压缩"></a>日志压缩</h2><p>可以看出，Raft用不断增长的日志记录了所有操作，随着服务的不断运行，日志会越来越长，而日志是所有机器必须持久化的状态，这会给机器带来没有上限的硬盘需求。为了避免这个问题，Raft采用快照的形式压缩日志，可以把<strong>已提交的</strong>日志项apply到状态机，把状态机的当前状态做个快照，之后基于快照中的状态在其上apply后续的日志项就可以了，快照所包含的日志项就可以被删掉而不占用磁盘空间。快照不光leader可以做，follower也可以在leader不知情的情况下自己做快照(判断条件一般是日志长度达到阈值)。对于之前kv存储的例子，日志压缩的示意图如下：<br><img src="/2023/10/15/%E5%85%B1%E8%AF%86%E7%AE%97%E6%B3%95-Raft/%E5%9B%BE10.png" alt="图10" title="图10"><br>Raft的快照不光需要存状态机的状态，还需要记录包含的最后一个日志项的index和term，这样可以在快照后紧接着AppendEntries时用来判断prevLogIndex和prevLogTerm是否一致。lastIndex也会在机器故障恢复时用于初始化commitIndex和lastApplied。为了兼容集群变更，快照中需要记录所见到的最后一个配置，这样就不需要存对应的配置日志项了。</p><p>日志压缩还会带来一个问题，当leader给落伍很久的follower发日志项的时候，follower所需的一些日志项可能已经被leader做快照的时候删掉了，那么就需要换一个思路，不传日志项而是传快照。所以就有了下图所示新的RPC接口InstallSnapshot，用于leader向落伍很久的follower传快照。follower接到足够新的快照的时候，就直接存起来，假如快照中最后一个日志项的index和term与自己对应index的日志项冲突，那就把自己的日志全删掉；如果不冲突那就删掉快照包含的日志项，保留快照不包含的后续日志项。<br><img src="/2023/10/15/%E5%85%B1%E8%AF%86%E7%AE%97%E6%B3%95-Raft/%E5%9B%BE11.png" alt="图11" title="图11"></p><h2 id="客户端交互"><a href="#客户端交互" class="headerlink" title="客户端交互"></a>客户端交互</h2><p>Raft集群中leader全权授理Client请求，Client假如发请求给follower，follower会拒绝请求并告知Client目前知道的leader，接下来Client就会找leader发送请求。</p><p>Raft目标要给客户端提供强一致性语义，Client发起的请求可能已经被leader提交了，但是可能没来得及回复leader就宕机或者是发送了回复但因为网络故障没有送到Client，Client就会认为自己的请求失败进而发起重试，重试可能会使得Client认知中的状态与集群维护的状态机不一致(比如Client给某个v加1，在Client视角成功了一次，但是实际上Raft集群成功完成两次，给v加了2)。所以Raft集群需要做<strong>幂等处理</strong>，机器的状态可以添加一个Hash表来处理幂等，记录下&lt;ClientID,RequestID&gt;到回复的映射，当遇到相同Client的相同请求的时候直接返回预存回复即可，每个机器apply日志项的时候就会更新这个Hash表，哪怕切换了leader，这个Hash表也是一致的，就避免了Client重试对一致性的影响。</p><p>虽然只读请求不会修改状态，但是假如直接让leader根据自己的状态回复Client的只读请求是可能违反一致性的。比如某个leader S1被网络分区了，虽然它还认为自己是leader，但由于无法联系超过一半的机器，将无法推进任何日志项的提交，另外的网络分区很可能选举了新的leader S2已经提交了很多新日志项。假如有Client发送只读请求到S1，S1自认为leader就直接回复，但其实回复的是过时数据，因为S2已经推进了很多新日志项的提交！所以对于只读请求，并不能是leader直接返回就可以，一种最直白的解决方式就是把<strong>只读请求也以日志项的形式在多个机器中同步，当提交了该日志项后leader可以apply该日志项，这时再根据状态机回复Client，这就保证了一致性</strong>。</p><p>还有一种更复杂但效率更高的方式处理只读请求，就是赋予leader租约，每当leader无论是RequestVote还是AppendEntries获得了大多数肯定回复，就会赋予leader一个租约，哪怕leader发生了切换，新leader也需要等到上一个leader的租约过期才允许处理写请求。<strong>新leader等待租约过期才会处理写请求保证了旧leader在持有租约期间返回的数据不可能是旧数据，进而保证了一致性</strong>。</p>]]></content>
    
    
    <categories>
      
      <category>MIT6.824 分布式系统</category>
      
    </categories>
    
    
    <tags>
      
      <tag>读论文</tag>
      
      <tag>共识算法</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>分布式计算 Spark</title>
    <link href="/2023/10/15/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%A1%E7%AE%97-Spark/"/>
    <url>/2023/10/15/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%A1%E7%AE%97-Spark/</url>
    
    <content type="html"><![CDATA[<p>论文标题为《Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing》，发表于2012年，这篇论文主要介绍了现如今已经被广泛使用的批式计算框架Spark的最初设计，使用RDD的抽象将计算任务抽象成依赖图，利用内存驻留达到很好的性能，并且可以很好地支持循环计算任务。</p><h2 id="RDD定义"><a href="#RDD定义" class="headerlink" title="RDD定义"></a>RDD定义</h2><p>Spark规范了计算过程中的中间结果形式——RDD(Resilient Distributed Dataset，弹性分布式数据集)，一个RDD是一个只读的已分区的数据集，其中存放了一系列它所负责的分区包含的记录。RDD可以依据持久化存储(例如HDFS)或者其他RDD产生，从持久化存储创建RDD其实就是对应了读数据的过程，从其他RDD产生其实对应的是在父RDD上进行map、fliter等变换操作产生新的子RDD的过程。Spark会在计算任务提交(比如collect方法)时，构建出所有RDD的依赖关系图，例如下图。图是有向无环的，并且起始的RDD要么来源于持久化存储，要么来源于用户程序中的初始化赋值，这种设计再加上RDD的只读性使得当故障发生时，一定可以根据故障RDD向前追溯的拓扑排序把故障的RDD再恢复出来，对于使用了随机数的变换操作，会记录下随机种子用于确定性恢复。Spark比Mapreduce要快，很大程度是<strong>尽量让RDD驻留在内存中(内存不够时使用LRU替换)，可以多轮计算复用</strong>，比如下图的links在多轮被使用，就可以显式的将它放在内存中。还有一点Spark明显的优势就在于，Mapreduce只抽象出了map、reduce两种操作，多轮的计算需要多个Mapreduce任务串起来一个接着一个执行，整个过程有多次任务的启停，非常耗时，Spark则可以<strong>很好的支持多轮的计算</strong>，都放在一个计算任务里，省去了子任务的启停时间。<br><img src="/2023/10/15/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%A1%E7%AE%97-Spark/%E5%9B%BE1.png" alt="图1 Page Rank RDD依赖图" title="图1 Page Rank RDD依赖图"></p><h2 id="变换算子"><a href="#变换算子" class="headerlink" title="变换算子"></a>变换算子</h2><p><img src="/2023/10/15/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%A1%E7%AE%97-Spark/%E5%9B%BE2.png" alt="图2" title="图2"><br>变换(Transformation)用于在已有RDD上做变换得到新的RDD，动作(Action)用于最后结果的输出，也代表了计算任务的提交，会整理出RDD依赖图。</p><h2 id="任务执行"><a href="#任务执行" class="headerlink" title="任务执行"></a>任务执行</h2><p><img src="/2023/10/15/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%A1%E7%AE%97-Spark/%E5%9B%BE3.png" alt="图3" title="图3"><br>要使用Spark，开发人员需要在本地Driver上编写Spark代码，Driver负责连接到Worker集群并向它们发送编译后的程序字节码，如上图所示。Spark代码中定义一个或多个RDD并在它们上调用Action以需要的形式输出结果。Driver为Spark代码构建RDD依赖图，并在整个任务生命流程中跟踪。Worker是长期进程，可以在整个计算任务生命流程中跨Transformation操作的在内存中存储负责的RDD分区。一般来说，计算任务的输入来源于分布式文件系统(如HDFS)，所以Worker一般也就是运行HDFS的存储节点做计算任务的兼职，可以从本地读取数据。一个RDD通常因为各个Worker所在机器作为存储节点负责的不同范围数据而进行分区(Partition)，尽量使得计算本地化。但是Spark同样提供给用户选择是否自定义分区规则，比如一个RDD的内容是1～10000的数字，可以每按照值的大小每1000条构建一个分区，那这个RDD就有1～1000、1001～2000…9001～10000十个分区，分别由不同的Worker负责。</p><h2 id="执行计划"><a href="#执行计划" class="headerlink" title="执行计划"></a>执行计划</h2><p>首先区分一下Spark定义的窄依赖、宽依赖。从前文可知Spark会为计算任务构建出RDD依赖图，在RDD依赖图中，一个或多个RDD产生一个新的RDD只需要一条有向边，但在实际执行的时候就不仅仅是一条边这么简单了。因为RDD内部有分区(Partition)，所以从父RDD的每个分区到子RDD的每个分区有时并不是一对一的生成关系，有可能每个子RDD分区需要多个父RDD分区一起产生出来(例如join、groupby，很像Mapreduce中的reduce阶段)。所以Spark区分了两种依赖关系，窄依赖的情况下，子分区依赖单个父RDD中的一个且仅一个父分区；宽依赖的情况下，子分区依赖单个父RDD中的多个父分区，示例见下图。<br><img src="/2023/10/15/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%A1%E7%AE%97-Spark/%E5%9B%BE4.png" alt="图4" title="图4"><br>窄依赖使得这一串RDD计算可以在单台机器上流水线式地完成，无需和其他机器跨分区交流，可以最大程度的并行化。但是造成宽依赖的变换操作使得这种美好愿望破灭，会采用类似Mapreduce的方式，父RDD的每台Worker计算出中间结果并Shuffle后(很像Mapreduce的Map阶段)，子RDD的每台Worker则根据自己所需要的分区范围找所有的父分区Worker要自己所需的输入数据然后进行处理(很像Mapreduce的Reduce阶段)。</p><p>由于两种依赖的不同，Spark根据宽依赖将执行计划划分了阶段(Stage)，如下图所示。所有可以独立地、并行地计算的窄依赖算子都被放在同一个Stage中，在一个Stage中的不同计算子任务之间除了父子依赖没有其他交互，可以无需等待地在本地尽快完成。所有宽依赖被作为划分Stage的依据，因为宽依赖要求子任务等待所有父任务完成，并且这个过程需要跨机器、跨分区的网络IO。Spark在调度任务的时候会默认先看输入数据的本地性，由于宽依赖的单个子RDD分区在遇到故障的时候需要寻找很多父分区进行恢复，这可能引发连锁反应，所以Spark的一个解决方案是宽依赖的父分区所在机器始终在内存中保留该分区，以备子分区恢复时使用。<br><img src="/2023/10/15/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%A1%E7%AE%97-Spark/%E5%9B%BE5.png" alt="图5" title="图5"></p><h2 id="检查点"><a href="#检查点" class="headerlink" title="检查点"></a>检查点</h2><p>虽说Spark可以根据RDD依赖图在任何Worker故障的时候重新恢复出其上所有RDD分区，但是类似图1中这种循环任务或者宽依赖的存在，使得在RDD依赖图后半部分的单个Worker故障时，可能引起前面一长串或者很多父分区的重新计算，这种情况会大大影响故障恢复的速度，所以Spark也提供了检查点机制，可以由用户定义将某些关键的RDD存放在持久化存储中，这样就避免了故障恢复时的长串重复计算。对于循环任务，可以每几个循环将中间结果RDD持久化一下，就大大节省了故障恢复的时间。</p><h2 id="Page-Rank示例代码"><a href="#Page-Rank示例代码" class="headerlink" title="Page Rank示例代码"></a>Page Rank示例代码</h2><p>RDD依赖图参见图1。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs scala"><span class="hljs-keyword">val</span> lines = spark.read.textFile(<span class="hljs-string">&quot;in&quot;</span>).rdd<br><span class="hljs-keyword">val</span> links1 = lines.map&#123; s =&gt;<br>  <span class="hljs-keyword">val</span> parts = s.split(<span class="hljs-string">&quot;\\s+&quot;</span>)<br>  (parts(<span class="hljs-number">0</span>), parts(<span class="hljs-number">1</span>))<br>&#125; <span class="hljs-comment">//从文件中读出一个个(src_url, dst_url)对</span><br><span class="hljs-keyword">val</span> links2 = links1.distinct() <span class="hljs-comment">//根据hash结果去重，实际相当于一次分区</span><br><span class="hljs-keyword">val</span> links3 = links2.groupByKey() <span class="hljs-comment">//根据scr_url聚合出dst_url List</span><br><span class="hljs-keyword">val</span> links4 = links3.cache() <span class="hljs-comment">//使links驻留在内存，后续频繁使用</span><br><span class="hljs-keyword">var</span> ranks = links4.mapValues(v =&gt; <span class="hljs-number">1.0</span>) <span class="hljs-comment">//ranks根据links的分区而分区，后面join的时候可以避免宽依赖，初始所有url的rank都为1</span><br><br><span class="hljs-keyword">for</span> (i &lt;- <span class="hljs-number">1</span> to <span class="hljs-number">10</span>) &#123; <span class="hljs-comment">//十次循环计算ranks</span><br>  <span class="hljs-keyword">val</span> jj = links4.join(ranks)<br>  <span class="hljs-keyword">val</span> contribs = jj.values.flatMap&#123;<br>    <span class="hljs-keyword">case</span> (urls, rank) =&gt;<br>      urls.map(url =&gt; (url, rank / urls.size))<br>  &#125;<br>  ranks = contribs.reduceByKey(_ + _).mapValues(<span class="hljs-number">0.15</span> + <span class="hljs-number">0.85</span> * _)<br>&#125;<br><br><span class="hljs-keyword">val</span> output = ranks.collect() <span class="hljs-comment">//调用action方法，会构建RDD依赖图，启动任务</span><br>output.foreach(tup =&gt; println(<span class="hljs-string">s&quot;<span class="hljs-subst">$&#123;tup._1&#125;</span> has rank:  <span class="hljs-subst">$&#123;tup._2&#125;</span> .&quot;</span>))<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>MIT6.824 分布式系统</category>
      
    </categories>
    
    
    <tags>
      
      <tag>读论文</tag>
      
      <tag>分布式计算</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>分布式计算 MapReduce</title>
    <link href="/2023/10/15/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%A1%E7%AE%97-MapReduce/"/>
    <url>/2023/10/15/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%A1%E7%AE%97-MapReduce/</url>
    
    <content type="html"><![CDATA[<p>论文标题为《MapReduce: Simplified Data Processing on Large Clusters》，发表于2004年，这篇论文主要介绍了Google三驾马车之一MapReduce，创造性地抽象出Map、Reduce两阶段的计算范式，基于GFS完成大数据量批式处理，对之后的分布式计算框架具有深远的意义。</p><h2 id="抽象模型"><a href="#抽象模型" class="headerlink" title="抽象模型"></a>抽象模型</h2><p>MapReduce创造性地抽象出了计算任务所需要的范式，即Map阶段和Reduce阶段。Map接受一个输入对&lt;k1,v1&gt;并产生一组中间键&#x2F;值对list&lt;I,v2&gt;，Map阶段结束后将与相同中间键I相关的所有中间值组合在一起，并将它们传递给Reduce阶段。Reduce阶段接受中间键I和该键的一组值&lt;I,list<v2>&gt;，它将这些值合并在一起，形成一个可能更小的值集list<v3>。通常每次Reduce调用只产生零或一个输出值。中间值通过迭代器模式提供给用户的Reduce函数，这允许函数处理过大而无法在内存中容纳的值列表。下图是经典的WordCount示例。<br><img src="/2023/10/15/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%A1%E7%AE%97-MapReduce/%E5%9B%BE1.png" alt="图1" title="图1"></p><h2 id="整体架构"><a href="#整体架构" class="headerlink" title="整体架构"></a>整体架构</h2><p><img src="/2023/10/15/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%A1%E7%AE%97-MapReduce/%E5%9B%BE2.png" alt="图2" title="图2"><br>结合上图介绍一下一次计算任务的生命流程：</p><ol><li>用户程序中的MapReduce库会根据输入文件的大小，按照64MB的粒度(可以被用户自定义)将Map阶段的处理并行度调成<code>M=size(file)/64MB</code>。Reduce阶段的并行度<code>R</code>和分区函数(例如hash(key))则直接由用户指定。M和R理应被设置的远远大于集群机器数量，可以更小粒度的安排计算任务，方便动态负载均衡。随后，用户程序中的driver就会将程序拷贝到很多worker之上，并且有一个特殊的Master统筹管理整个计算任务。</li><li>Master会承担起分配任务的角色，会根据数据的就近性原则挑出M个Map阶段的worker执行Map函数；并在之后安排R个Reduce阶段执行Reduce函数的worker。</li><li>Map阶段的worker会从分布式文件系统中读取自己所负责的文件块，进行Map函数所定义的处理。</li><li>Map阶段的worker会将产生的中间结果换存在内存中，并周期性地写到本地硬盘中，写入时会根据用户定义的分区函数将对应的中间结果存放在相应的桶中方便之后Reducer读取。这些中间结果在本地硬盘的存储位置会由Mapper在完成计算后上报给Master。</li><li>当Master收到了所有Mapper完成任务的通知后，就会给每个Reducer发送他需要向谁的哪个位置读取中间结果用于后续Reduce处理。Reducer接到通知就会到各个Mapper那里远程读取自己需要的那部分数据，并将得到的所有数据进行大排序，聚合相同key的value得到一个list。</li><li>Reducer在自己预处理好的数据上执行Reduce函数，最终以输出文件的形式将结果输出(文件名由用户指定)。为了避免容错机制导致的多个Reducer写同一个分区的输出文件，MapReduce利用了GFS的原子修改文件名的操作，Reducer在输出时使用一个临时文件名，当完成后则原子修改文件名为目标文件名，这防止了多个Reducer同时写一个文件，并且最终只产生一个目标输出文件。当所有Reducer都告知Master已完成任务后，整个计算任务就完成了。</li></ol><h2 id="容错机制"><a href="#容错机制" class="headerlink" title="容错机制"></a>容错机制</h2><h3 id="Worker故障"><a href="#Worker故障" class="headerlink" title="Worker故障"></a>Worker故障</h3><p>Master会周期性地和各个Worker进行心跳检查，如果Worker没有心跳，Master会认为该Worker故障。Master发现的任何处于未完成状态的Mapper、Reducer故障都会重新安排新的Worker跑一遍。任何处于已完成状态的Mapper故障也会被找一个新的Worker重跑，因为原本已完成的Mapper是在本地硬盘中存放了中间结果，它的宕机会导致后续的Reducer无法读到中间结果，所以需要重跑一遍提供中间结果读取的服务。任何处于已完成状态的Reducer故障则不用管，因为已完成的Reduce任务会将结果已经输出到了分布式文件系统中而非本地硬盘，所以无需重跑。</p><h3 id="Master故障"><a href="#Master故障" class="headerlink" title="Master故障"></a>Master故障</h3><p>可以用周期性检查点的方式方便故障恢复，但是考虑到单机故障没有那么普遍，所以论文中给出的现实解决办法是直接结束整个任务。</p><h3 id="后备任务"><a href="#后备任务" class="headerlink" title="后备任务"></a>后备任务</h3><p>有些Worker所在的机器可能因为硬件问题运行得很慢，正所谓舰队的行进速度取决于最慢的那一艘，一些慢的Worker会拖累整体节奏，所以MapReduce采用后背任务的方式，当整体任务趋近于完成的时候，MapReduce会为那些还在未完成状态的Worker找额外的机器做同样的工作，避免这些慢Worker因为自己机器慢影响整体的进度。</p>]]></content>
    
    
    <categories>
      
      <category>MIT6.824 分布式系统</category>
      
    </categories>
    
    
    <tags>
      
      <tag>读论文</tag>
      
      <tag>分布式计算</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>分布式文件系统 GFS</title>
    <link href="/2023/10/15/%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F-GFS/"/>
    <url>/2023/10/15/%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F-GFS/</url>
    
    <content type="html"><![CDATA[<p>论文标题为《The Google File System》，发表于2003年，这篇论文主要介绍了Google三驾马车之一GFS，GFS使用大量普通的机器构建出了高吞吐(大批量连续读写，而非低延迟的点查、随机写)、高可用(多备份容错)的分布式文件系统，具有划时代意义。</p><h2 id="架构设计"><a href="#架构设计" class="headerlink" title="架构设计"></a>架构设计</h2><p><img src="/2023/10/15/%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F-GFS/%E5%9B%BE1.png" alt="图1" title="图1"></p><p>GFS集群包含了一个负责存储元数据的master和很多个负责存储数据、真正读写数据的chunkserver。GFS主要应对的是较大的文件(100MB以上，数GB也很常见)，为了方便存储大文件，GFS将文件拆分成多个64MB的chunk，每个文件都是由一个或多个chunk组成，每个chunk是基本的存储、容错单元，被存在多个(默认3个，分布在不同机架)chunkserver上。虽然呈现的是文件系统，但是client真正读写的是chunk，GFS会给每个chunk赋予一个64bit全局唯一的chunk handle，可以唯一标识某个chunk。64MB的大小(相比单机4KB硬盘页)的chunk虽然会更容易产生碎片，但是可以使得文件包含的chunk数有效减少，master需要维护的状态将变得更小，可以管理更多、更大的文件，对于client来说也可以更少地切换与不同chunkserver之间的TCP连接。</p><p>master存了整个GFS集群的元信息，包括了目录结构、权限控制、文件到包含的chunk的映射、chunk所在位置(机器)等等。master不处理真正的数据读写，在client读写时主要起到路由的作用，client想要读写某个文件的某个chunk时，先联系master得到chunk handle和chunk所在的chunkserver，随后使用chunk handle和对应的chunkserver交互获得真正的数据，文件到chunk再到chunkserver的映射关系client会自己缓存着避免频繁找master查相同的映射信息。</p><p>为了性能考虑master将元信息直接存在内存之中，所有关键元信息的修改都需要预写到操作日志(持久化存储)中，这样可以保证故障后可以恢复。为了方便恢复，master会定期给内部状态做快照(不影响master处理新请求)存到硬盘中，但是需要注意的是“chunk所在位置”这部分信息不会持久化，这一部分信息是master启动的时候跟各个chunkserver心跳沟通的时候得到的，相当于把这部分信息持久化的工作分摊给了各个chunkserver。</p><p>master是GFS中的大脑，如果发生了单点故障，并不会影响已有的数据读写，但会影响即将发生的数据读写以及元信息的修改。GFS的设计中有影子master，实时同步着master的状态，有一定延迟(1s左右)，但是在master宕机后恢复期间，影子master会承担起master路由读写请求的责任，不能够进行元信息修改。</p><h2 id="一致性保证"><a href="#一致性保证" class="headerlink" title="一致性保证"></a>一致性保证</h2><h3 id="写请求处理"><a href="#写请求处理" class="headerlink" title="写请求处理"></a>写请求处理</h3><p>GFS支持Random写和Append写两种写操作，GFS应对的主要是大量Append写，虽然支持Random写，但是性能会比较差。GFS保证在单个chunk上的多个修改操作需要在不同副本chunkserver上按照相同顺序执行，保证单chunk上修改操作的顺序性。GFS通过给每个chunk选一个primary chunkserver来达成顺序性的需求，master负责给primary chunkserver发租约(大概60s)，在租约有效期间，primary chunkserver负责主导chunk的写操作，可以通过和master的心跳信息续约。所有跨chunk的写请求会被打散成只写单chunk的多个写请求，写单个chunk的写请求处理流程如下所示：</p><p><img src="/2023/10/15/%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F-GFS/%E5%9B%BE2.png" alt="图2" title="图2"></p><ol><li>Client首先找master要chunk存在哪些chunkserver上，并且要知道primary chunkserver是谁(没有的话master会当即选一个)</li><li>master返回给Client它要的chunk位置映射信息，以及primary chunkserver</li><li>Client将要写的内容就近发送给某个chunkserver(并不一定是primary chunkserver)，然后该chunkserver再链式转发给所有chunkserver，chunkserver拿到数据后在LRU缓存中暂存起来，直到数据被使用或者过期被替换</li><li>已经预先传完数据后，Client向primary chunkserver要求开始写操作，并且指明刚才传递的数据。primary chunkserver会依据请求到来的先后给操作排序，并将写操作apply到自己的存储中</li><li>primary chunkserver将所有排好序的请求按序转发给其他chunkserver</li><li>其他chunkserver按照primary chunkserver排好的顺序apply各个写操作</li><li>primary chunkserver需要根据所有chunkserver是否写成功将结果返回给Client。首先如果primary chunkserver写失败那么这个写请求就不会被分配序列号，不会被转发给其他chunkserver，写请求直接是失败的；假如任意一个chunkserver写失败，但是其他的chunkserver成功了，那么写请求也被认为是失败的，并且这使得所有副本处于不一致的状态。对于失败的写请求，Client会不断重试3-7步来解决副本不一致的问题。</li></ol><p>对于chunkserver因为宕机或者断连错过很多写请求导致chunk存的是过时数据的问题，master为每个chunk赋予了版本号，版本号会在赋予primary chunkserver租约的时候增进，并且通知到所有chunkserver，master和chunkserver都需要持久化这个版本号。当存有过期数据的chunkserver后续重回集群时，master会检查它心跳信息中包含的chunk版本号是否足够新，如果是旧的就认为这个副本上的chunk已过期，就把其上的chunk垃圾回收掉。master在给Client路由读请求的时候只会给版本号达到最新的chunkserver，落伍的不会返回给Client，并且在master看来过期的chunk就已经是等待被回收的垃圾，它所在的chunkserver也不被认为是该chunk的一个备份机器了。</p><h3 id="至少一次Append写"><a href="#至少一次Append写" class="headerlink" title="至少一次Append写"></a>至少一次Append写</h3><p>对于很多应用，Append写很关键，比如对于消息队列而言，多个写者Append写同一个文件，读者读该文件就可以模拟消息队列。除了按照图2的流程外，GFS对于Append写还有一些特殊的设定，以应对多个写者并发Append写的场景。</p><p>GFS规定Append写的数据大小必须小于chunk size(默认64MB)的1&#x2F;4，这个要求是因为单个Append写不能跨chunk，当primary chunkserver发现此次Append写会跨chunk的时候，就会将写操作变成把当前chunk的剩余部分都用padding填充，然后告知Client去新创建的下一个chunk进行Append写。</p><p>与Random写不同，Append写不需要Client指定写的offset，只用给数据就可以，primary chunkserver会根据自己chunk的当前结束位置决定Append写的offset，并同步给其他chunkserver。如果遇到了多个副本局部Append写失败的问题，依靠Client不断重试来保证最终的成功，重试会使得有的副本重复写了多次同样的数据，有的副本则只写了一次，GFS并不在意副本之间bit级别的一致，在意的是最终成功后，多个副本在成功的那次offset都能读到Append写入的数据，所以是保证至少一次Append写(例如下图就是三次重试，最终一次成功后三个副本的存储内容)，至于某些副本上重复内容的问题，就需要读者来进行幂等。需要注意的是哪怕多个Append写期间切换了primary chunkserver并且此chunkserver上chunk结束位置比较早，也不会因为后续的Append写令之前成功的Append写被覆盖，因为成功的Append写一定是让所有副本都在相同offset写成功，某个副本结束位置最早也就是最后一次成功Append写的结束位置，这保证了成功Append写一定不会被覆盖，至于失败的Append写被覆盖了也无所谓。</p><p><img src="/2023/10/15/%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F-GFS/%E5%9B%BE3.jpeg" alt="图3" title="图3"></p><h3 id="数据正确性检查"><a href="#数据正确性检查" class="headerlink" title="数据正确性检查"></a>数据正确性检查</h3><p>正如前面介绍，GFS不保证各个副本上bit级别的一致，有时候硬件错误会导致数据损坏，因为每个副本上存的内容很可能不一样(如图3)，所以需要每个chunkserver负责检查自己的chunk的正确性。每个chunk被切分成64KB的块，每个块都有32bit的校验和，每个块的校验和被作为元信息存在chunkserver的内存中，并且使用日志进行持久化。在响应读请求的时候，chunkserver需要根据校验和检查数据是否正确，只有正确的数据才返回给Client，如果不正确就上报给master等待这个chunk被删除。</p><p>在进行Append写时，第一个块通常并不是一个崭新的块，而是写了一部分的块，chunkserver可以直接基于这个块上已有的校验和以及新写的内容增量计算出新的校验和，可以这么做的原因是Append写不会覆盖原有数据，原有数据如果损坏了，新的校验和依然可以用于检查原有数据是否正确。Append写的后续块，则需要根据每个块完整计算出校验和。</p><p>在进行Random写时，完整覆盖的块可以直接重新计算出新的校验和，没有完整覆盖的块则需要先校验原块是否正确，验证正确后才可以修改然后整体计算新的校验和。非完整覆盖块之所以要先检查是为了防止掩盖原块已经损坏的数据，假设原块已经损坏，未经检查就进行修改并计算新的校验和，那么原本损坏的内容就这样瞒天过海了。</p><p>chunkserver会有后台任务扫描检查各个chunk的正确性，不正确就会上报给master等待被删除。</p><h2 id="Master职责"><a href="#Master职责" class="headerlink" title="Master职责"></a>Master职责</h2><h3 id="元信息修改"><a href="#元信息修改" class="headerlink" title="元信息修改"></a>元信息修改</h3><p>GFS虽然呈现出目录树结构，但是实际是使用全路径来组织文件。元信息的修改是对正确性要求很高但是频率不高的操作，所以master不光需要预写日志来保障故障后恢复，还需要确保元信息修改是互斥、顺序执行的。master给每个文件(目录)都赋予了读写锁，读锁共享，写锁互斥，当一个修改操作要创建“&#x2F;A&#x2F;B&#x2F;C”时，需要给“&#x2F;A”和“&#x2F;A&#x2F;B”加读锁，给“&#x2F;A&#x2F;B&#x2F;C”加写锁，保证整个修改过程没有冲突发生。</p><h3 id="chunk创建、复制、负载均衡"><a href="#chunk创建、复制、负载均衡" class="headerlink" title="chunk创建、复制、负载均衡"></a>chunk创建、复制、负载均衡</h3><p>master创建新chunk时需要选好存chunk的若干个chunkserver。在有chunkserver宕机后，master需要给其上的chunk找替代chunkserver，来保证容错要求，这时会根据一些条件为其上每个chunk选择新的chunkserver，并找到足够新的chunkserver将其上的chunk复制到替代者上。master还需要负责负载均衡，将负载过高的chunkserver上的chunk匀一些给其他负载较低的chunkserver。</p><h3 id="快照"><a href="#快照" class="headerlink" title="快照"></a>快照</h3><p>GFS支持对文件系统、目录进行快照，使用了Copy-on-Write技术，整个快照的过程是懒复制的。首先，master会召回所有涉及到的chunk的租约，避免master不知情的写请求修改了当前快照要求的数据。之后master会在目录树中添加所有快照文件的元信息，元信息中指向的chunk和真正文件指向的chunk一样(有些像软链接，引用计数变成了2)。之后再有某个需要快照的chunk上的写请求，master会注意到引用计数为2，就会新复制出来一个新的chunk，然后把真正文件指向的chunk改为新建的这个chunk，之后就如正常写操作一样，赋予primary chunkserver租约后，client就在新的chunk上进行写操作，老的chunk就留给了快照。</p><h3 id="垃圾回收"><a href="#垃圾回收" class="headerlink" title="垃圾回收"></a>垃圾回收</h3><p>GFS中文件的删除并不是立即发生的，而是先进行一个重命名操作，相当于把文件移到回收站，这期间还可以找回。master中有一个后台任务定期扫描回收站，删除在回收站中呆了超过3天文件的元信息。同时，后台任务还会扫描所有chunk的引用计数，引用计数为0的chunk的元信息也会被删除。当master上不存在某个chunk的元信息后，假如收到了某个chunkserver的心跳信息说自己拥有这个chunk，master会令其删除，至此便完成了数据的真正删除。</p>]]></content>
    
    
    <categories>
      
      <category>MIT6.824 分布式系统</category>
      
    </categories>
    
    
    <tags>
      
      <tag>读论文</tag>
      
      <tag>分布式文件系统</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>浅析MySQL GTID</title>
    <link href="/2023/10/15/%E6%B5%85%E6%9E%90MySQL-GTID/"/>
    <url>/2023/10/15/%E6%B5%85%E6%9E%90MySQL-GTID/</url>
    
    <content type="html"><![CDATA[<h1 id="一、总览"><a href="#一、总览" class="headerlink" title="一、总览"></a>一、总览</h1><p>在GTID(Global Transaction Identifier)诞生之前，MySQL主备同步是使用“<strong>文件名+位移</strong>”来确定binlog的复制位点的，在实际使用中很不便利。因此GTID应运而生，每个被写入binlog的事务都被分配了一个全局唯一的GTID，主库和备库通过计算<strong>已执行GTID的集合差集</strong>就可以确定复制位点了，大大简化确认位点的流程。</p><p>GTID是与主库上提交的每个事务关联的唯一标识符。需要注意的是，不被写入binlog的事务(例如，事务被过滤、事务是只读的)，则不会被分配 GTID，<strong>GTID和binlog是强相关的</strong>。</p><p>由于备库重放事务时binlog中已经有了GTID，所以会使用@@SESSION.gtid_next变量直接指定事务的GTID，就不会像主库一样给事务分配新的GTID。为了保证复制的<strong>幂等性</strong>，一旦在给定服务器上提交了某个事务，则该服务器将忽略具有相同GTID的后续事务。</p><p>本文将对GTID相关数据结构、状态维护、初始化流程、生命周期进行探究，其中包含的源码取自MySQL 8.0.34，想要深入了解代码实现的同学可以根据注释阅读。</p><h1 id="二、为什么需要这么一个ID"><a href="#二、为什么需要这么一个ID" class="headerlink" title="二、为什么需要这么一个ID"></a><strong>二、为什么需要这么一个ID</strong></h1><p>对于分布式系统保证一致性而言，最简单的情况其实就是主备复制，为了能达成一致性，是需要备库把主库执行的事务都执行一遍，并且是按照主库的顺序，这就是<strong>全局顺序一致</strong>。当然为了备库回放的速度，并不需要严格按照主库的顺序，不具有依赖关系的不同事务是可以并发回放的，这也就是Writeset所做的工作，它大概模拟了一个事务间的<strong>因果顺序</strong>。但是仅仅有顺序是不行的，我们需要在分布式系统中唯一标识一个事务，并且能够知道一个事务是否已经执行，那么最简单的办法就是给每个事务分配一个<strong>全局唯一ID</strong>，并且追踪事务是否已执行来保证<strong>幂等性</strong>，那么其实到这里就可以发现GTID就承担起了这个责任，它是我们保证全局顺序一致的一分子。</p><h1 id="三、如何开启GTID"><a href="#三、如何开启GTID" class="headerlink" title="三、如何开启GTID"></a><strong>三、如何开启GTID</strong></h1><h2 id="gtid-mode"><a href="#gtid-mode" class="headerlink" title="gtid_mode"></a><strong>gtid_mode</strong></h2><ul><li>Scope : Global</li><li>Dynamic : Yes</li><li>Type : Enumeration</li><li>Default Value : OFF</li><li>Valid Values：<ul><li>OFF：新的和复制事务都使用anonymous</li><li>OFF_PERMISSIVE：新的事务都使用anonymous，而复制事务可以使用GTID或anonymous</li><li>ON_PERMISSIVE：新的事务都使用 GTID，而复制事务可以使用GTID或anonymous</li><li>ON：新的和复制事务都使用 GTID</li></ul></li></ul><p>其中 anonymous transaction 用 binlog file 和 position 来标识事务，需要注意的是 SET gtid_mode 的值必须是临近状态，不能跳着设置，即只能如下渐变修改：</p><blockquote><p>OFF&lt;-&gt;OFF_PERMISSIVE&lt;-&gt;ON_PERMISSIVE&lt;-&gt;ON</p></blockquote><p>为啥要求这么奇葩？是因为gtid_mode从OFF到ON的切换需要如下流程：</p><ol><li>将所有机器的gtid_mode从OFF设置为OFF_PERMISSIVE，这时所有机器作为备库都可以接受GTID事务</li><li>将所有机器的gtid_mode从OFF_PERMISSIVE设置为ON_PERMISSIVE，这时所有机器作为主库的新事务都变成了GTID事务，在这个状态下机器之间传输的事务<strong>同时有GTID事务和匿名事务</strong></li><li><strong>等待所有匿名事务被复制完毕</strong>，这时互相传输的事务只剩下GTID事务了，就可以将所有机器的gtid_mode从ON_PERMISSIVE设置为ON了</li></ol><p>可以看到ON_PERMISSIVE这个状态就是为了等待遗留匿名事务的复制完成；同理OFF_PERMISSIVE状态就是gtid_mode从ON到OFF时用于等待遗留GITD事务复制完成的<strong>中间过渡状态</strong>。</p><h1 id="四、GTID长啥样"><a href="#四、GTID长啥样" class="headerlink" title="四、GTID长啥样"></a><strong>四、GTID长啥样</strong></h1><h2 id="GTID格式"><a href="#GTID格式" class="headerlink" title="GTID格式"></a><strong>GTID格式</strong></h2><p>GTID由两部分组成，source_id是提交该事务的服务器ID也就是server_uuid，是一个Mysql实例的唯一标识，在mysql第一次启动时，会自动生成并持久化到auto.cnf文件，下次启动从中加载即可；transaction_id是在主库上提交事务时分配的事务ID，一般情况下是从1开始无间隙递增的，最大为2^63-1，下面是GTID的格式和一个示例：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">GTID</span> = source_id:transaction_id<br><span class="hljs-attribute">3E11FA47</span>-<span class="hljs-number">71</span>CA-<span class="hljs-number">11</span>E1-<span class="hljs-number">9</span>E33-C80AA9429562:<span class="hljs-number">23</span><br></code></pre></td></tr></table></figure><p>源码中结构体的定义如下：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-keyword">struct</span> <span class="hljs-title class_">Gtid</span> &#123;<br>  <span class="hljs-comment">/* GTID的服务器号，int类型，可以使用Sid_map将sidno转为sid再得到UUID */</span><br>  rpl_sidno sidno;<br>  <span class="hljs-comment">/* GTID的事务号，int64类型 */</span><br>  rpl_gno gno;<br>  ...<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="GTID-Set"><a href="#GTID-Set" class="headerlink" title="GTID Set"></a><strong>GTID Set</strong></h2><p>GTID Set就如同它的名字一样，存的是一个GTID集合，像gtid_executed和gtid_purged变量其实就是GTID Set。因为GTID大多数情况下是连续分配的，所以可以用区间进行简化存储，如下示例：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">3E11FA47</span>-<span class="hljs-number">71</span>CA-<span class="hljs-number">11</span>E1-<span class="hljs-number">9</span>E33-C80AA9429562:<span class="hljs-number">1</span>-<span class="hljs-number">3</span>:<span class="hljs-number">11</span>:<span class="hljs-number">47</span>-<span class="hljs-number">49</span>, <span class="hljs-number">24</span>DA167-<span class="hljs-number">0</span>C0C-<span class="hljs-number">11</span>E8-<span class="hljs-number">8442</span>-<span class="hljs-number">00059</span>A3C7B00:<span class="hljs-number">1</span>-<span class="hljs-number">19</span><br></code></pre></td></tr></table></figure><p>其中同一个服务器UUID后面用冒号分隔不同区间，不同服务器的GTID用逗号分隔。<br>源码中有一个Gtid_set类定义，该类对象其实可以看作一系列链表，每个sidno(机器)对应了一条链表，链表内是有序排列的不相交的前闭后开区间，结构体的示意图与定义如下：<br><img src="/2023/10/15/%E6%B5%85%E6%9E%90MySQL-GTID/gtid_set.jpeg"></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Gtid_set</span> &#123;<br><span class="hljs-keyword">public</span>:<br>  <span class="hljs-comment">/* 定义了Gtid_set基本组成单位前闭后开区间[start,end) */</span><br>  <span class="hljs-keyword">struct</span> <span class="hljs-title class_">Interval</span> &#123;<br>   <span class="hljs-keyword">public</span>:<br>    rpl_gno start;<br>    rpl_gno end;<br>    <span class="hljs-comment">/* 属于同一个sidno的区间会串成一串 */</span><br>    Interval *next;<br>  &#125;;<br>  ...<br><span class="hljs-keyword">private</span>:<br>  <span class="hljs-comment">/* 使用的Sid_map */</span><br>  Sid_map *sid_map;<br>  <span class="hljs-comment">/* 第N个元素代表sidno为N的区间链表头，Gtid_set内绝大多数</span><br><span class="hljs-comment">     函数、迭代器等都是在m_intervals上操作，包括找空闲GTID、</span><br><span class="hljs-comment">     插入GTID、删除GTID、合并区间等等 */</span><br>  Prealloced_array&lt;Interval *, <span class="hljs-number">8</span>&gt; m_intervals;<br>...<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="UUID怎么变成了整数sidno"><a href="#UUID怎么变成了整数sidno" class="headerlink" title="UUID怎么变成了整数sidno"></a><strong>UUID怎么变成了整数sidno</strong></h2><p>从上面的介绍可以发现，明明GTID的第一部分是一个很长的字符串UUID，但代码里怎么就变成了一个整数sidno？这是为了避免在代码内处理过多字符串，所以每个机器都有个全局变量global_sid_map用来对sidno和UUID双向映射，该变量是Sid_map类对象，该类示意图与源码定义如下：<br><img src="/2023/10/15/%E6%B5%85%E6%9E%90MySQL-GTID/sid_map.jpeg"></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Sid_map</span> &#123;<br><span class="hljs-keyword">public</span>:<br>  <span class="hljs-comment">/* 检查内部数据结构是否已有sid，已有就直接返回对应的sidno，</span><br><span class="hljs-comment">     没有就生成新的sidno添加到内部数据结构并返回sidno */</span><br>  <span class="hljs-function">rpl_sidno <span class="hljs-title">add_sid</span><span class="hljs-params">(<span class="hljs-type">const</span> rpl_sid &amp;sid)</span></span>;<br>  <span class="hljs-comment">/* 将sid映射到sidno */</span><br>  <span class="hljs-function">rpl_sidno <span class="hljs-title">sid_to_sidno</span><span class="hljs-params">(<span class="hljs-type">const</span> rpl_sid &amp;sid)</span> <span class="hljs-type">const</span></span>;<br>  <span class="hljs-comment">/* 将sidno映射到sid */</span><br>  <span class="hljs-function"><span class="hljs-type">const</span> rpl_sid &amp;<span class="hljs-title">sidno_to_sid</span><span class="hljs-params">(rpl_sidno sidno, <span class="hljs-type">bool</span> need_lock = <span class="hljs-literal">false</span>)</span> <span class="hljs-type">const</span></span>;<br>  <span class="hljs-comment">/* 获得当前最大的sidno */</span><br>  <span class="hljs-function">rpl_sidno <span class="hljs-title">get_max_sidno</span><span class="hljs-params">()</span> <span class="hljs-type">const</span></span>;<br>  ...<br><span class="hljs-keyword">private</span>:<br>  <span class="hljs-comment">/* 内部array、map基本元素类型，包含sidno(int类型)</span><br><span class="hljs-comment">     和sid(binary_log::Uuid类型)两部分 */</span><br>  <span class="hljs-keyword">struct</span> <span class="hljs-title class_">Node</span> &#123;<br>    rpl_sidno sidno;<br>    rpl_sid sid;<br>  &#125;;<br>  <span class="hljs-comment">/* 理应为global_sid_lock */</span><br>  <span class="hljs-keyword">mutable</span> Checkable_rwlock *sid_lock;<br>  <span class="hljs-comment">/* 用于将sidno映射为sid */</span><br>  Prealloced_array&lt;Node *, <span class="hljs-number">8</span>&gt; _sidno_to_sid;<br>  <span class="hljs-comment">/* 用于将sid映射为sidno */</span><br>  malloc_unordered_map&lt;rpl_sid, unique_ptr_my_free&lt;Node&gt;, binary_log::Hash_Uuid&gt;<br>      _sid_to_sidno&#123;key_memory_Sid_map_Node&#125;;<br>  ...<br>&#125;<br></code></pre></td></tr></table></figure><h1 id="五、机器的GTID状态"><a href="#五、机器的GTID状态" class="headerlink" title="五、机器的GTID状态"></a><strong>五、机器的GTID状态</strong></h1><h2 id="系统变量"><a href="#系统变量" class="headerlink" title="系统变量"></a><strong>系统变量</strong></h2><p>当我们SHOW VARIABLES LIKE ‘%GTID%’的时候，经常可以看到gtid_executed、gtid_owned和gtid_purged这几个变量，它们其实反映出了当前机器的GTID状态，它们的相关介绍如下。</p><h3 id="gtid-executed"><a href="#gtid-executed" class="headerlink" title="gtid_executed"></a>gtid_executed</h3><ul><li>Scope : Global</li><li>Dynamic : No</li><li>Type : String</li></ul><p>代表了当前机器<strong>已经执行过</strong>的所有事务。</p><h3 id="gtid-owned"><a href="#gtid-owned" class="headerlink" title="gtid_owned"></a>gtid_owned</h3><ul><li>Scope : Global, Session</li><li>Dynamic : No</li><li>Type : String</li></ul><p>Global变量表示目前<strong>正在提交</strong>的并且<strong>被分配了GTID</strong>的事务的GTID，Session变量代表当前事务线程拥有(正在执行)的GTID。</p><h3 id="gtid-purged"><a href="#gtid-purged" class="headerlink" title="gtid_purged"></a><strong>gtid_purged</strong></h3><ul><li>Scope : Global</li><li>Dynamic : Yes</li><li>Type : String</li></ul><p>gtid_purged 变量是 gtid_executed 变量的子集，代表了已经执行但已经不在服务器上任何binlog文件中的GTID，意味着无法传给备库回放。</p><h2 id="运行时维护"><a href="#运行时维护" class="headerlink" title="运行时维护"></a><strong>运行时维护</strong></h2><h3 id="Gtid-state"><a href="#Gtid-state" class="headerlink" title="Gtid_state"></a><strong>Gtid_state</strong></h3><p>代码中有一个全局变量gtid_state，它是Gtid_state类指针，可以说gtid_state是GTID相关源码中最重要的变量，在后续很多模块都可以看到gtid_state的影子，其内的executed_gtids、owned_gtids、lost_gtids就对应着上面介绍的三个系统变量，在分配GTID的时候也就是调用该类的方法，相关源码如下：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Gtid_state</span> &#123;<br><span class="hljs-keyword">public</span>:<br>  <span class="hljs-comment">/* 初始化gtid_state，主要是将全局变量server_uuid解析成</span><br><span class="hljs-comment">     rpl_sid类型变量，调用sid_map.add_sid函数获得sid_no */</span><br>  <span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">init</span><span class="hljs-params">()</span></span>;<br>  <span class="hljs-comment">/* 检查gtid是否在executed_gtids中，即已执行 */</span><br>  <span class="hljs-function"><span class="hljs-type">bool</span> <span class="hljs-title">is_executed</span><span class="hljs-params">(<span class="hljs-type">const</span> Gtid &amp;gtid)</span> <span class="hljs-type">const</span></span>;<br>  <span class="hljs-comment">/* 检查gtid是否在owned_gtids中，即正在被某个线程own，执行中 */</span><br>  <span class="hljs-function"><span class="hljs-type">bool</span> <span class="hljs-title">is_owned</span><span class="hljs-params">(<span class="hljs-type">const</span> Gtid &amp;gtid)</span> <span class="hljs-type">const</span></span>;<br>  <span class="hljs-comment">/* 分配GTID时自动计算得到下个gno */</span><br>  <span class="hljs-function">rpl_gno <span class="hljs-title">get_automatic_gno</span><span class="hljs-params">(rpl_sidno sidno)</span> <span class="hljs-type">const</span></span>;<br>  <span class="hljs-comment">/* 为thd分配一个新的GTID，会加入到owned_gtids中 */</span><br>  <span class="hljs-function">enum_return_status <span class="hljs-title">generate_automatic_gtid</span><span class="hljs-params">(THD *thd,</span></span><br><span class="hljs-params"><span class="hljs-function">                                             rpl_sidno specified_sidno = <span class="hljs-number">0</span>,</span></span><br><span class="hljs-params"><span class="hljs-function">                                             rpl_gno specified_gno = <span class="hljs-number">0</span>,</span></span><br><span class="hljs-params"><span class="hljs-function">                                             rpl_sidno *locked_sidno = <span class="hljs-literal">nullptr</span>)</span></span>;<br>  <span class="hljs-comment">/* 在owned_gtids中绑定GTID和线程的关系 */</span><br>  <span class="hljs-function">enum_return_status <span class="hljs-title">Gtid_state::acquire_ownership</span><span class="hljs-params">(THD *thd, <span class="hljs-type">const</span> Gtid &amp;gtid)</span></span>;<br>  ...<br><span class="hljs-keyword">private</span>:<br>  <span class="hljs-comment">/* 理应为global_sid_lock */</span><br>  <span class="hljs-keyword">mutable</span> Checkable_rwlock *sid_lock;<br>  <span class="hljs-comment">/* 使用的Sid_map */</span><br>  <span class="hljs-keyword">mutable</span> Sid_map *sid_map;<br>  <span class="hljs-comment">/* 每个sidno都有一把锁，操作不同sidno下的GTID时减小锁粒度 */</span><br>  Mutex_cond_array sid_locks;<br>  <span class="hljs-comment">/* 代表了binlog中没有存储，无法根据binlog恢复的事务，是executed_gtids的子集 */</span><br>  Gtid_set lost_gtids;<br>  <span class="hljs-comment">/* 代表了本服务器执行过的所有事务 */</span><br>  Gtid_set executed_gtids;<br>  <span class="hljs-comment">/* 代表了分配给某个线程正在执行尚未提交的事务 */</span><br>  Owned_gtids owned_gtids;<br>  <span class="hljs-comment">/* init函数中设置，存服务器的sidno */</span><br>  rpl_sidno server_sidno;<br>  <span class="hljs-comment">/* 用于优化分配gno性能的变量，存的gno大概率无人使用，这样</span><br><span class="hljs-comment">     get_automatic_gno函数就可以从这个变量开始快速尝试 */</span><br>  rpl_gno next_free_gno;<br>  ...<br>&#125;<br></code></pre></td></tr></table></figure><h3 id="Owned-gtids"><a href="#Owned-gtids" class="headerlink" title="Owned_gtids"></a><strong>Owned_gtids</strong></h3><p>executed_gtids和lost_gtids都是前面介绍过的Gtid_set类型，但是owned_gtids则是Owned_gtids类型，从owned_gtids的功能也可以意识到它跟另外两个不太一样，它在分配GTID时起到了重要作用，需要维护事务和GTID的持有关系。</p><p>于是其内用一个数组为每个sidno存了一个Hash Map，可以将gno映射到拥有该GTID的线程ID，在owned_gtids内存的&lt;GTID, THD.id&gt;基本可以认为是正在提交流程中的事务，当事务完成提交后会清掉这个拥有关系，并在executed_gtids中加入GTID，示意图与类定义如下：<br><img src="/2023/10/15/%E6%B5%85%E6%9E%90MySQL-GTID/owned_gtids.jpeg"></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Owned_gtids</span> &#123;<br><span class="hljs-keyword">public</span>:<br>  <span class="hljs-comment">/* 将&lt;gtid.gno,owner&gt;的绑定关系添加到sidno_to_hash[gtid.sidno]中 */</span><br>  <span class="hljs-function">enum_return_status <span class="hljs-title">add_gtid_owner</span><span class="hljs-params">(<span class="hljs-type">const</span> Gtid &amp;gtid, my_thread_id owner)</span></span>;<br><span class="hljs-keyword">private</span>:<br>  <span class="hljs-comment">/* 基本单元，gno和所属线程 */</span><br>  <span class="hljs-keyword">struct</span> <span class="hljs-title class_">Node</span> &#123;<br>    rpl_gno gno;<br>    my_thread_id owner;<br>  &#125;;<br>  <span class="hljs-comment">/* 理应为global_sid_lock */</span><br>  <span class="hljs-keyword">mutable</span> Checkable_rwlock *sid_lock;<br>  <span class="hljs-comment">/* 每个sidno对应一个unordered_map，每个map管理了gno到Node的映射</span><br><span class="hljs-comment">     能快速根据sidno_to_hash[gtid.sidno][gtid.gno]查到其所属线程 */</span><br>  Prealloced_array&lt;malloc_unordered_multimap&lt;rpl_gno, unique_ptr_my_free&lt;Node&gt;&gt; *, <span class="hljs-number">8</span>&gt; sidno_to_hash;<br>&#125;<br></code></pre></td></tr></table></figure><p>分配GTID时，gtid_state的acquire_ownership函数就会调用owned_gtids的add_gtid_owner方法，绑定了GTID和事务线程的关系，并且设置线程自己的owned_gtid、owned_sid字段，如下：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-function">enum_return_status <span class="hljs-title">Gtid_state::acquire_ownership</span><span class="hljs-params">(THD *thd, <span class="hljs-type">const</span> Gtid &amp;gtid)</span> </span>&#123;<br>  <span class="hljs-comment">/* 向owned_gtids中添加gtid和thd的绑定关系 */</span><br>  <span class="hljs-keyword">if</span> (owned_gtids.<span class="hljs-built_in">add_gtid_owner</span>(gtid, thd-&gt;<span class="hljs-built_in">thread_id</span>()) != RETURN_STATUS_OK)<br>    <span class="hljs-keyword">goto</span> err;<br>  <br>  <span class="hljs-comment">/* 设置thd的owned_gtid、owned_sid(为了便利)和最近使用的GTID */</span><br>  thd-&gt;owned_gtid = gtid;<br>  thd-&gt;owned_sid = sid_map-&gt;<span class="hljs-built_in">sidno_to_sid</span>(gtid.sidno);<br>  thd-&gt;rpl_thd_ctx.<span class="hljs-built_in">last_used_gtid_tracker_ctx</span>().<span class="hljs-built_in">set_last_used_gtid</span>(gtid);<br>  ...<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="持久化"><a href="#持久化" class="headerlink" title="持久化"></a><strong>持久化</strong></h2><h3 id="mysql-gtid-executed表"><a href="#mysql-gtid-executed表" class="headerlink" title="mysql.gtid_executed表"></a>mysql.gtid_executed表</h3><p>在系统database mysql中存了一张名为gtid_executed的表，该表存的内容是gtid_executed变量的一个子集(有可能最新的部分还没有同步到表里面)，这张表由系统自动创建，它的CREATE TABLE信息如下：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">CREATE</span> <span class="hljs-keyword">TABLE</span> gtid_executed (<br>    source_uuid <span class="hljs-type">CHAR</span>(<span class="hljs-number">36</span>) <span class="hljs-keyword">NOT</span> <span class="hljs-keyword">NULL</span>,<br>    interval_start <span class="hljs-type">BIGINT</span>(<span class="hljs-number">20</span>) <span class="hljs-keyword">NOT</span> <span class="hljs-keyword">NULL</span>,<br>    interval_end <span class="hljs-type">BIGINT</span>(<span class="hljs-number">20</span>) <span class="hljs-keyword">NOT</span> <span class="hljs-keyword">NULL</span>,<br>    <span class="hljs-keyword">PRIMARY</span> KEY (source_uuid, interval_start)<br>)<br></code></pre></td></tr></table></figure><p>其实每行数据就是机器ID加上对应的事务ID区间，与GTID Set很类似。<br>关于何时写入数据到gtid_executed表，有如下规则：</p><ul><li>打开GTID并且<strong>关闭binlog</strong>的情况下，每个事务提交的时候都会把分配的GTID更新到表中</li><li>打开GTID并且<strong>打开binlog</strong>的情况下，从8.0.17开始，Innodb有自己专门的写入gtid_executed表的线程，即innodb&#x2F;clone_gtid_thread，每个事务提交的时候都会把GTID登记到该线程，该线程会将GTID成组写入表中并进行压缩，后面会详细介绍这个过程</li></ul><p>在代码中有一个Gtid_table_persistor类，该类包含了一系列操作mysql.gtid_executed表的函数，后面介绍的clone_gtid_thread线程就是调用该类的方法来操作mysql.gtid_executed表，如下：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Gtid_table_persistor</span> &#123;<br> <span class="hljs-keyword">public</span>:<br>  <span class="hljs-comment">/* mysql.gtid_executed表有三列：source_uuid、interval_start、interval_end */</span><br>  <span class="hljs-type">static</span> <span class="hljs-type">const</span> uint number_fields = <span class="hljs-number">3</span>;<br>  <span class="hljs-comment">/* 一系列操作mysql.gtid_executed表的函数，读、写、压缩等等 */</span><br>  ...<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a><strong>初始化</strong></h2><p>GTID相关的初始化主要包括以下几个方面：</p><ul><li>相关全局变量的初始化：<ul><li>global_sid_lock：GTID相关数据结构使用的读写锁</li><li>global_sid_map：sidno&lt;-&gt;sid的双向映射</li><li>gtid_table_persistor：该对象包含操作mysql.gtid_executed表的一系列方法</li></ul></li><li>gtid_state的初始化：<ul><li>设置当前机器的UUID，将其加入global_sid_map获得当前机器的sidno</li><li>内部executed_gtids、lost_gtids、previous_gtids_logged等变量的初始化，计算过程可见下图</li></ul></li><li>上面计算得到的previous_gtids_logged写入当前机器新生成的binlog中，作为Previous_gtids</li></ul><p><img src="/2023/10/15/%E6%B5%85%E6%9E%90MySQL-GTID/initialize.jpeg"><br>下面是初始化相关的函数栈与源码：<br><img src="/2023/10/15/%E6%B5%85%E6%9E%90MySQL-GTID/initialize_stack.png"></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br></pre></td><td class="code"><pre><code class="hljs cpp">global_sid_lock<span class="hljs-comment">/* GTID相关全局变量定义 */</span><br>Checkable_rwlock *global_sid_lock = <span class="hljs-literal">nullptr</span>;<br>Sid_map *global_sid_map = <span class="hljs-literal">nullptr</span>;<br>Gtid_state *gtid_state = <span class="hljs-literal">nullptr</span>;<br>Gtid_table_persistor *gtid_table_persistor = <span class="hljs-literal">nullptr</span>;<br><br><span class="hljs-function"><span class="hljs-type">bool</span> <span class="hljs-title">gtid_server_init</span><span class="hljs-params">()</span> </span>&#123;<br>  <span class="hljs-comment">/* 设置GTID Mode */</span><br>  global_gtid_mode.<span class="hljs-built_in">set</span>(<span class="hljs-built_in">static_cast</span>&lt;Gtid_mode::value_type&gt;(Gtid_mode::sysvar_mode));<br>  <span class="hljs-comment">/* GTID相关全局变量初始化 */</span><br>  global_sid_lock = <span class="hljs-keyword">new</span> <span class="hljs-built_in">Checkable_rwlock</span>();<br>  global_sid_map = <span class="hljs-keyword">new</span> <span class="hljs-built_in">Sid_map</span>(global_sid_lock));<br>  gtid_state = <span class="hljs-keyword">new</span> <span class="hljs-built_in">Gtid_state</span>(global_sid_lock, global_sid_map);<br>  gtid_table_persistor = <span class="hljs-keyword">new</span> <span class="hljs-built_in">Gtid_table_persistor</span>();<br>  ...<br>&#125;<br><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">mysqld_main</span><span class="hljs-params">(<span class="hljs-type">int</span> argc, <span class="hljs-type">char</span> **argv)</span> </span>&#123;<br>  ...<br>  <span class="hljs-comment">/* 1.内部调用gtid_server_init函数初始化GTID相关全局变量，</span><br><span class="hljs-comment">     2.调用generate_server_uuid函数为服务器生成UUID存到</span><br><span class="hljs-comment">       server_uuid变量中</span><br><span class="hljs-comment">     3.调用链ha_post_recover-&gt;post_recover_handlerton</span><br><span class="hljs-comment">       -&gt;ha_post_recover-&gt;innobase_post_recover</span><br><span class="hljs-comment">       -&gt;srv_start_threads_after_ddl_recovery</span><br><span class="hljs-comment">       -&gt;gtid_persistor.start()启动gtid persistor线程，</span><br><span class="hljs-comment">       该线程定期将GTID写入mysql.gtid_executed表 */</span><br>  <span class="hljs-built_in">init_server_components</span>();<br>  ...<br>  global_sid_lock-&gt;<span class="hljs-built_in">wrlock</span>();<br>  <span class="hljs-comment">/* 初始化gtid_state */</span><br>  <span class="hljs-type">int</span> gtid_ret = gtid_state-&gt;<span class="hljs-built_in">init</span>();<br>  global_sid_lock-&gt;<span class="hljs-built_in">unlock</span>();<br><br>  <span class="hljs-comment">/* 从mysql.gtid_executed表加载GTID到gtid_state-&gt;executed_gtids */</span><br>  gtid_state-&gt;<span class="hljs-built_in">read_gtid_executed_from_table</span>();<br><br>  <span class="hljs-comment">/* 开启binlog才进行 */</span><br>  <span class="hljs-keyword">if</span> (opt_bin_log) &#123;<br>    <span class="hljs-comment">/* 指向gtid_state-&gt;executed_gtids */</span><br>    Gtid_set *executed_gtids = ...;<br>    <span class="hljs-comment">/* 指向gtid_state-&gt;lost_gtids */</span><br>    Gtid_set *lost_gtids = ...;<br>    <span class="hljs-comment">/* 指向gtid_state-&gt;gtids_only_in_table */</span><br>    Gtid_set *gtids_only_in_table = ...;<br>    <span class="hljs-comment">/* 指向gtid_state-&gt;previous_gtids_logged */</span><br>    Gtid_set *previous_gtids_logged = ...;<br>    <span class="hljs-comment">/* 临时变量，代表历史中已经被purge的binlog中的GTID */</span><br>    <span class="hljs-function">Gtid_set <span class="hljs-title">purged_gtids_from_binlog</span><span class="hljs-params">(global_sid_map, global_sid_lock)</span></span>;<br>    <span class="hljs-comment">/* 临时变量，代表在binlog文件中存储过的GTID */</span><br>    <span class="hljs-function">Gtid_set <span class="hljs-title">gtids_in_binlog</span><span class="hljs-params">(global_sid_map, global_sid_lock)</span></span>;<br>    <span class="hljs-comment">/* 临时变量，代表binlog文件中有但是mysql.gtid_executed表中没有的GTID */</span><br>    <span class="hljs-function">Gtid_set <span class="hljs-title">gtids_in_binlog_not_in_table</span><span class="hljs-params">(global_sid_map, global_sid_lock)</span></span>;<br><br>    <span class="hljs-comment">/* 调用完成后，gtids_in_binlog存最新非空binlog文件中</span><br><span class="hljs-comment">       Previous_gtids_log_event和所有gtid_event的并集；</span><br><span class="hljs-comment">       purged_gtids_from_binlog存最旧binlog文件中的</span><br><span class="hljs-comment">       Previous_gtids_log_event */</span><br>    mysql_bin_log.<span class="hljs-built_in">init_gtid_sets</span>(<br>        &amp;gtids_in_binlog, &amp;purged_gtids_from_binlog,<br>        opt_source_verify_checksum, <span class="hljs-literal">true</span>,<br>        <span class="hljs-literal">nullptr</span>, <span class="hljs-literal">nullptr</span>, <span class="hljs-literal">true</span>);<br><br>    global_sid_lock-&gt;<span class="hljs-built_in">wrlock</span>();<br><br>    <span class="hljs-comment">/* gtids_in_binlog_not_in_table = gtids_in_binlog - gtids_in_table</span><br><span class="hljs-comment">       目前executed_gtids只加载了表中的GTID */</span><br>    gtids_in_binlog_not_in_table.<span class="hljs-built_in">add_gtid_set</span>(&amp;gtids_in_binlog);<br>    gtids_in_binlog_not_in_table.<span class="hljs-built_in">remove_gtid_set</span>(executed_gtids);<br>    <span class="hljs-comment">/* 把表中没有的GTID持久化到表中 */</span><br>    gtid_state-&gt;<span class="hljs-built_in">save</span>(&amp;gtids_in_binlog_not_in_table);<br>    <span class="hljs-comment">/* 完成executed_gtids的初始化，取两个的并集 */</span><br>    executed_gtids-&gt;<span class="hljs-built_in">add_gtid_set</span>(&amp;gtids_in_binlog_not_in_table);<br><br>    <span class="hljs-comment">/* gtids_only_in_table = executed_gtids - gtids_in_binlog */</span><br>    gtids_only_in_table-&gt;<span class="hljs-built_in">add_gtid_set</span>(executed_gtids)<br>    gtids_only_in_table-&gt;<span class="hljs-built_in">remove_gtid_set</span>(&amp;gtids_in_binlog);<br>    <span class="hljs-comment">/* lost_gtids = gtids_only_in_table + purged_gtids_from_binlog;</span><br><span class="hljs-comment">       lost_gtids代表了binlog中没有存储，无法根据binlog恢复的事务 */</span><br>    lost_gtids-&gt;<span class="hljs-built_in">add_gtid_set</span>(gtids_only_in_table);<br>    lost_gtids-&gt;<span class="hljs-built_in">add_gtid_set</span>(&amp;purged_gtids_from_binlog);<br><br>    <span class="hljs-comment">/* 为本次启动创建的binlog文件准备Previous_gtids_log_event */</span><br>    previous_gtids_logged-&gt;<span class="hljs-built_in">add_gtid_set</span>(&amp;gtids_in_binlog);<br>    <span class="hljs-function">Previous_gtids_log_event <span class="hljs-title">prev_gtids_ev</span><span class="hljs-params">(&amp;gtids_in_binlog)</span></span>;<br><br>    global_sid_lock-&gt;<span class="hljs-built_in">unlock</span>();<br><br>    <span class="hljs-comment">/* 为Previous_gtids_log_event计算checksum，随后写入binlog */</span><br>    (prev_gtids_ev.common_footer)-&gt;checksum_alg =<br>        <span class="hljs-built_in">static_cast</span>&lt;enum_binlog_checksum_alg&gt;(binlog_checksum_options);<br>    mysql_bin_log.<span class="hljs-built_in">write_event_to_binlog_and_sync</span>(&amp;prev_gtids_ev);<br>    ...<br>  &#125;<br>&#125;<br></code></pre></td></tr></table></figure><h1 id="六、GTID的生命周期"><a href="#六、GTID的生命周期" class="headerlink" title="六、GTID的生命周期"></a><strong>六、GTID的生命周期</strong></h1><p><img src="/2023/10/15/%E6%B5%85%E6%9E%90MySQL-GTID/life_cycle.jpeg"><br>GTID生命周期主要分为主备两部分，大部分工作是在主库完成，包括分配、构造GTID Event写入binlog、刷写GTID到持久化表、提交完成更新GTID状态；备库上则是根据Relay Log解析GTID，设置到gtid_next中并对事务进行回放。接下来会逐一介绍每个部分。</p><h2 id="分配GTID"><a href="#分配GTID" class="headerlink" title="分配GTID"></a><strong>分配GTID</strong></h2><p>分配GTID的时机是事务两阶段提交Commit阶段的Flush Stage，该提交组leader线程会搜索executed_gtids的所有区间间隙，通过检查owned_gtids判断一个GTID当前是否空闲(不被其他事务拥有)，所有检查都完成后便绑定事务和GTID的拥有关系，并且设置next_free_gno方便下次搜索。整个过程都是受global_sid_lock读锁和相应sidno互斥锁保护的，如下图所示：<br><img src="/2023/10/15/%E6%B5%85%E6%9E%90MySQL-GTID/assign_gtid.jpeg"></p><p>调用栈及源码如下所示，其中process_flush_stage_queue函数内会首先拿到Flush队列的leader线程，然后传给assign_automatic_gtids_to_flush_group函数为组内所有线程的事务分配GTID。<br><img src="/2023/10/15/%E6%B5%85%E6%9E%90MySQL-GTID/assign_gtid_stack.png"></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-function"><span class="hljs-type">bool</span> <span class="hljs-title">MYSQL_BIN_LOG::assign_automatic_gtids_to_flush_group</span><span class="hljs-params">(THD *first_seen)</span> </span>&#123;<br>  <span class="hljs-type">bool</span> error = <span class="hljs-literal">false</span>;<br>  <span class="hljs-type">bool</span> is_global_sid_locked = <span class="hljs-literal">false</span>;<br>  rpl_sidno locked_sidno = <span class="hljs-number">0</span>;<br>  <span class="hljs-comment">/* 遍历组内所有线程，为其分配GTID */</span><br>  <span class="hljs-keyword">for</span> (THD *head = first_seen; head; head = head-&gt;next_to_commit) &#123;<br>    <span class="hljs-comment">/* gtid_next类型为AUTOMATIC_GTID才需要分配 */</span><br>    <span class="hljs-keyword">if</span> (head-&gt;variables.gtid_next.type == AUTOMATIC_GTID) &#123;<br>      <span class="hljs-comment">/* 为了避免重复加锁释放锁，组内所有线程只加一次 */</span><br>      <span class="hljs-keyword">if</span> (!is_global_sid_locked) &#123;<br>        global_sid_lock-&gt;<span class="hljs-built_in">rdlock</span>();<br>        is_global_sid_locked = <span class="hljs-literal">true</span>;<br>      &#125;<br>      <span class="hljs-comment">/* 为线程事务自动生成gtid</span><br><span class="hljs-comment">         其中head-&gt;get_transaction()-&gt;get_rpl_transaction_ctx()</span><br><span class="hljs-comment">         就是thd-&gt;m_transaction-&gt;m_rpl_transaction_ctx，线程THD类</span><br><span class="hljs-comment">         绑事务的时候会进行初始化，*/</span><br>      <span class="hljs-keyword">if</span> (gtid_state-&gt;<span class="hljs-built_in">generate_automatic_gtid</span>(<br>              head,<br>              head-&gt;<span class="hljs-built_in">get_transaction</span>()-&gt;<span class="hljs-built_in">get_rpl_transaction_ctx</span>()-&gt;<span class="hljs-built_in">get_sidno</span>(), <span class="hljs-comment">/* 理应为0 */</span><br>              head-&gt;<span class="hljs-built_in">get_transaction</span>()-&gt;<span class="hljs-built_in">get_rpl_transaction_ctx</span>()-&gt;<span class="hljs-built_in">get_gno</span>(), <span class="hljs-comment">/* 理应为0 */</span><br>              &amp;locked_sidno) != RETURN_STATUS_OK) &#123;<br>        head-&gt;commit_error = THD::CE_FLUSH_GNO_EXHAUSTED_ERROR;<br>        error = <span class="hljs-literal">true</span>;<br>      &#125;<br>    &#125; <span class="hljs-keyword">else</span> &#123;<br>       ... <span class="hljs-comment">/* 仅仅assert检查，不做修改 */</span><br>    &#125;<br>  &#125;<br>  <br>  <span class="hljs-comment">/* 必要时释放锁 */</span><br>  <span class="hljs-keyword">if</span> (locked_sidno &gt; <span class="hljs-number">0</span>) gtid_state-&gt;<span class="hljs-built_in">unlock_sidno</span>(locked_sidno);<br>  <span class="hljs-keyword">if</span> (is_global_sid_locked) global_sid_lock-&gt;<span class="hljs-built_in">unlock</span>();<br><br>  <span class="hljs-keyword">return</span> error;<br>&#125;<br></code></pre></td></tr></table></figure><p>generate_automatic_gtid函数内主要是加锁、解锁的操作，核心是调用get_automatic_gno函数为GTID分配gno，如下：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-function">enum_return_status <span class="hljs-title">Gtid_state::generate_automatic_gtid</span><span class="hljs-params">(</span></span><br><span class="hljs-params"><span class="hljs-function">    THD *thd, rpl_sidno specified_sidno, rpl_gno specified_gno,</span></span><br><span class="hljs-params"><span class="hljs-function">    rpl_sidno *locked_sidno)</span> </span>&#123;<br>  <span class="hljs-comment">/* 在上一级caller中传了locked_sidno，所以这里是false，</span><br><span class="hljs-comment">     后续忽略基于该变量进行的判断 */</span><br>  <span class="hljs-type">bool</span> locked_sidno_was_passed_null = (locked_sidno == <span class="hljs-literal">nullptr</span>);<br><br>  <span class="hljs-comment">/* 上一级caller中调用了global_sid_lock-&gt;rdlock()，</span><br><span class="hljs-comment">     与gtid_state的sid_lock一样，所以可以通过检查 */</span><br>  sid_lock-&gt;<span class="hljs-built_in">assert_some_lock</span>();<br><br>  <span class="hljs-comment">/* GTID_MODE是ON_PERMISSIVE或ON，生成一个新的GTID */</span><br>  <span class="hljs-keyword">if</span> (global_gtid_mode.<span class="hljs-built_in">get</span>() &gt;= Gtid_mode::ON_PERMISSIVE) &#123;<br>    <span class="hljs-comment">/* 传入的specified_sidno和specified_gno理应都为0 */</span><br>    Gtid automatic_gtid = &#123;specified_sidno, specified_gno&#125;;<br>    <span class="hljs-comment">/* 获得服务器的sidno，设置到GTID中 */</span><br>    <span class="hljs-keyword">if</span> (automatic_gtid.sidno == <span class="hljs-number">0</span>) automatic_gtid.sidno = <span class="hljs-built_in">get_server_sidno</span>();<br>    <br>    <span class="hljs-comment">/* 上一级caller是在循环中为组内每一个线程分配GTID，第一次调用</span><br><span class="hljs-comment">       本函数时*locked_sidno=0，会给服务器的sidno加锁，之后就不</span><br><span class="hljs-comment">       需要了，除非有个线程事务换了其他sidno */</span><br>    <span class="hljs-type">bool</span> need_to_lock_sidno = *locked_sidno != automatic_gtid.sidno);<br>    <span class="hljs-keyword">if</span> (need_to_lock_sidno) &#123;<br>      <span class="hljs-comment">/* 切换了sidno，需要将之前sidno的锁释放掉再上新sidno的锁 */</span><br>      <span class="hljs-keyword">if</span> (*locked_sidno != <span class="hljs-number">0</span>)<br>        <span class="hljs-built_in">unlock_sidno</span>(*locked_sidno);<br>      <span class="hljs-built_in">lock_sidno</span>(automatic_gtid.sidno);<br>      <span class="hljs-comment">/* locked_sidno存了当前上锁的sidno，上一级caller负责释放 */</span><br>      *locked_sidno = automatic_gtid.sidno;<br>    &#125;<br><br>    <span class="hljs-comment">/* 生成GTID的gno部分 */</span><br>    <span class="hljs-keyword">if</span> (automatic_gtid.gno == <span class="hljs-number">0</span>) &#123;<br>      automatic_gtid.gno = <span class="hljs-built_in">get_automatic_gno</span>(automatic_gtid.sidno);<br>      <span class="hljs-comment">/* 设置next_free_gno，该变量会在get_automatic_gno函数</span><br><span class="hljs-comment">         中被使用，可以使得下一次分配更快，不需要频繁检查owned_gtids */</span><br>      <span class="hljs-keyword">if</span> (automatic_gtid.sidno == <span class="hljs-built_in">get_server_sidno</span>() &amp;&amp; automatic_gtid.gno != <span class="hljs-number">-1</span>)<br>        next_free_gno = automatic_gtid.gno + <span class="hljs-number">1</span>;<br>    &#125;<br>    <span class="hljs-comment">/* 在owned_gtids中绑定GTID和线程的关系，设置线程相关变量 */</span><br>    <span class="hljs-built_in">acquire_ownership</span>(thd, automatic_gtid);<br>  &#125; <span class="hljs-keyword">else</span> &#123;<br>    <span class="hljs-comment">/* GTID_MODE是OFF或OFF_PERMISSIVE，标记一下即可 */</span><br>    thd-&gt;owned_gtid.sidno = THD::OWNED_SIDNO_ANONYMOUS;<br>    thd-&gt;owned_gtid.gno = <span class="hljs-number">0</span>;<br>    <span class="hljs-built_in">acquire_anonymous_ownership</span>();<br>  &#125;<br>  ...<br>&#125;<br></code></pre></td></tr></table></figure><p>get_automatic_gno函数则会从next_free_gno开始，遍历executed_gtids内所有区间的空隙，递增并检查是否被own，没有就代表找到了真正空闲的gno随后可以返回，如下：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-function">rpl_gno <span class="hljs-title">Gtid_state::get_automatic_gno</span><span class="hljs-params">(rpl_sidno sidno)</span> <span class="hljs-type">const</span> </span>&#123;<br>    <span class="hljs-comment">/* ivit是遍历executed_gtids内sidno下所有区间[start,end)的迭代器 */</span><br>    <span class="hljs-function">Gtid_set::Const_interval_iterator <span class="hljs-title">ivit</span><span class="hljs-params">(&amp;executed_gtids, sidno)</span></span>;<br>    <span class="hljs-comment">/* 本服务器生成的GTID，直接从next_free_gno开始检查，加快分配速度 */</span><br>    Gtid next_candidate = &#123;sidno, sidno == <span class="hljs-built_in">get_server_sidno</span>() ? next_free_gno : <span class="hljs-number">1</span>&#125;;<br>    <span class="hljs-comment">/* 从头遍历所有区间，从空隙中找第一个没有被提交并且没有被own的gno */</span><br>    <span class="hljs-keyword">while</span> (<span class="hljs-literal">true</span>) &#123;<br>        <span class="hljs-type">const</span> Gtid_set::Interval *iv = ivit.<span class="hljs-built_in">get</span>();<br>        rpl_gno next_interval_start = iv != <span class="hljs-literal">nullptr</span> ? iv-&gt;start : GNO_END;<br>        <span class="hljs-comment">/* 从当前空隙里逐一尝试GTID，没有被占有即可返回 */</span><br>        <span class="hljs-keyword">while</span> (next_candidate.gno &lt; next_interval_start) &#123;<br>            <span class="hljs-keyword">if</span> (owned_gtids.<span class="hljs-built_in">is_owned_by</span>(next_candidate, <span class="hljs-number">0</span>)) <span class="hljs-keyword">return</span> next_candidate.gno;<br>            next_candidate.gno++;<br>        &#125;<br>        <span class="hljs-comment">/* 已经耗尽gno，报错 */</span><br>        <span class="hljs-keyword">if</span> (iv == <span class="hljs-literal">nullptr</span>) &#123;<br>            <span class="hljs-built_in">my_error</span>(ER_GNO_EXHAUSTED, <span class="hljs-built_in">MYF</span>(<span class="hljs-number">0</span>));<br>            <span class="hljs-keyword">return</span> <span class="hljs-number">-1</span>;<br>        &#125;<br>        <span class="hljs-comment">/* 设置gno为该区间的结束位置，下一轮尝试的开始 */</span><br>        <span class="hljs-keyword">if</span> (next_candidate.gno &lt; iv-&gt;end) next_candidate.gno = iv-&gt;end;<br>        ivit.<span class="hljs-built_in">next</span>();<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="写入binlog"><a href="#写入binlog" class="headerlink" title="写入binlog"></a><strong>写入binlog</strong></h2><p><img src="/2023/10/15/%E6%B5%85%E6%9E%90MySQL-GTID/write_binlog_stack.png"><br>在Flush Stage分配完各个事务的GTID后，就会将binlog cache中的内容flush到binlog文件中(Page Cache内)，这个过程中与GTID相关的部分首先会构建一个Gtid_log_event，把该event作为本事务的第一个event写到binlog中，如下：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-function"><span class="hljs-type">bool</span> <span class="hljs-title">MYSQL_BIN_LOG::write_transaction</span><span class="hljs-params">(THD *thd, binlog_cache_data *cache_data,</span></span><br><span class="hljs-params"><span class="hljs-function">                                      Binlog_event_writer *writer)</span> </span>&#123;<br>  int64 sequence_number, last_committed;<br>  <span class="hljs-comment">/* sequence_number代表事务的逻辑时间戳，last_committed代表本事务</span><br><span class="hljs-comment">     必须在sequence_number等于last_committed的事务提交之后再提交，</span><br><span class="hljs-comment">     用于维持备库执行事务的顺序 */</span><br>  m_dependency_tracker.<span class="hljs-built_in">get_dependency</span>(thd, sequence_number, last_committed);<br>  <span class="hljs-comment">/* 事务在本机上提交的物理时间戳 */</span><br>  ulonglong immediate_commit_timestamp = <span class="hljs-built_in">my_micro_time</span>();<br>  <span class="hljs-comment">/* 事务在源机器上提交的物理时间戳，这里省略了后面对于 undefined 的处理，</span><br><span class="hljs-comment">     本机就是源机时等于 immediate 时间戳 */</span><br>  ulonglong original_commit_timestamp = thd-&gt;variables.original_commit_timestamp;<br>  <span class="hljs-comment">/* 本机版本 */</span><br>  <span class="hljs-type">uint32_t</span> trx_immediate_server_version = <span class="hljs-built_in">do_server_version_int</span>(::server_version);<br>  <span class="hljs-comment">/* 源机版本，省略后面对于 undefined 的判断 */</span><br>  <span class="hljs-type">uint32_t</span> trx_original_server_version = thd-&gt;variables.original_server_version;<br>  <span class="hljs-comment">/* 构建 Gtid_log_event */</span><br>  <span class="hljs-function">Gtid_log_event <span class="hljs-title">gtid_event</span><span class="hljs-params">(</span></span><br><span class="hljs-params"><span class="hljs-function">      thd, cache_data-&gt;is_trx_cache(), last_committed, sequence_number,</span></span><br><span class="hljs-params"><span class="hljs-function">      cache_data-&gt;may_have_sbr_stmts(), original_commit_timestamp,</span></span><br><span class="hljs-params"><span class="hljs-function">      immediate_commit_timestamp, trx_original_server_version,</span></span><br><span class="hljs-params"><span class="hljs-function">      trx_immediate_server_version)</span></span>;<br>  <br>  <span class="hljs-comment">/* 根据 binlog cache 的 size 设置事务长度到构建 Gtid_log_event 中 */</span><br>  gtid_event.<span class="hljs-built_in">set_trx_length_by_cache_size</span>(cache_data-&gt;<span class="hljs-built_in">get_byte_position</span>(),<br>                                          writer-&gt;<span class="hljs-built_in">is_checksum_enabled</span>(),<br>                                          cache_data-&gt;<span class="hljs-built_in">get_event_counter</span>());<br>  <span class="hljs-comment">/* 将构建Gtid_log_event写到binlog文件中 */</span><br>  <span class="hljs-type">bool</span> ret = gtid_event.<span class="hljs-built_in">write</span>(writer);<br>  ... <span class="hljs-comment">/* flush剩余event */</span><br>&#125;<br></code></pre></td></tr></table></figure><p>binlog文件中GTID Event的body内容如下：</p><table><thead><tr><th><strong>Name</strong></th><th><strong>Format</strong></th><th><strong>Description</strong></th></tr></thead><tbody><tr><td>GTID_FLAGS</td><td>1 byte</td><td>00000001 &#x3D; Transaction may have changes logged with SBR. In 5.6, 5.7.0-5.7.18, and 8.0.0-8.0.1, this flag is always set. Starting in 5.7.19 and 8.0.2, this flag is cleared if the transaction only contains row events. It is set if any part of the transaction is written in statement format.</td></tr><tr><td>SID</td><td>16 byte sequence</td><td>UUID representing the SID</td></tr><tr><td>GNO</td><td>8 byte integer</td><td>Group number, second component of GTID.</td></tr><tr><td>logical clock timestamp typecode</td><td>1 byte integer</td><td>The type of logical timestamp used in the logical clock fields.</td></tr><tr><td>last_committed</td><td>8 byte integer</td><td>Store the transaction’s commit parent sequence_number</td></tr><tr><td>sequence_number</td><td>8 byte integer</td><td>The transaction’s logical timestamp assigned at prepare phase</td></tr><tr><td>immediate_commit_timestamp</td><td>7 byte integer</td><td>Timestamp of commit on the immediate master</td></tr><tr><td>original_commit_timestamp</td><td>7 byte integer</td><td>Timestamp of commit on the originating master</td></tr><tr><td>transaction_length</td><td>1 to 9 byte integer &#x2F;&#x2F; See net_length_size(ulonglong num)</td><td>The packed transaction’s length in bytes, including the Gtid</td></tr><tr><td>immediate_server_version</td><td>4 byte integer</td><td>Server version of the immediate server</td></tr><tr><td>original_server_version</td><td>4 byte integer</td><td>Version of the server where the transaction was originally executed</td></tr></tbody></table><h2 id="写入mysql-gtid-executed表"><a href="#写入mysql-gtid-executed表" class="headerlink" title="写入mysql.gtid_executed表"></a><strong>写入mysql.gtid_executed表</strong></h2><p><img src="/2023/10/15/%E6%B5%85%E6%9E%90MySQL-GTID/persist.jpeg"><br>前面提到了，从8.0.17开始，InnoDB新增了一个clone_gtid_thread线程，负责将所有事务的GTID刷写到mysql.gtid_executed表中，这个过程在事务提交阶段是异步的，事务线程将GTID写到clone_gtid_thread线程的Buffer中就可以去干别的事了，clone_gtid_thread线程会定期将Buffer中的GTID刷入表中，整个Buffer使用了双缓冲机制。同时clone_gtid_thread线程会承担起mysql.gtid_executed表的压缩工作，定期检查刷GTID Set次数是否达到压缩阈值，到了就会进行一次压缩，因此之前的压缩线程compress_gtid_table从8.0.23开始就默认弃用了。</p><p>关于线程的初始化、整个异步刷写流程后面逐一介绍。</p><h3 id="持久化线程初始化"><a href="#持久化线程初始化" class="headerlink" title="持久化线程初始化"></a><strong>持久化线程初始化</strong></h3><p>在Innodb中有专门的线程来将GTID写入mysql.gtid_executed表，来缓解提交阶段的工作量。这个线程的启动在之前“初始化”小节内概述了一下，下面看下Clone_persist_gtid::start()函数：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-function"><span class="hljs-type">bool</span> <span class="hljs-title">Clone_persist_gtid::start</span><span class="hljs-params">()</span> </span>&#123;<br>  ...<br>  <span class="hljs-comment">/* 该线程就是Innodb诸多后台线程中的m_gtid_persister，</span><br><span class="hljs-comment">     执行的是clone_gtid_thread函数，该函数就是直接调用</span><br><span class="hljs-comment">     Clone_persist_gtid::periodic_write()函数 */</span><br>  srv_threads.m_gtid_persister =<br>      <span class="hljs-built_in">os_thread_create</span>(clone_gtid_thread_key, <span class="hljs-number">0</span>, clone_gtid_thread, <span class="hljs-keyword">this</span>);<br>  srv_threads.m_gtid_persister.<span class="hljs-built_in">start</span>();<br>  ...<br>&#125;<br></code></pre></td></tr></table></figure><h3 id="添加GTID到持久化线程"><a href="#添加GTID到持久化线程" class="headerlink" title="添加GTID到持久化线程"></a><strong>添加GTID到持久化线程</strong></h3><p><img src="/2023/10/15/%E6%B5%85%E6%9E%90MySQL-GTID/persist_stack1.png"><br>如上图所示，这一部分工作是在2PC的提交阶段Commit Stage完成的，并且藏得很深，最终落到了gtid_persistor.add()函数，该函数就是将GTID加入到持久化线程的缓存中，等待后续被持久化到表中，首先看下该函数的调用者trx_release_impl_and_expl_locks函数相关部分的实现：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-function"><span class="hljs-type">static</span> <span class="hljs-type">void</span> <span class="hljs-title">trx_release_impl_and_expl_locks</span><span class="hljs-params">(<span class="hljs-type">trx_t</span> *trx, <span class="hljs-type">bool</span> serialised)</span> </span>&#123;<br>  <span class="hljs-comment">/* 从trx的owned_gtid提取GTID信息到gtid_desc中 */</span><br>  Gtid_desc gtid_desc&#123;&#125;;<br>  <span class="hljs-keyword">if</span> (serialised) &#123;<br>    <span class="hljs-keyword">auto</span> &amp;gtid_persistor = clone_sys-&gt;<span class="hljs-built_in">get_gtid_persistor</span>();<br>    gtid_persistor.<span class="hljs-built_in">get_gtid_info</span>(trx, gtid_desc);<br>  &#125;<br>  ...<br>  <span class="hljs-comment">/* 将GTID加到持久化线程的buffer中 */</span><br>  <span class="hljs-keyword">if</span> (serialised) &#123;<br>    <span class="hljs-keyword">if</span> (gtid_desc.m_is_set) &#123;<br>      <span class="hljs-keyword">auto</span> &amp;gtid_persistor = clone_sys-&gt;<span class="hljs-built_in">get_gtid_persistor</span>();<br>      gtid_persistor.<span class="hljs-built_in">add</span>(gtid_desc);<br>    &#125;<br>  &#125;<br>&#125;<br></code></pre></td></tr></table></figure><p>再来看下gtid_persistor.add()函数的实现：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">Clone_persist_gtid::add</span><span class="hljs-params">(<span class="hljs-type">const</span> Gtid_desc &amp;gtid_desc)</span> </span>&#123;<br>  ... <span class="hljs-comment">/* 一些检查 */</span><br><br>  <span class="hljs-comment">/* 缓存内GTID数量过多，并且持久化线程正在工作中，令当前线程等待一下 */</span><br>  <span class="hljs-keyword">if</span> (<span class="hljs-built_in">check_max_gtid_threshold</span>() &amp;&amp; <span class="hljs-built_in">is_thread_active</span>()) &#123;<br>    <span class="hljs-built_in">trx_sys_serialisation_mutex_exit</span>();<br>    <span class="hljs-built_in">wait_flush</span>(<span class="hljs-literal">false</span>, <span class="hljs-literal">false</span>, <span class="hljs-literal">nullptr</span>);<br>    <span class="hljs-built_in">trx_sys_serialisation_mutex_enter</span>();<br>  &#125;<br><br>  <span class="hljs-comment">/* 采用双Buffer机制，获得前台Buffer，随后将GTID加入Buffer，</span><br><span class="hljs-comment">     并且递增原子变量m_num_gtid_mem */</span><br>  <span class="hljs-keyword">auto</span> &amp;current_gtids = <span class="hljs-built_in">get_active_list</span>();<br>  current_gtids.<span class="hljs-built_in">push_back</span>(gtid_desc.m_info);<br>  <span class="hljs-type">int</span> current_value = ++m_num_gtid_mem;<br><br>  <span class="hljs-comment">/* 已经到达了缓存数量上限，直接设置m_event事件唤醒持久化线程开刷 */</span><br>  <span class="hljs-keyword">if</span> (current_value == s_gtid_threshold) &#123;<br>    <span class="hljs-built_in">os_event_set</span>(m_event);<br>  &#125;<br>&#125;<br></code></pre></td></tr></table></figure><h3 id="持久化线程周期性刷写"><a href="#持久化线程周期性刷写" class="headerlink" title="持久化线程周期性刷写"></a><strong>持久化线程周期性刷写</strong></h3><p><img src="/2023/10/15/%E6%B5%85%E6%9E%90MySQL-GTID/persist_stack2.png"><br>如上图所示，持久化线程执行的是periodic_write函数，其内是一个无限循环，不断调用flush_gtids函数将缓存的GTID刷到mysql.gtid_executed表中，最终落在gtid_table_persistor-&gt;save()函数，其中gtid_table_persistor是mysqld.cc定义的Gtid_table_persistor类型全局变量。</p><p>下面逐层看下函数实现，首先看下periodic_write函数，内部循环除了等待一定时间外，就是直接调用flush_gtids函数，如下：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">Clone_persist_gtid::periodic_write</span><span class="hljs-params">()</span> </span>&#123;<br>  ...<br>  <span class="hljs-keyword">for</span> (;;) &#123;<br>    ... <span class="hljs-comment">/* 遇到shutdown就break */</span><br>    <span class="hljs-comment">/* 除非需要立即刷，等s_time_threshold毫秒 */</span><br>    <span class="hljs-keyword">if</span> (!<span class="hljs-built_in">flush_immediate</span>()) &#123;<br>      <span class="hljs-built_in">os_event_wait_time</span>(m_event, s_time_threshold);<br>    &#125;<br>    <span class="hljs-built_in">os_event_reset</span>(m_event);<br>    <span class="hljs-comment">/* 刷当前缓存的GTID到表中 */</span><br>    <span class="hljs-built_in">flush_gtids</span>(thd);<br>  &#125;<br>  ...<br>&#125;<br></code></pre></td></tr></table></figure><p>flush_gtids函数包含了刷写缓存GTID和压缩两个任务，实现如下：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">Clone_persist_gtid::flush_gtids</span><span class="hljs-params">(THD *thd)</span> </span>&#123;<br>  ...<br>  <span class="hljs-comment">/* Buffer内的GTID数量大于0，进行刷写 */</span><br>  <span class="hljs-keyword">if</span> (m_num_gtid_mem.<span class="hljs-built_in">load</span>() != <span class="hljs-number">0</span>) &#123;<br>    m_flush_in_progress.<span class="hljs-built_in">store</span>(<span class="hljs-literal">true</span>);<br>    <span class="hljs-comment">/* 双Buffer机制，获得当前前台Buffer并切换，后续就可以刷后台Buffer，</span><br><span class="hljs-comment">       前台Buffer则继续接新增GTID */</span><br>    <span class="hljs-keyword">auto</span> flush_list_number = switch_active_list();<br>    <span class="hljs-comment">/* 进行刷写，后两个参数与recovery相关，暂不关注 */</span><br>    err = <span class="hljs-built_in">write_to_table</span>(flush_list_number, table_gtid_set, sid_map);<br>    m_flush_in_progress.<span class="hljs-built_in">store</span>(<span class="hljs-literal">false</span>);<br>  &#125;<br>  ...<br>  <span class="hljs-comment">/* 检查压缩条件，符合就对表进行压缩，同样落在全局变量</span><br><span class="hljs-comment">     gtid_table_persistor的方法上 */</span><br>  <span class="hljs-type">bool</span> debug_skip = <span class="hljs-built_in">debug_skip_write</span>(<span class="hljs-literal">true</span>);<br>  <span class="hljs-keyword">if</span> (err == <span class="hljs-number">0</span> &amp;&amp; !debug_skip &amp;&amp; (compress_recovery || <span class="hljs-built_in">check_compress</span>())) &#123;<br>    m_compression_counter = <span class="hljs-number">0</span>;<br>    m_compression_gtid_counter = <span class="hljs-number">0</span>;<br>    <span class="hljs-comment">/* 将非Innodb事务的GTID也持久化到表中 */</span><br>    <span class="hljs-built_in">write_other_gtids</span>();<br>    <span class="hljs-comment">/* 调用压缩 */</span><br>    err = gtid_table_persistor-&gt;<span class="hljs-built_in">compress</span>(thd);<br>  &#125;<br>  ...<br>&#125;<br></code></pre></td></tr></table></figure><p>write_to_table函数则是从缓存中取出GTID并构成GTID Set，并真正调用gtid_table_persistor-&gt;save()函数进行持久化到表的操作，如下：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">Clone_persist_gtid::write_to_table</span><span class="hljs-params">(<span class="hljs-type">uint64_t</span> flush_list_number,</span></span><br><span class="hljs-params"><span class="hljs-function">                                       Gtid_set &amp;table_gtid_set,</span></span><br><span class="hljs-params"><span class="hljs-function">                                       Sid_map &amp;sid_map)</span> </span>&#123;<br>  <span class="hljs-function">Gtid_set <span class="hljs-title">write_gtid_set</span><span class="hljs-params">(&amp;sid_map, <span class="hljs-literal">nullptr</span>)</span></span>;<br>  <span class="hljs-comment">/* 获得刚被切为后台Buffer的待刷写Buffer，逐个取出加入到write_gtid_set中 */</span><br>  <span class="hljs-keyword">auto</span> &amp;flush_list = <span class="hljs-built_in">get_list</span>(flush_list_number);<br>  <span class="hljs-keyword">for</span> (<span class="hljs-keyword">auto</span> &amp;gtid_info : flush_list) &#123;<br>    <span class="hljs-keyword">auto</span> gtid_str = <span class="hljs-built_in">reinterpret_cast</span>&lt;<span class="hljs-type">const</span> <span class="hljs-type">char</span> *&gt;(&amp;gtid_info[<span class="hljs-number">0</span>]);<br>    <span class="hljs-keyword">auto</span> status = write_gtid_set.<span class="hljs-built_in">add_gtid_text</span>(gtid_str);<br>  &#125;<br>  ...<br>  <span class="hljs-comment">/* 调用gtid_table_persistor-&gt;save()函数真正将GTID持久化到表中 */</span><br>  <span class="hljs-keyword">if</span> (!write_gtid_set.<span class="hljs-built_in">is_empty</span>()) &#123;<br>    ++m_compression_counter;<br>    err = gtid_table_persistor-&gt;<span class="hljs-built_in">save</span>(&amp;write_gtid_set, <span class="hljs-literal">false</span>);<br>  &#125;<br><br>  <span class="hljs-comment">/* 清空后台Buffer，将已刷写Buffer的编号存到对象字段中 */</span><br>  flush_list.<span class="hljs-built_in">clear</span>();<br>  m_flush_number.<span class="hljs-built_in">store</span>(flush_list_number);<br>  <span class="hljs-keyword">return</span> (err);<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="提交完成更新GTID状态"><a href="#提交完成更新GTID状态" class="headerlink" title="提交完成更新GTID状态"></a><strong>提交完成更新GTID状态</strong></h2><p><img src="/2023/10/15/%E6%B5%85%E6%9E%90MySQL-GTID/update.jpeg"><br>当事务完成提交后，需要在owned_gtids中解除自己和GTID的绑定关系，并且根据提交结果进一步更新gtid_stat，如果提交成功则将GTID加入到executed_gtids中；回滚则更新next_free_gno，方便下次分配时能够分到这些被回滚事务的GTID(更小)。整个流程的调用栈及源码如下：<br><img src="/2023/10/15/%E6%B5%85%E6%9E%90MySQL-GTID/update_stack.png"></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">Gtid_state::update_commit_group</span><span class="hljs-params">(THD *first_thd)</span> </span>&#123;<br>  <span class="hljs-comment">/* 给总锁加读锁 */</span><br>  global_sid_lock-&gt;<span class="hljs-built_in">rdlock</span>();<br>  <span class="hljs-comment">/* 组内事务涉及到的所有sidno加锁 */</span><br>  <span class="hljs-built_in">update_gtids_impl_lock_sidnos</span>(first_thd);<br><br>  <span class="hljs-keyword">for</span> (THD *thd = first_thd; thd != <span class="hljs-literal">nullptr</span>; thd = thd-&gt;next_to_commit) &#123;<br>    <span class="hljs-comment">/* 该事务是提交还是rollback */</span><br>    <span class="hljs-type">bool</span> is_commit = (thd-&gt;commit_error != THD::CE_COMMIT_ERROR);<br>    ... <span class="hljs-comment">/* 1.跳过不需要处理GTID的事务 */</span><br>    <span class="hljs-comment">/* 用于处理违反GTID一致性的事务，略过 */</span><br>    <span class="hljs-type">bool</span> more_trx_with_same_gtid_next = <span class="hljs-built_in">update_gtids_impl_begin</span>(thd);<br>    ...<br>    <span class="hljs-comment">/* 释放线程对GTID的拥有权，根据is_commit决定是否将</span><br><span class="hljs-comment">       GTID加入executed_gtids */</span><br>    <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span> (thd-&gt;owned_gtid.sidno &gt; <span class="hljs-number">0</span>) &#123;<br>      <span class="hljs-built_in">update_gtids_impl_own_gtid</span>(thd, is_commit);<br>    &#125;<br>    ...<br>  &#125;<br><br>  <span class="hljs-comment">/* 释放所有上的锁 */</span><br>  <span class="hljs-built_in">update_gtids_impl_broadcast_and_unlock_sidnos</span>();<br>  global_sid_lock-&gt;<span class="hljs-built_in">unlock</span>();<br>&#125;<br></code></pre></td></tr></table></figure><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">Gtid_state::update_gtids_impl_own_gtid</span><span class="hljs-params">(THD *thd, <span class="hljs-type">bool</span> is_commit)</span> </span>&#123;<br>  <span class="hljs-comment">/* 解除线程和GTID的绑定关系 */</span><br>  owned_gtids.<span class="hljs-built_in">remove_gtid</span>(thd-&gt;owned_gtid, thd-&gt;<span class="hljs-built_in">thread_id</span>());<br><br>  <span class="hljs-keyword">if</span> (is_commit) &#123;<br>    <span class="hljs-comment">/* 已提交的事务GTID加入到execute_gtids中 */</span><br>    executed_gtids._add_gtid(thd-&gt;owned_gtid);<br>    <span class="hljs-comment">/*  */</span><br>    thd-&gt;rpl_thd_ctx.<span class="hljs-built_in">session_gtids_ctx</span>().<span class="hljs-built_in">notify_after_gtid_executed_update</span>(thd);<br>    <span class="hljs-comment">/* 备库回放线程在开启binlog的情况下，如果被关闭了写binlog的能力，</span><br><span class="hljs-comment">       这些回放的事务就无法记录在binlog中，需要将GTID加入到lost_gtids</span><br><span class="hljs-comment">       和gtids_only_in_table中 */</span><br>    <span class="hljs-keyword">if</span> (thd-&gt;slave_thread &amp;&amp; opt_bin_log &amp;&amp; !opt_log_replica_updates) &#123;<br>      lost_gtids._add_gtid(thd-&gt;owned_gtid);<br>      gtids_only_in_table._add_gtid(thd-&gt;owned_gtid);<br>    &#125;<br>  &#125; <span class="hljs-keyword">else</span> &#123;<br>    <span class="hljs-comment">/* 事务回滚，GTID又可用了，根据大小比较回退next_free_gno */</span><br>    <span class="hljs-keyword">if</span> (thd-&gt;owned_gtid.sidno == server_sidno &amp;&amp; next_free_gno &gt; thd-&gt;owned_gtid.gno)<br>      next_free_gno = thd-&gt;owned_gtid.gno;<br>  &#125;<br><br>  <span class="hljs-comment">/* 清理线程内相关字段，gtid_next如果是ASSIGNED_GTID类型就设为undefined */</span><br>  thd-&gt;<span class="hljs-built_in">clear_owned_gtids</span>();<br>  <span class="hljs-keyword">if</span> (thd-&gt;variables.gtid_next.type == ASSIGNED_GTID) &#123;<br>    thd-&gt;variables.gtid_next.<span class="hljs-built_in">set_undefined</span>();<br>  &#125;<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="备库回放"><a href="#备库回放" class="headerlink" title="备库回放"></a><strong>备库回放</strong></h2><p><img src="/2023/10/15/%E6%B5%85%E6%9E%90MySQL-GTID/replay.jpeg"><br>当在备库START SLAVE时，备库上会调用start_slave_cmd-&gt;start_slave-&gt;start_slave_threads函数，会为每个源创建两个线程，分别是IO线程和SQL线程，其中IO线程(对应handle_slave_io函数)负责不断从主库获取binlog内容并追加到自己的relay log中，SQL线程(对应handle_slave_sql函数)则不断回放relay log中的event，对于GTID Event，SQL线程会检查GTID并为事务设置gtid_next变量，随后开启事务。其中gtid_next变量的信息如下所示：</p><h3 id="gtid-next"><a href="#gtid-next" class="headerlink" title="gtid_next"></a>gtid_next</h3><ul><li>Scope : Session</li><li>Dynamic : Yes</li><li>Type : Enumeration</li><li>Default Value : AUTOMATIC</li><li>Valid Values :<ul><li>AUTOMATIC: 使用自动产生的下一个GTID</li><li>ANONYMOUS: 事务没有GTID, 只使用file and position作为标识</li><li>UUID:NUMBER: 指定分配</li></ul></li></ul><p>gtid_next是主备同步的关键变量，用于保证主备库上同一个事务被分配了相同的GTID。在主库上一般设置为AUTOMATIC也就是自动分配，会为即将写入binlog的事务分配连续自增的GTID；在备库上进行复制事务时，会根据binlog中的GTID event存的GTID，把备库的gtid_next变量设置为一个确切的GTID，之后事务提交完成后，无论事务会不会被写入备库binlog，都会将该GTID加入到gtid_executed变量中，即标记已执行。</p><p>在代码中对应的是thd-&gt;variables.gtid_next，该变量标识了GTID分配方式，该字段类型为Gtid_specification，如下：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-keyword">struct</span> <span class="hljs-title class_">Gtid_specification</span> &#123;<br>  <span class="hljs-comment">/* GTID所属类型，AUTOMATIC、ANONYMOUS、ASSIGNED_GTID三种 */</span><br>  enum_gtid_type type;<br>  <span class="hljs-comment">/* type为ASSIGNED_GTID会存具体的sidno、gno，否则都为0 */</span><br>  Gtid gtid;<br>  ...<br>&#125;<br></code></pre></td></tr></table></figure><p>备库设置gtid_next的调用栈及源码如下：<br><img src="/2023/10/15/%E6%B5%85%E6%9E%90MySQL-GTID/replay_stack.png"></p><p>备库SQL线程执行GTID Event的时候会调用Gtid_log_event::do_apply_event函数，该函数会设置线程的gtid_next变量，判断事务是否应被取消或跳过，如下：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">Gtid_log_event::do_apply_event</span><span class="hljs-params">(Relay_log_info <span class="hljs-type">const</span> *rli)</span> </span>&#123;<br>  <span class="hljs-type">const</span> Gtid_specification *gtid_next = &amp;thd-&gt;variables.gtid_next;<br>  <span class="hljs-comment">/* 在Relay Log某个事务不完整时，会导致thd已经设置了GTID并且在事务中，</span><br><span class="hljs-comment">     这时需要rollback这个不完整的事务 */</span><br>  <span class="hljs-keyword">if</span> (!thd-&gt;<span class="hljs-built_in">owned_gtid_is_empty</span>() ||<br>      (thd-&gt;<span class="hljs-built_in">owned_gtid_is_empty</span>() &amp;&amp; gtid_next-&gt;type == ASSIGNED_GTID)) &#123;<br>    ...<br>    gtid_state-&gt;<span class="hljs-built_in">update_on_rollback</span>(thd);<br>  &#125;<br>  ...<br>  <span class="hljs-comment">/* 将GTID Event内记录的GTID信息spec视情况设置到thd的gtid_next变量中 */</span><br>  <span class="hljs-keyword">if</span> (<span class="hljs-built_in">set_gtid_next</span>(thd, spec))<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">1</span>;<br><br>  <span class="hljs-comment">/* 会判断GTID对应的事务是否需要被取消、跳过，用在之后的</span><br><span class="hljs-comment">     started_processing函数 */</span><br>  enum_gtid_statement_status state = <span class="hljs-built_in">gtid_pre_statement_checks</span>(thd);<br>  <span class="hljs-comment">/* 将GTID Event记录的一系列信息设置到thd相应变量中 */</span><br>  thd-&gt;variables.original_commit_timestamp = original_commit_timestamp;<br>  thd-&gt;<span class="hljs-built_in">set_original_commit_timestamp_for_slave_thread</span>();<br>  thd-&gt;variables.original_server_version = original_server_version;<br>  thd-&gt;variables.immediate_server_version = immediate_server_version;<br>  <span class="hljs-built_in">const_cast</span>&lt;Relay_log_info *&gt;(rli)-&gt;<span class="hljs-built_in">started_processing</span>(<br>      thd-&gt;variables.gtid_next.gtid, original_commit_timestamp,<br>      immediate_commit_timestamp, state == GTID_STATEMENT_SKIP);<br>  ...<br>&#125;<br></code></pre></td></tr></table></figure><p>再细致看一下set_gtid_next函数，该函数会对GTID的executed、owned情况进行检查，设置的gtid_next变量会被gtid_pre_statement_checks函数用于判断是否应该跳过事务，set_gtid_next函数实现如下：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-function"><span class="hljs-type">bool</span> <span class="hljs-title">set_gtid_next</span><span class="hljs-params">(THD *thd, <span class="hljs-type">const</span> Gtid_specification &amp;spec)</span> </span>&#123;<br>  ... <span class="hljs-comment">/* 检查thd的owned_gtid需要是空的 */</span><br>  <span class="hljs-comment">/* 根据GTID类型区分，对于备库回放来说只可能是ANONYMOUS或ASSIGNED */</span><br>  <span class="hljs-keyword">switch</span> (spec.type) &#123;<br>    <span class="hljs-keyword">case</span> AUTOMATIC_GTID:<br>      thd-&gt;variables.gtid_next.<span class="hljs-built_in">set_automatic</span>();<br>      <span class="hljs-keyword">break</span>;<br><br>    <span class="hljs-keyword">case</span> ANONYMOUS_GTID:<br>      <span class="hljs-comment">/* 开启GTID的模式下不允许 */</span><br>      <span class="hljs-keyword">if</span> (global_gtid_mode.<span class="hljs-built_in">get</span>() == Gtid_mode::ON) &#123;<br>        <span class="hljs-built_in">my_error</span>(ER_CANT_SET_GTID_NEXT_TO_ANONYMOUS_WHEN_GTID_MODE_IS_ON, <span class="hljs-built_in">MYF</span>(<span class="hljs-number">0</span>));<br>        <span class="hljs-keyword">goto</span> err;<br>      &#125;<br>      thd-&gt;variables.gtid_next.<span class="hljs-built_in">set_anonymous</span>();<br>      thd-&gt;owned_gtid.sidno = THD::OWNED_SIDNO_ANONYMOUS;<br>      thd-&gt;owned_gtid.gno = <span class="hljs-number">0</span>;<br>      gtid_state-&gt;<span class="hljs-built_in">acquire_anonymous_ownership</span>();<br>      <span class="hljs-keyword">break</span>;<br><br>    <span class="hljs-keyword">case</span> ASSIGNED_GTID:<br>      <span class="hljs-keyword">while</span> (<span class="hljs-literal">true</span>) &#123;<br>        <span class="hljs-comment">/* GTID不应是关闭模式 */</span><br>        <span class="hljs-keyword">if</span> (global_gtid_mode.<span class="hljs-built_in">get</span>() == Gtid_mode::OFF) &#123;<br>          <span class="hljs-built_in">my_error</span>(ER_CANT_SET_GTID_NEXT_TO_GTID_WHEN_GTID_MODE_IS_OFF, <span class="hljs-built_in">MYF</span>(<span class="hljs-number">0</span>));<br>          <span class="hljs-keyword">goto</span> err;<br>        &#125;<br>        <br>        <span class="hljs-comment">/* 获得sidno对应的锁 */</span><br>        gtid_state-&gt;<span class="hljs-built_in">lock_sidno</span>(spec.gtid.sidno);<br>        <span class="hljs-comment">/* GTID已经被执行过了，仅仅设置gtid_next，不设置owned_gtid，</span><br><span class="hljs-comment">           用于之后gtid_pre_statement_checks函数判断是否跳过执行事务 */</span><br>        <span class="hljs-keyword">if</span> (gtid_state-&gt;<span class="hljs-built_in">is_executed</span>(spec.gtid)) &#123;<br>          thd-&gt;variables.gtid_next = spec;<br>          <span class="hljs-keyword">break</span>;<br>        &#125;<br><br>        <span class="hljs-comment">/* 检查该GTID是否已经被某个线程拥有，没有的话赋予拥有权，</span><br><span class="hljs-comment">           否则不断在while循环内等待GTID被释放 */</span><br>        <span class="hljs-keyword">if</span> (!gtid_state-&gt;<span class="hljs-built_in">is_owned</span>(spec.gtid)) &#123;<br>          <span class="hljs-keyword">if</span> (gtid_state-&gt;<span class="hljs-built_in">acquire_ownership</span>(thd, spec.gtid)) <span class="hljs-keyword">goto</span> err;<br>          thd-&gt;variables.gtid_next = spec;<br>          <span class="hljs-keyword">break</span>;<br>        &#125;<br>        <span class="hljs-keyword">else</span> &#123;<br>          <span class="hljs-comment">/* 等待GTID拥有权释放出来 */</span><br>          gtid_state-&gt;<span class="hljs-built_in">wait_for_gtid</span>(thd, spec.gtid);<br>          ...<br>        &#125;<br>      &#125;<br>      <span class="hljs-keyword">break</span>;<br><br>    <span class="hljs-keyword">case</span> PRE_GENERATE_GTID: &#123;<br>      Gtid_specification new_spec = spec;<br>      gtid_state-&gt;<span class="hljs-built_in">lock_sidno</span>(new_spec.gtid.sidno);<br>      new_spec.gtid.gno = gtid_state-&gt;<span class="hljs-built_in">get_automatic_gno</span>(new_spec.gtid.sidno);<br>      <span class="hljs-keyword">if</span> (new_spec.gtid.gno == <span class="hljs-number">-1</span>) <span class="hljs-keyword">goto</span> err;<br>      <span class="hljs-keyword">if</span> (gtid_state-&gt;<span class="hljs-built_in">acquire_ownership</span>(thd, new_spec.gtid)) <span class="hljs-keyword">goto</span> err;<br>      new_spec.type = ASSIGNED_GTID;<br>      thd-&gt;variables.gtid_next = new_spec;<br>      <span class="hljs-keyword">break</span>;<br>    &#125;<br>  &#125;<br>  ...<br>  <span class="hljs-keyword">if</span> (lock_count == <span class="hljs-number">2</span>) gtid_state-&gt;<span class="hljs-built_in">unlock_sidno</span>(spec.gtid.sidno);<br>  <span class="hljs-keyword">if</span> (lock_count &gt;= <span class="hljs-number">1</span>) global_sid_lock-&gt;<span class="hljs-built_in">unlock</span>();<br>  ...<br>&#125;<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>MySQL</category>
      
      <category>Server</category>
      
      <category>Replication Layer</category>
      
    </categories>
    
    
    <tags>
      
      <tag>MySQL</tag>
      
      <tag>读源码</tag>
      
      <tag>复制</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>分布式文件系统 Frangipani</title>
    <link href="/2023/10/15/%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F-Frangipani/"/>
    <url>/2023/10/15/%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F-Frangipani/</url>
    
    <content type="html"><![CDATA[<p>论文标题为《Frangipani: A Scalable Distributed File System》，发表于1997年，这篇论文主要介绍了一个用于多人协作的分布式文件系统，介绍了一个通过读写锁解决缓存一致性的方案，虽然年代久远并且设计比较粗犷，但是还是有一些可借鉴之处。</p><h2 id="概览"><a href="#概览" class="headerlink" title="概览"></a>概览</h2><p><img src="/2023/10/15/%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F-Frangipani/%E5%9B%BE1.png" alt="图1" title="图1"></p><p>Frangipani用于多个用户在同一个文件系统中协同工作，底层存储系统Petal是作者之前做的块存储系统，可以看作一个存储空间很大的硬盘。所以可以将Frangipani看作是以Petal为存储底座的文件系统中间层，目标让多个用户使用这个文件系统时，仿佛就像同时在一台Linux机器的文件系统下协同工作。因为用户对于文件系统有读、写、修改元数据(改名、创建、删除文件等)操作，所以文件系统对多个用户的并发操作需要有协调作用，就有了一个分布式锁服务。</p><p>所以总体来看，整个架构中，多个客户端连接多个Frangipani服务器获取可以协同工作的分布式文件系统服务，多个Frangipani服务器连接到同一个Petal块存储服务共享同一个远程云硬盘，多个Frangipani服务器连接到同一个锁服务来完成并发协同。</p><h2 id="硬盘布局"><a href="#硬盘布局" class="headerlink" title="硬盘布局"></a>硬盘布局</h2><p><img src="/2023/10/15/%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F-Frangipani/%E5%9B%BE2.png" alt="图2" title="图2"></p><p>Petal提供的块存储服务可以呈现0～2^64的地址空间，Petal用64KB区分一个chunk，只会在真正存放数据的时候才在物理硬盘分配空间存这个chunk，所以是按需分配的，并且提供接口用于释放内存。</p><p>有了Petal提供的超大硬盘抽象，Frangipani对于硬盘的布局设计还是十分粗犷的，主要是六个部分：</p><ol><li>第一个部分存放了整个文件系统最基本的配置信息，虽然实际只用几KB，但是给它分配了1TB。</li><li>第二个部分是256个日志区，平分了1TB的空间，每个Frangipani服务器会被分配一个日志区(所以也限制了Frangipani服务器的数量)，日志区是被Frangipani服务器用于预写变更，来完成修改元数据的原子性，其实就是WAL的作用，避免因为故障的发生导致部分写。</li><li>第三个部分是bitmap，有3TB空间，用于标识对应地址的存储空间使用与否，每个bit对应4KB的块。</li><li>第四个部分是Inode区，正如Linux的文件系统一样，每个文件都有自己的Inode，里面记录了自己的基本信息，比如文件名、最近访问时间等等，Frangipani给每个Inode分了512B的大小，整体上占1TB的空间，所以Frangipani的文件数有上限的。在Frangipani的设计中，Inode和所能管辖的最大存储范围和bitmap中的范围是一一对应的，所以猜测当一个文件大小超过它的Inode管辖范围时，就需要找拥有更大管辖范围的未使用Inode进行替换、复制。</li><li>第五和第六部分则用于真正存储文件内容，不同的是，第五部分每个块大小为4KB，第六部分每个块大小为1TB。Frangipani分配块极其粗犷，文件小于等于64KB的部分在小块区，后续的部分就都放在大块区，同时限制了单个文件大小需要小于64KB+1TB。</li></ol><h2 id="缓存一致性"><a href="#缓存一致性" class="headerlink" title="缓存一致性"></a>缓存一致性</h2><p>每个Frangipani服务器很像是Petal上真正存储内容的缓存，客户端在Frangipani服务器上的读写操作都是发生在Frangipani服务器本地。所以Fangipani服务器自身有一部分Petal存储内容的缓存对外提供读写服务，读还好说，主要是写会修改数据，需要让Petal与其他Frangnipani服务器上的缓存感知到，所以就需要缓存一致性。为了达到缓存一致性，Frangipani设计了一个锁服务用于读写操作，读锁是共享锁，写锁是互斥锁。</p><p>Frangipani服务器需要遵守几个规则：</p><ul><li>如果没有拥有文件、目录的读锁或写锁，不能在本地缓存该文件、目录的数据(这里保证了无法读正在写的、过期的文件)</li><li>如果要写数据，需要获得对应数据块的写锁，并且在完成整个写操作的时候，才释放写锁(保证了正在写的中间态对外不可见)</li><li>获得的锁都有租约期限(30s)，到期前需要找锁服务续约，如果联系不上则认为自己掉线，在本地记录中释放锁，认为自己已不再拥有锁(保证了断线不宕机的情况下不会有多个服务器同时拥有一把互斥锁)</li></ul><p>锁服务基于Paxos共识协议实现，需要维护一个表，里面记录了每个文件、目录的锁的当前持有者，并且需要满足读写锁的限制。与锁服务交互相关的有四个接口：</p><ul><li>request：该接口用于Frangipani服务器向锁服务申请某个文件、目录的锁</li><li>grant：该接口用于锁服务向Frangipani服务器授予某个文件、目录的锁</li><li>revoke：该接口用于锁服务向Frangipani服务器要回某个文件、目录的锁(这里也可以因为租约到期，探测Frangipani服务器是不是挂了)</li><li>release：该接口用于Frangipani服务器向锁服务释放某个文件、目录的锁</li></ul><p>因为读写锁的存在加上Frangipani本身的缓存规则，保证了当某个Frangipani服务器写某个文件的数据时，其他Frangipani服务器上对于该文件的缓存要清空，同时也将所有写操作完成了排序，达到了缓存一致性(其实单机多核缓存一致性是不是也是类似)。</p><h2 id="原子性"><a href="#原子性" class="headerlink" title="原子性"></a>原子性</h2><p>对于创建文件、删除文件、修改文件名这种元数据操作，Frangipani需要保证操作的原子性，要么完全完成，要么完全失败，不存在写了一半就被外部看到中间结果的情况。像创建文件这种操作，需要先创建一个Inode，再在父目录中添加一个条目，分两步的写操作更像是一个事务。所以为了保证这样的事务的原子性，需要找锁服务获取所有需要的文件、目录的锁，然后再在其上进行修改操作。</p><h2 id="故障恢复"><a href="#故障恢复" class="headerlink" title="故障恢复"></a>故障恢复</h2><p>在硬盘布局里面给每一个Frangipani服务器都留了预写日志(WAL)的空间，可以有效防止服务器崩溃，留下一次写操作的中间态的问题。出于性能与空间的考虑，Frangipani不用日志记录修改的文件内容数据(所以这一部分内容故障无法恢复)，只记录体积很小的元数据修改，并且Frangipani服务器在进行修改操作的时候，会先在本地完成，并将相应的日志项纪录下来，缓存在自己本地，每隔一段时间将缓存在本地的日志项写到Petal的对应日志区，只有已经写到日志区的日志项里描述的变更才可以后续将缓存中的修改块写到存储数据中。</p><p>预写日志的存在，保证了元数据的修改一旦已经写到日志中，不管后续真正修改的过程中Frangipani服务器是否宕机，其他接手的服务器一定可以根据预写日志的描述进行故障恢复。假设某个Frangipani服务器F1写完日志后还没来得及在磁盘中修改完成便宕机了，锁服务会因为租约到期或者revoke没有回复而判断出F1宕机，那么就会找另一个服务器F2去接手F1日志中已经预写但没有完成的日志项。这里需要注意的是，因为规定一个日志区只能被一个Frangipani服务器写，所以锁服务也会给每个日志区分配一个互斥锁，也同样是会租约超时的，这就保证了假如F1只是掉线没有宕机，也不会继续向日志区写东西了。F2接手后会把F1的日志项每个修改都同步到真正的数据中，完成后就可以让锁服务把F1之前申请的锁都释放掉了。</p><p>还有一个问题，根据日志做恢复的时候怎么判断是否是过时的日志？日志项里面会存单调递增的日志id标识日志的顺序性，同时日志项中会记录修改的块的新版本号，所以故障恢复时检查版本号是否过期就可以知道是否过时。</p>]]></content>
    
    
    <categories>
      
      <category>MIT6.824 分布式系统</category>
      
    </categories>
    
    
    <tags>
      
      <tag>读论文</tag>
      
      <tag>分布式文件系统</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>状态复制 VM-FT</title>
    <link href="/2023/10/15/%E7%8A%B6%E6%80%81%E5%A4%8D%E5%88%B6-VM-FT/"/>
    <url>/2023/10/15/%E7%8A%B6%E6%80%81%E5%A4%8D%E5%88%B6-VM-FT/</url>
    
    <content type="html"><![CDATA[<p>论文标题为《The Design of a Practical System for Fault-Tolerant Virtual Machines》，发表于2010年，这篇论文主要介绍了VMware的虚拟机状态同步设计VM-FT，通过传递操作日志，达到机器级别的同步，并且具有外部一致性。</p><h2 id="状态同步"><a href="#状态同步" class="headerlink" title="状态同步"></a>状态同步</h2><p><img src="/2023/10/15/%E7%8A%B6%E6%80%81%E5%A4%8D%E5%88%B6-VM-FT/%E5%9B%BE1.png" alt="图1" title="图1"></p><p>VM-FT要达到的同步是虚拟机级别的同步，虚拟机不需要感知内部的应用程序的状态，所要做的就是所有虚拟机下辖内存、寄存器等机器级别的同步。为了方便备份虚拟机的快速上任，以及避免硬盘的同步(速度慢)，在VM-FT的设计中，如上图所示主备虚拟机使用的是远端共享硬盘。</p><p>为了完成状态同步，一般有两种选择：</p><ul><li>直接同步状态，比如对于数据库来说直接同步数据表</li><li><strong>同步操作日志</strong>，主备机从同样的状态开始(VMware实现了VMotion，可以不到1s完成虚拟机的状态复制，使得两个虚拟机可以有相同初始状态)，执行同样的操作，得到同样的确定结果，走到同样的状态</li></ul><p>第一种方式简单直接，但是会有很大的网络IO压力，因为虚拟机级别的状态是一个很大的镜像，不适合实时通过网络同步。VM-FT选择的是第二种方式，在主备虚拟机之间建立一个Logging channel，主虚拟机将所有操作以日志的形式同步给备虚拟机。在主备虚拟机上都有日志缓冲区，分别用于发送、接受，这也相当于是一种速率控制机制，当主虚拟机的发送缓冲区满了时，会调低自己在物理机上的CPU占比，减缓处理Client请求、发送日志的速率。</p><p>为了保证主备虚拟机状态的完全一致，需要所有操作按照相同顺序在主备虚拟机上执行，主虚拟机会按照顺序记录下执行的所有操作，对于那些可能产生不确定性结果的指令(例如获得当前时间)，还需要在日志项中记录产生的结果等信息，确保在备虚拟机上回放时可以产生相同的结果。还需要考虑中断的处理，VM-FT会将中断按照它发生的时间点，写成日志项插入到对应的日志位置中，使得备虚拟机可以在同样的位置模拟中断的发生。</p><p>下面给出一些会产生不确定性结果的例子及解决方案：</p><ul><li>获得当前时间：主虚拟机执行相应指令触发中断并随后获得当前时间，将指令和结果发送给备虚拟机，备虚拟机重放日志项的时候同样执行指令触发中断，但是采用日志项中记录的结果。</li><li>中断处理：主虚拟机执行指令时遇到了中断，会从CPU读指令号，随后在日志项中标记在某个指令遇到了什么中断，备虚拟机重放日志时，VM-FT在虚拟机执行对应指令时模拟出同样的中断。</li><li>硬盘操作：首先备虚拟机仅模拟硬盘读写，采用的是主虚拟机的结果，并不会真正读写共享硬盘。在VMware的虚拟机中使用了DMA，可以直接完成硬盘到内存的加载，以及内存到硬盘的写，但由于没有CPU介入会带来一些影响确定性的问题。比如：<ul><li>多个DMA任务同时用了同一块内存，VM-FT通过检测冲突，并给它们排序解决。</li><li>恰巧某个应用程序读了正在被DMA写的内存，读到的内容存在不确定性(写了10%？30%？100%？)，为了避免这种小概率事件的发生，VM-FT设计了专用的缓冲区，缓冲区所占内存是在物理机上而不是虚拟机中，DMA是将硬盘内容加载到缓冲区，当DMA完成后，再CPU介入将内容拷进虚拟机内存的对应位置，避免了中间状态。写硬盘也变成CPU介入将内存内容写到缓冲区，再由DMA完成缓冲区到硬盘的写。外部物理机的缓冲区使得整个过程不会因为DMA产生不确定性，进而保证了主备状态经过日志重放后完全一致。</li></ul></li><li>网络IO：网络异步IO会带来不确定性，所以VM-FT杜绝了网络的异步IO(虽然影响性能，但保证了确定性)，网卡向主虚拟机传收到的数据包时，需要发起中断引起CPU介入，那么日志项就如同中断处理一样，将中断、传输的数据都记录在日志项中，备虚拟机重放日志时就在同样的指令时模拟中断，根据日志项记录的数据模拟出从网卡读入数据包。</li></ul><h2 id="外部一致性"><a href="#外部一致性" class="headerlink" title="外部一致性"></a>外部一致性</h2><p>VM-FT希望达到的目标是，外部Client会在主虚拟机正常工作时与之交互，当主虚拟机发生故障后，备虚拟机可以立马上任，与外部Client交互，使得外部Client感知不到机器的切换。</p><p>首先备虚拟机通过心跳机制以及Logging channel(时间中断就如同心跳一样，每隔一段时间就中断一下产生日志)可以感知到主虚拟机的宕机，这时备虚拟机会将自己日志缓冲区内的日志项重放完成后就转入主虚拟机的角色，向交换机广播自己的MAC地址，开始直接与外部Client交互，而不是向之前一样屏蔽掉自己对外的输出。这个过程可能会产生<strong>脑裂</strong>，即主虚拟机并没有宕机，而是两者网络断连，为了避免这种情况，VM-FT需要虚拟机在升为主节点时在共享磁盘上完成一个test and set操作，如果成功就正式成为主节点，如果失败了就说明另一个机器正在作为主节点，这时这个虚拟机会直接自杀。当备虚拟机升级为主虚拟机后，集群便可以选择另一个机器作为备虚拟机达到容错的目的，可以使用之前说的VMotion创建备虚拟机，这时主虚拟机切换成Log模式，就开始将自己的操作以日志形式同步给新的备虚拟机了。</p><p>解决了主备切换的问题后，还需要考虑的是对外部Client表现的需要像是没有切换一样，因为主备虚拟机之间一定存在至少一个指令(日志项)的延迟，假如主虚拟机在宕机前处理了某个请求(比如给x&#x3D;10加1得到11)返回结果给Client，但对应的日志还没有传给备虚拟机，当备虚拟机升级为主虚拟机时，就会停在处理这个请求之前的状态(即x&#x3D;10而不是11)，这就违反了外部一致性。为了避免这种情况，VM-FT令主备虚拟机遵守**输出规则(Output Rule)**，当对外进行输出(发送请求、回复)时，主虚拟机需要确保备虚拟机ACK了对应的日志项后，才可以对外真正输出，如下图所示。</p><p><img src="/2023/10/15/%E7%8A%B6%E6%80%81%E5%A4%8D%E5%88%B6-VM-FT/%E5%9B%BE2.png" alt="图2" title="图2"></p><p>输出规则保证了备虚拟机一定知晓对外产生了影响(包括写共享硬盘)的日志项，至于因为延迟而忽略的没来得及发送的操作，由于尚未对外产生影响，可以忽略掉，不会损害对外的一致性。为了避免主虚拟机还没来得及发送消息就宕机，备虚拟机接管后会发送消息(上图Backup的虚线箭头)，虽然有可能产生重复消息(主虚拟机已经发过了)，但是总比没发好，重发问题可以通过Client的TCP排除重复消息以及其他幂等方法解决。</p><h2 id="非共享硬盘场景"><a href="#非共享硬盘场景" class="headerlink" title="非共享硬盘场景"></a>非共享硬盘场景</h2><p>假如因为硬件条件或其他原因无法使用共享硬盘，就可以采用下图的设计，硬盘也被认为是状态的一部分。在开始同步时，除了原有VMotion创建备虚拟机外，还需要再同步一下硬盘内容，之后就依据日志完成同步。在这种场景下，备虚拟机要写自己的硬盘，但出于性能考虑不需要真正读硬盘，直接依赖日志中读的结果就可以。由于硬盘已经是自己私有的，不用像之前一样认为写硬盘是一种对外输出，需要主备虚拟机遵守输出规则，主虚拟机需要等到ACK才真正写，在这种场景中，主虚拟机直接写硬盘就可以，不需要等待。同时，由于缺少了共享存储，应对脑裂问题就需要借助第三方服务来保证只有一个主虚拟机对外交互。</p><p><img src="/2023/10/15/%E7%8A%B6%E6%80%81%E5%A4%8D%E5%88%B6-VM-FT/%E5%9B%BE3.png" alt="图3" title="图3"></p>]]></content>
    
    
    <categories>
      
      <category>MIT6.824 分布式系统</category>
      
    </categories>
    
    
    <tags>
      
      <tag>读论文</tag>
      
      <tag>复制</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>链式复制 CRAQ</title>
    <link href="/2023/10/15/%E9%93%BE%E5%BC%8F%E5%A4%8D%E5%88%B6-CRAQ/"/>
    <url>/2023/10/15/%E9%93%BE%E5%BC%8F%E5%A4%8D%E5%88%B6-CRAQ/</url>
    
    <content type="html"><![CDATA[<p>论文标题为《Object Storage on CRAQ: High-throughput chain replication for read-mostly workloads》，发表于2009年，这篇论文主要介绍了链式复制CR的升级版，可以让非尾节点响应读请求。</p><h2 id="架构设计"><a href="#架构设计" class="headerlink" title="架构设计"></a>架构设计</h2><p><img src="/2023/10/15/%E9%93%BE%E5%BC%8F%E5%A4%8D%E5%88%B6-CRAQ/%E5%9B%BE1.png" alt="图1" title="图1"></p><p>存储服务对外暴露两个接口write、read，分别对应了写和读：</p><ul><li>write写操作需要整条链的参与，客户端向首节点发写请求，首节点直接计算出要写入存储的结果，存入本地之后将结果传给下一个节点，直到传给尾节点，尾节点处理完成后向客户端发送回复。每个节点接到写操作时把新版本的数据标记为dirty，这时会是多个版本数据共存的状态，当接到尾节点反向传播的ACK时把对应版本的数据标记为clean，并将旧版本的数据删除。</li><li>与CR只允许尾节点处理读请求不同，CRAQ中read读操作可以由任意节点完成。当读请求发送给一个节点，假如该节点上读取对象的版本仅有一个clean版本时直接返回即可(上图后两个节点就可以clean读)；假如该节点上读取对象的版本有dirty版本数据(即正在写但不确定是否已commit，上图前两个节点就是dirty读)，就需要找尾节点确认最新的已commit版本号(上图第2个节点与尾节点之间的虚线)，随后节点将对应版本的数据返回即可。</li></ul><p>CRAQ整个设计是在CR基础上完成的，主要是为每个节点都增加了处理读请求的能力，为尾节点分担读压力。CRAQ与CR一样也是读写线性(强一致性)的，clean读一定是commit过的，dirty读找尾节点确认版本退化成和CR一样。CRAQ的故障恢复策略与CR一致。</p>]]></content>
    
    
    <categories>
      
      <category>MIT6.824 分布式系统</category>
      
    </categories>
    
    
    <tags>
      
      <tag>读论文</tag>
      
      <tag>复制</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>链式复制 CR</title>
    <link href="/2023/10/15/%E9%93%BE%E5%BC%8F%E5%A4%8D%E5%88%B6-CR/"/>
    <url>/2023/10/15/%E9%93%BE%E5%BC%8F%E5%A4%8D%E5%88%B6-CR/</url>
    
    <content type="html"><![CDATA[<p>论文标题为《Chain Replication for Supporting High Throughput and Availability》，发表于2004年，这篇论文主要介绍了常被用于主备复制的经典的链式复制CR。</p><h2 id="架构设计"><a href="#架构设计" class="headerlink" title="架构设计"></a>架构设计</h2><p><img src="/2023/10/15/%E9%93%BE%E5%BC%8F%E5%A4%8D%E5%88%B6-CR/%E5%9B%BE1.png" alt="图1" title="图1"></p><p>存储服务对外暴露两个接口query、update，分别对应了读和写：</p><ul><li>query读操作仅发生在尾节点，客户端向尾节点发读请求，尾节点从自身存储找到对应内容后回复</li><li>update写操作需要整条链的参与，客户端向首节点发写请求，首节点直接计算出要写入存储的结果(这样避免了重复计算，并且保证随机结果全链一致)，存入本地之后将结果传给下一个节点，直到传给尾节点，尾节点处理完成后向客户端发送回复</li></ul><p>整个设计还是简单直接的，保证了线性读写，能容错n-1台机器故障。虽然单个链的读性能仅仅依靠一个尾节点，很容易达到性能瓶颈，但是整个存储服务有很多条链，每条链分别负责不同范围的内容，那么同一台机器可能在某个链上是中间节点，在另一个链上就是尾节点，这样就可以把整个存储服务的读请求分摊给多个机器了。</p><h2 id="故障恢复"><a href="#故障恢复" class="headerlink" title="故障恢复"></a>故障恢复</h2><p>为了能够协调全链有哪些机器，以及机器的先后关系，CR要求有个全局的Master节点，它统筹规划了所有链的结构。</p><h3 id="首节点故障"><a href="#首节点故障" class="headerlink" title="首节点故障"></a>首节点故障</h3><p>Master发现某个链首节点故障后，直接将旧首节点的后继节点提拔为首节点，并通知客户端即可，仅仅损失一些到达旧首节点但是未到新首节点的写请求，因为这些请求尚未写成功，所以忽略掉不影响一致性。</p><h3 id="尾节点故障"><a href="#尾节点故障" class="headerlink" title="尾节点故障"></a>尾节点故障</h3><p>Master发现某个链尾节点故障后，直接将旧尾节点的前继节点提拔为尾节点，并通知客户端即可，因为任何旧尾节点有的内容新尾节点都有，这个动作不会产生任何损失，会令一些写请求提前完成。</p><h3 id="中间节点故障"><a href="#中间节点故障" class="headerlink" title="中间节点故障"></a>中间节点故障</h3><p>中间节点故障也很类似首尾，将该节点的前继节点的后继节点修改为该节点的后继节点，相当于把该节点摘掉。但是与首尾节点不同的是，可能有一部分前继节点传给本节点的写请求还没有传给后继节点，这一部分驻留在故障节点的请求会被永远忽略掉，导致链中前后不一致。</p><p>为了在中间节点故障时，前继节点知道该给后继节点重传哪些写请求，CR要求节点维护自己传出的写请求集合S，这个S不是无限增长的，当一个写请求被尾节点处理后，会沿链反向ACK这个请求，这时节点会将请求从S中剔除。</p><p>因此当中间节点故障时，Master会联系后继节点获得它已接收到的请求的序列号x，然后发给前继节点，前继节点就会将自己的S中序列号高于x的写请求打包发给后继节点，之后就和正常一样向后继节点发送收到的写请求，完成故障恢复。</p><h3 id="新增节点"><a href="#新增节点" class="headerlink" title="新增节点"></a>新增节点</h3><p>当故障发生后，虽然CR完成了故障恢复可以正常工作，但是链长度减少了，容错能力就变弱了，这时可以给链加入新节点恢复链长度。新增节点时可以直接将新节点接到尾节点之后，令老的尾节点停止对外服务，将存储内容复制给新节点，这期间旧尾节点接到的写请求被存在自己的集合S中，随后发给新节点，完成这些后新节点正式成为新的尾节点，Master通知所有客户端这个变化。</p>]]></content>
    
    
    <categories>
      
      <category>MIT6.824 分布式系统</category>
      
    </categories>
    
    
    <tags>
      
      <tag>读论文</tag>
      
      <tag>复制</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
